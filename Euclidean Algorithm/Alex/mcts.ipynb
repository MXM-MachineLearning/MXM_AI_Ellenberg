{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2023-09-26T03:50:58.435576Z",
     "start_time": "2023-09-26T03:50:58.430138Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [],
   "source": [
    "ACTIONS = torch.tensor([[1, 1], [0, -1]]), torch.tensor([[0, 1], [1, 0]], dtype=torch.float)\n",
    "\n",
    "def gen_start_config():\n",
    "    return torch.round(torch.rand((1, 2)) * 100)   # TODO make a better stochastic thing\n",
    "\n",
    "def terminate(s):\n",
    "    k_eps = 1e-4    # for float imprecision\n",
    "    # terminate if one of the coordinates is 0\n",
    "    return s[0] <= k_eps or s[1] <= k_eps"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-09-26T03:50:58.440496Z",
     "start_time": "2023-09-26T03:50:58.438155Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "        self.visits = 0\n",
    "        self.children = []\n",
    "    \n",
    "    def UCT_fn(self, child, C):\n",
    "        return child.value + 2 * C * torch.sqrt(2 * torch.log2(self.visits) / child.visits)\n",
    "    \n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-09-26T03:50:58.457775Z",
     "start_time": "2023-09-26T03:50:58.454351Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "inf\n",
      "-0.70703125\n"
     ]
    }
   ],
   "source": [
    "LR = 1e-4\n",
    "GAMMA = 0.99\n",
    "STEP_CAP = 100\n",
    "TAU = 0.005\n",
    "\n",
    "target_net = DQN().to(device)\n",
    "target_net.load_state_dict(target_net.state_dict())\n",
    "optimizer = optim.AdamW(target_net.parameters(), lr=LR, amsgrad=True)\n",
    "\n",
    "memory = ReplayMemory(STEP_CAP)\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "def next_states(state):\n",
    "    return state @ ACTIONS[0], state @ ACTIONS[0]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-09-26T03:50:58.464082Z",
     "start_time": "2023-09-26T03:50:58.460108Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[322.  73.]\n",
      "Accuracy: 0.9\n"
     ]
    }
   ],
   "source": [
    "# TODO implement batch\n",
    "def optimize_model():\n",
    "    transitions = memory.sample()\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    \n",
    "    print(batch.next_state)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None and terminate(s) is not True,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if not (s is None or terminate(s))])\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(torch.cat(batch.state))\n",
    "    \n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(1)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = torch.max(target_net(non_final_next_states))\n",
    "\n",
    "    # Compute the expected Q values\n",
    "    global steps_done\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + steps_done\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-09-26T03:50:58.504347Z",
     "start_time": "2023-09-26T03:50:58.467866Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE 0\n",
      "step 0\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[85.],\n",
      "        [53.]]), action=tensor(1), next_state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), reward=tensor(-100.1699))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "step 1\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[85.],\n",
      "        [53.]]), action=tensor(1), next_state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), reward=tensor(-200.3397))\n",
      "(tensor([[85],\n",
      "        [53]], dtype=torch.int32),)\n",
      "step 2\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), action=tensor(1), next_state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), reward=tensor(-300.5096))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "step 3\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), action=tensor(1), next_state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), reward=tensor(-400.6794))\n",
      "(tensor([[85],\n",
      "        [53]], dtype=torch.int32),)\n",
      "step 4\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[85.],\n",
      "        [53.]]), action=tensor(1), next_state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), reward=tensor(-500.8493))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "step 5\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), action=tensor(1), next_state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), reward=tensor(-601.0191))\n",
      "(tensor([[85],\n",
      "        [53]], dtype=torch.int32),)\n",
      "step 6\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), action=tensor(1), next_state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), reward=tensor(-701.1890))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "step 7\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), action=tensor(1), next_state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), reward=tensor(-801.3588))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "step 8\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), action=tensor(1), next_state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), reward=tensor(-901.5287))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "step 9\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), action=tensor(1), next_state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), reward=tensor(-1001.6985))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "step 10\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), action=tensor(1), next_state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), reward=tensor(-1101.8684))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "step 11\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), action=tensor(1), next_state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), reward=tensor(-1202.0382))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "step 12\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[85.],\n",
      "        [53.]]), action=tensor(1), next_state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), reward=tensor(-1302.2080))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "step 13\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), action=tensor(1), next_state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), reward=tensor(-1402.3778))\n",
      "(tensor([[85],\n",
      "        [53]], dtype=torch.int32),)\n",
      "step 14\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), action=tensor(1), next_state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), reward=tensor(-1502.5476))\n",
      "(tensor([[85],\n",
      "        [53]], dtype=torch.int32),)\n",
      "step 15\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), action=tensor(1), next_state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), reward=tensor(-1602.7174))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "step 16\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), action=tensor(1), next_state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), reward=tensor(-1702.8872))\n",
      "(tensor([[85],\n",
      "        [53]], dtype=torch.int32),)\n",
      "step 17\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), action=tensor(1), next_state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), reward=tensor(-1803.0570))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "step 18\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), action=tensor(1), next_state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), reward=tensor(-1903.2268))\n",
      "(tensor([[85],\n",
      "        [53]], dtype=torch.int32),)\n",
      "step 19\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), action=tensor(1), next_state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), reward=tensor(-2003.3966))\n",
      "(tensor([[85],\n",
      "        [53]], dtype=torch.int32),)\n",
      "step 20\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), action=tensor(1), next_state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), reward=tensor(-2103.5664))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "step 21\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), action=tensor(1), next_state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), reward=tensor(-2203.7363))\n",
      "(tensor([[85],\n",
      "        [53]], dtype=torch.int32),)\n",
      "step 22\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), action=tensor(1), next_state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), reward=tensor(-2303.9062))\n",
      "(tensor([[85],\n",
      "        [53]], dtype=torch.int32),)\n",
      "step 23\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), action=tensor(1), next_state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), reward=tensor(-2404.0762))\n",
      "(tensor([[85],\n",
      "        [53]], dtype=torch.int32),)\n",
      "step 24\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), action=tensor(1), next_state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), reward=tensor(-2504.2461))\n",
      "(tensor([[85],\n",
      "        [53]], dtype=torch.int32),)\n",
      "step 25\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), action=tensor(1), next_state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), reward=tensor(-2604.4160))\n",
      "(tensor([[85],\n",
      "        [53]], dtype=torch.int32),)\n",
      "step 26\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), action=tensor(1), next_state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), reward=tensor(-2704.5859))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "step 27\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), action=tensor(1), next_state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), reward=tensor(-2804.7559))\n",
      "(tensor([[85],\n",
      "        [53]], dtype=torch.int32),)\n",
      "step 28\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), action=tensor(1), next_state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), reward=tensor(-2904.9258))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "step 29\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), action=tensor(1), next_state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), reward=tensor(-3005.0957))\n",
      "(tensor([[85],\n",
      "        [53]], dtype=torch.int32),)\n",
      "step 30\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), action=tensor(1), next_state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), reward=tensor(-3105.2656))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "step 31\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), action=tensor(1), next_state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), reward=tensor(-3205.4355))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "step 32\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), action=tensor(1), next_state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), reward=tensor(-3305.6055))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "step 33\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), action=tensor(1), next_state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), reward=tensor(-3405.7754))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "step 34\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), action=tensor(1), next_state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), reward=tensor(-3505.9453))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "step 35\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), action=tensor(1), next_state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), reward=tensor(-3606.1152))\n",
      "(tensor([[85],\n",
      "        [53]], dtype=torch.int32),)\n",
      "step 36\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), action=tensor(1), next_state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), reward=tensor(-3706.2852))\n",
      "(tensor([[85],\n",
      "        [53]], dtype=torch.int32),)\n",
      "step 37\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), action=tensor(1), next_state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), reward=tensor(-3806.4551))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "step 38\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), action=tensor(1), next_state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), reward=tensor(-3906.6250))\n",
      "(tensor([[85],\n",
      "        [53]], dtype=torch.int32),)\n",
      "step 39\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), action=tensor(1), next_state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), reward=tensor(-4006.7949))\n",
      "(tensor([[85],\n",
      "        [53]], dtype=torch.int32),)\n",
      "step 40\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), action=tensor(1), next_state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), reward=tensor(-4106.9648))\n",
      "(tensor([[85],\n",
      "        [53]], dtype=torch.int32),)\n",
      "step 41\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), action=tensor(1), next_state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), reward=tensor(-4207.1348))\n",
      "(tensor([[85],\n",
      "        [53]], dtype=torch.int32),)\n",
      "step 42\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), action=tensor(1), next_state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), reward=tensor(-4307.3047))\n",
      "(tensor([[85],\n",
      "        [53]], dtype=torch.int32),)\n",
      "step 43\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), action=tensor(1), next_state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), reward=tensor(-4407.4746))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "step 44\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), action=tensor(1), next_state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), reward=tensor(-4507.6445))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "step 45\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), action=tensor(1), next_state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), reward=tensor(-4607.8145))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "step 46\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), action=tensor(1), next_state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), reward=tensor(-4707.9844))\n",
      "(tensor([[85],\n",
      "        [53]], dtype=torch.int32),)\n",
      "step 47\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), action=tensor(1), next_state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), reward=tensor(-4808.1543))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "step 48\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), action=tensor(1), next_state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), reward=tensor(-4908.3242))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "step 49\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), action=tensor(1), next_state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), reward=tensor(-5008.4941))\n",
      "(tensor([[85],\n",
      "        [53]], dtype=torch.int32),)\n",
      "step 50\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), action=tensor(1), next_state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), reward=tensor(-5108.6641))\n",
      "(tensor([[85],\n",
      "        [53]], dtype=torch.int32),)\n",
      "step 51\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), action=tensor(1), next_state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), reward=tensor(-5208.8340))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "step 52\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), action=tensor(1), next_state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), reward=tensor(-5309.0039))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "step 53\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), action=tensor(1), next_state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), reward=tensor(-5409.1738))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "step 54\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), action=tensor(1), next_state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), reward=tensor(-5509.3438))\n",
      "(tensor([[85],\n",
      "        [53]], dtype=torch.int32),)\n",
      "step 55\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), action=tensor(1), next_state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), reward=tensor(-5609.5137))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "step 56\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), action=tensor(1), next_state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), reward=tensor(-5709.6836))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "step 57\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), action=tensor(1), next_state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), reward=tensor(-5809.8535))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "step 58\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), action=tensor(1), next_state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), reward=tensor(-5910.0234))\n",
      "(tensor([[85],\n",
      "        [53]], dtype=torch.int32),)\n",
      "step 59\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), action=tensor(1), next_state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), reward=tensor(-6010.1934))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "step 60\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), action=tensor(1), next_state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), reward=tensor(-6110.3633))\n",
      "(tensor([[85],\n",
      "        [53]], dtype=torch.int32),)\n",
      "step 61\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), action=tensor(1), next_state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), reward=tensor(-6210.5332))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "step 62\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), action=tensor(1), next_state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), reward=tensor(-6310.7031))\n",
      "(tensor([[85],\n",
      "        [53]], dtype=torch.int32),)\n",
      "step 63\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), action=tensor(1), next_state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), reward=tensor(-6410.8730))\n",
      "(tensor([[85],\n",
      "        [53]], dtype=torch.int32),)\n",
      "step 64\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), action=tensor(1), next_state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), reward=tensor(-6511.0430))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "step 65\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), action=tensor(1), next_state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), reward=tensor(-6611.2129))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "step 66\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), action=tensor(1), next_state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), reward=tensor(-6711.3828))\n",
      "(tensor([[85],\n",
      "        [53]], dtype=torch.int32),)\n",
      "step 67\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), action=tensor(1), next_state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), reward=tensor(-6811.5527))\n",
      "(tensor([[85],\n",
      "        [53]], dtype=torch.int32),)\n",
      "step 68\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), action=tensor(1), next_state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), reward=tensor(-6911.7227))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "step 69\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), action=tensor(1), next_state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), reward=tensor(-7011.8926))\n",
      "(tensor([[85],\n",
      "        [53]], dtype=torch.int32),)\n",
      "step 70\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), action=tensor(1), next_state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), reward=tensor(-7112.0625))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "step 71\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), action=tensor(1), next_state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), reward=tensor(-7212.2324))\n",
      "(tensor([[85],\n",
      "        [53]], dtype=torch.int32),)\n",
      "step 72\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), action=tensor(1), next_state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), reward=tensor(-7312.4023))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "step 73\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), action=tensor(1), next_state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), reward=tensor(-7412.5723))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "step 74\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), action=tensor(1), next_state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), reward=tensor(-7512.7422))\n",
      "(tensor([[85],\n",
      "        [53]], dtype=torch.int32),)\n",
      "step 75\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), action=tensor(1), next_state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), reward=tensor(-7612.9121))\n",
      "(tensor([[85],\n",
      "        [53]], dtype=torch.int32),)\n",
      "step 76\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), action=tensor(1), next_state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), reward=tensor(-7713.0820))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "step 77\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), action=tensor(1), next_state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), reward=tensor(-7813.2520))\n",
      "(tensor([[85],\n",
      "        [53]], dtype=torch.int32),)\n",
      "step 78\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), action=tensor(1), next_state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), reward=tensor(-7913.4219))\n",
      "(tensor([[85],\n",
      "        [53]], dtype=torch.int32),)\n",
      "step 79\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), action=tensor(1), next_state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), reward=tensor(-8013.5918))\n",
      "(tensor([[85],\n",
      "        [53]], dtype=torch.int32),)\n",
      "step 80\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), action=tensor(1), next_state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), reward=tensor(-8113.7617))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "step 81\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), action=tensor(1), next_state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), reward=tensor(-8213.9316))\n",
      "(tensor([[85],\n",
      "        [53]], dtype=torch.int32),)\n",
      "step 82\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), action=tensor(1), next_state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), reward=tensor(-8314.1016))\n",
      "(tensor([[85],\n",
      "        [53]], dtype=torch.int32),)\n",
      "step 83\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), action=tensor(1), next_state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), reward=tensor(-8414.2715))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "step 84\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), action=tensor(1), next_state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), reward=tensor(-8514.4414))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "step 85\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), action=tensor(1), next_state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), reward=tensor(-8614.6113))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "step 86\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), action=tensor(1), next_state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), reward=tensor(-8714.7812))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "step 87\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), action=tensor(1), next_state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), reward=tensor(-8814.9512))\n",
      "(tensor([[85],\n",
      "        [53]], dtype=torch.int32),)\n",
      "step 88\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), action=tensor(1), next_state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), reward=tensor(-8915.1211))\n",
      "(tensor([[85],\n",
      "        [53]], dtype=torch.int32),)\n",
      "step 89\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), action=tensor(1), next_state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), reward=tensor(-9015.2910))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "step 90\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), action=tensor(1), next_state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), reward=tensor(-9115.4609))\n",
      "(tensor([[85],\n",
      "        [53]], dtype=torch.int32),)\n",
      "step 91\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), action=tensor(1), next_state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), reward=tensor(-9215.6309))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "step 92\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), action=tensor(1), next_state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), reward=tensor(-9315.8008))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "step 93\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), action=tensor(1), next_state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), reward=tensor(-9415.9707))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "step 94\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[85.],\n",
      "        [53.]]), action=tensor(1), next_state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), reward=tensor(-9516.1406))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "step 95\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), action=tensor(1), next_state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), reward=tensor(-9616.3105))\n",
      "(tensor([[85],\n",
      "        [53]], dtype=torch.int32),)\n",
      "step 96\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), action=tensor(1), next_state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), reward=tensor(-9716.4805))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "step 97\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), action=tensor(1), next_state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), reward=tensor(-9816.6504))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "step 98\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), action=tensor(1), next_state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), reward=tensor(-9916.8203))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "step 99\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "Transition(state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), action=tensor(1), next_state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), reward=tensor(-10016.9902))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "step 100\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "tensor([[85.],\n",
      "        [53.]])\n",
      "torch.Size([2, 1])\n",
      "tensor(100.1699)\n",
      "early tensor([False])\n",
      "killed True\n",
      "Transition(state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), action=tensor(1), next_state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), reward=tensor(-10217.3301))\n",
      "(tensor([[85],\n",
      "        [53]], dtype=torch.int32),)\n",
      "EPISODE 1\n",
      "step 0\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "tensor([[31.],\n",
      "        [81.]])\n",
      "torch.Size([2, 1])\n",
      "tensor(86.7295)\n",
      "early tensor([False])\n",
      "killed True\n",
      "Transition(state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), action=tensor(1), next_state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), reward=tensor(-10217.3301))\n",
      "(tensor([[85],\n",
      "        [53]], dtype=torch.int32),)\n",
      "EPISODE 2\n",
      "step 0\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "tensor([[26.],\n",
      "        [77.]])\n",
      "torch.Size([2, 1])\n",
      "tensor(81.2711)\n",
      "early tensor([False])\n",
      "killed True\n",
      "Transition(state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), action=tensor(1), next_state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), reward=tensor(-10217.3301))\n",
      "(tensor([[85],\n",
      "        [53]], dtype=torch.int32),)\n",
      "EPISODE 3\n",
      "step 0\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "tensor([[69.],\n",
      "        [89.]])\n",
      "torch.Size([2, 1])\n",
      "tensor(112.6144)\n",
      "early tensor([False])\n",
      "killed True\n",
      "Transition(state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), action=tensor(1), next_state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), reward=tensor(-10217.3301))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "EPISODE 4\n",
      "step 0\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "tensor([[30.],\n",
      "        [12.]])\n",
      "torch.Size([2, 1])\n",
      "tensor(32.3110)\n",
      "early tensor([False])\n",
      "killed True\n",
      "Transition(state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), action=tensor(1), next_state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), reward=tensor(-10217.3301))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "EPISODE 5\n",
      "step 0\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "tensor([[38.],\n",
      "        [41.]])\n",
      "torch.Size([2, 1])\n",
      "tensor(55.9017)\n",
      "early tensor([False])\n",
      "killed True\n",
      "Transition(state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), action=tensor(1), next_state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), reward=tensor(-10217.3301))\n",
      "(tensor([[85],\n",
      "        [53]], dtype=torch.int32),)\n",
      "EPISODE 6\n",
      "step 0\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "tensor([[74.],\n",
      "        [29.]])\n",
      "torch.Size([2, 1])\n",
      "tensor(79.4796)\n",
      "early tensor([False])\n",
      "killed True\n",
      "Transition(state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), action=tensor(1), next_state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), reward=tensor(-10217.3301))\n",
      "(tensor([[85],\n",
      "        [53]], dtype=torch.int32),)\n",
      "EPISODE 7\n",
      "step 0\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "tensor([[11.],\n",
      "        [64.]])\n",
      "torch.Size([2, 1])\n",
      "tensor(64.9384)\n",
      "early tensor([False])\n",
      "killed True\n",
      "Transition(state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), action=tensor(1), next_state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), reward=tensor(-10217.3301))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "EPISODE 8\n",
      "step 0\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "tensor([[43.],\n",
      "        [67.]])\n",
      "torch.Size([2, 1])\n",
      "tensor(79.6116)\n",
      "early tensor([False])\n",
      "killed True\n",
      "Transition(state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), action=tensor(1), next_state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), reward=tensor(-10217.3301))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "EPISODE 9\n",
      "step 0\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "tensor([[100.],\n",
      "        [ 78.]])\n",
      "torch.Size([2, 1])\n",
      "tensor(126.8227)\n",
      "early tensor([False])\n",
      "killed True\n",
      "Transition(state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), action=tensor(1), next_state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), reward=tensor(-10217.3301))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "EPISODE 10\n",
      "step 0\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "tensor([[61.],\n",
      "        [34.]])\n",
      "torch.Size([2, 1])\n",
      "tensor(69.8355)\n",
      "early tensor([False])\n",
      "killed True\n",
      "Transition(state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), action=tensor(1), next_state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), reward=tensor(-10217.3301))\n",
      "(tensor([[53],\n",
      "        [85]], dtype=torch.int32),)\n",
      "EPISODE 11\n",
      "step 0\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "tensor([[89.],\n",
      "        [43.]])\n",
      "torch.Size([2, 1])\n",
      "tensor(98.8433)\n",
      "early tensor([False])\n",
      "killed True\n",
      "Transition(state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), action=tensor(1), next_state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), reward=tensor(-10217.3301))\n",
      "(tensor([[85],\n",
      "        [53]], dtype=torch.int32),)\n",
      "EPISODE 12\n",
      "step 0\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "tensor([[93.],\n",
      "        [15.]])\n",
      "torch.Size([2, 1])\n",
      "tensor(94.2019)\n",
      "early tensor([False])\n",
      "killed True\n",
      "Transition(state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), action=tensor(1), next_state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), reward=tensor(-10217.3301))\n",
      "(tensor([[85],\n",
      "        [53]], dtype=torch.int32),)\n",
      "EPISODE 13\n",
      "step 0\n",
      "action tensor(1) tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "tensor([[54.],\n",
      "        [25.]])\n",
      "torch.Size([2, 1])\n",
      "tensor(59.5063)\n",
      "early tensor([False])\n",
      "killed True\n",
      "Transition(state=tensor([[53],\n",
      "        [85]], dtype=torch.int32), action=tensor(1), next_state=tensor([[85],\n",
      "        [53]], dtype=torch.int32), reward=tensor(-10217.3301))\n",
      "(None,)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "torch.cat(): expected a non-empty list of Tensors",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[620], line 53\u001B[0m\n\u001B[1;32m     50\u001B[0m state \u001B[38;5;241m=\u001B[39m next_state\n\u001B[1;32m     52\u001B[0m \u001B[38;5;66;03m# Perform one step of the optimization (on the policy network)\u001B[39;00m\n\u001B[0;32m---> 53\u001B[0m \u001B[43moptimize_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     55\u001B[0m \u001B[38;5;66;03m# Soft update of the target network's weights\u001B[39;00m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;66;03m# θ′ ← τ θ + (1 −τ )θ′\u001B[39;00m\n\u001B[1;32m     57\u001B[0m target_net_state_dict \u001B[38;5;241m=\u001B[39m target_net\u001B[38;5;241m.\u001B[39mstate_dict()\n",
      "Cell \u001B[0;32mIn[619], line 9\u001B[0m, in \u001B[0;36moptimize_model\u001B[0;34m()\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28mprint\u001B[39m(batch\u001B[38;5;241m.\u001B[39mnext_state)\n\u001B[1;32m      7\u001B[0m non_final_mask \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(\u001B[38;5;28mtuple\u001B[39m(\u001B[38;5;28mmap\u001B[39m(\u001B[38;5;28;01mlambda\u001B[39;00m s: s \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m terminate(s) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m      8\u001B[0m                                       batch\u001B[38;5;241m.\u001B[39mnext_state)), device\u001B[38;5;241m=\u001B[39mdevice, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mbool)\n\u001B[0;32m----> 9\u001B[0m non_final_next_states \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43ms\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43ms\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnext_state\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m                                            \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mterminate\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# Compute Q(s_t, a) - the model computes Q(s_t), then we select the\u001B[39;00m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# columns of actions taken. These are the actions which would've been taken\u001B[39;00m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# for each batch state according to policy_net\u001B[39;00m\n\u001B[1;32m     14\u001B[0m state_action_values \u001B[38;5;241m=\u001B[39m policy_net(torch\u001B[38;5;241m.\u001B[39mcat(batch\u001B[38;5;241m.\u001B[39mstate)\u001B[38;5;241m.\u001B[39mt())\n",
      "\u001B[0;31mRuntimeError\u001B[0m: torch.cat(): expected a non-empty list of Tensors"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    num_episodes = 600\n",
    "else:\n",
    "    num_episodes = 50\n",
    "\n",
    "def env_step(action, state):\n",
    "    print(\"action\", action, ACTIONS[action])\n",
    "    pos = ACTIONS[action].int() @ state.int()   # todo \n",
    "    return pos, terminate(pos) \n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    print(\"EPISODE\", str(i_episode))\n",
    "    # Initialize the environment and get its state\n",
    "    start = gen_start_config()\n",
    "    if start.size() != (2, 1):\n",
    "        print(\"failed\", start)\n",
    "    \n",
    "    state = start\n",
    "    reward = 0\n",
    "    for t in count():\n",
    "        print(\"step\", t)\n",
    "        action = select_action(state)\n",
    "        new_state, done_early = env_step(action, state)\n",
    "        killed = steps_done > STEP_CAP\n",
    "        \n",
    "        # update reward\n",
    "        reward -= torch.linalg.norm(new_state.float())\n",
    "        if done_early:\n",
    "            reward += torch.sqrt(torch.norm(start)) # pretty arbitrary rn\n",
    "        elif killed:\n",
    "            print(start)\n",
    "            print(start.size())\n",
    "            print(torch.linalg.norm(start))\n",
    "            reward -= torch.norm(start) # todo change penalty magnitude to decrease as norm of start increases?\n",
    "        \n",
    "        done = done_early or killed\n",
    "        if done:\n",
    "            print(\"early\", done_early)\n",
    "            print(\"killed\", killed)\n",
    "        if done:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = new_state\n",
    "            \n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "        print(memory.sample()[0])\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "print('Complete')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-09-19T20:43:37.195124Z",
     "start_time": "2023-09-19T20:43:36.861093Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-19T20:43:37.194274Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
