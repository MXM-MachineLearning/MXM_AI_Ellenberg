{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T02:45:00.232700Z",
     "start_time": "2023-11-06T02:45:00.218140Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch import autograd\n",
    "\n",
    "import math\n",
    "\n",
    "from util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d12017b9641b3200",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T02:45:00.243783Z",
     "start_time": "2023-11-06T02:45:00.230649Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def terminal(state, k_eps=1e-4):\n",
    "    for i in state.flatten():\n",
    "        if abs(i) <= k_eps:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, parent, state, n_children, value, depth=0):\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.visits = 0\n",
    "        self.depth = depth\n",
    "        self.children = [None] * n_children\n",
    "        self.is_terminal = terminal(self.state)\n",
    "        self.value = value\n",
    "        self.subtree_value = torch.zeros(1)\n",
    "\n",
    "    def __str__(self):\n",
    "        return (\"State: \" + str(self.state) + \"; Value: \" + str(self.value)\n",
    "                + \"; Subtree Value: \" + str(self.subtree_value) + \"; Visits:\", str(self.visits))\n",
    "\n",
    "    def is_leaf(self):\n",
    "        for i in self.state:\n",
    "            if i is not None:\n",
    "                return False\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8031c754a4dbbeae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T02:48:56.728064Z",
     "start_time": "2023-11-06T02:48:56.722471Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MCTS:\n",
    "    def __init__(self, actions, C, weight, value_fn):\n",
    "        self.actions = actions\n",
    "        self.k_C = C\n",
    "        self.k_weight = weight\n",
    "        self.value_fn = value_fn\n",
    "        self.max_depth = 0\n",
    "        self.terminal = None    # None if no terminal state found; terminal Node if found\n",
    "        self.root = None\n",
    "\n",
    "    def pick_child(self, node):\n",
    "        # UCT\n",
    "        t = []\n",
    "        for i in node.children:\n",
    "            if i is None:\n",
    "                continue\n",
    "            t.append(UCT_fn(i, self.k_C))\n",
    "\n",
    "        t = torch.tensor(t)\n",
    "\n",
    "        rvs = torch.squeeze(torch.argwhere(t == torch.max(t)), axis=1)\n",
    "        if len(rvs) == 0:\n",
    "            return random.randint(0, len(node.children)-1)\n",
    "        return int(random.choice(rvs))\n",
    "\n",
    "    def default_search(self, node):\n",
    "        \"\"\"\n",
    "        If node is fully explored (neither child is None), return True\n",
    "        Otherwise, initialize value of a random unexplored next state\n",
    "\n",
    "        :param node: node to search from\n",
    "        :return: if fully explored, True. Else, value of the random unexplored next state\n",
    "        \"\"\"\n",
    "        possible = []\n",
    "        for i in range(len(node.children)):\n",
    "            if node.children[i] is None:\n",
    "                possible.append(i)\n",
    "        if len(possible) == 0:\n",
    "            return True\n",
    "\n",
    "        i = random.choice(possible)\n",
    "        # if unexplored or non-terminal, get value\n",
    "        state = torch.tensor(self.actions[i](node.state.flatten()), dtype=torch.float)\n",
    "        state = state.reshape(node.state.shape)\n",
    "        # child_val = self.value_fn(state) - node.depth - 1  # give penalty -1 for each additional step taken\n",
    "        child_val = self.value_fn(state)\n",
    "        child_val = child_val.flatten()[0]\n",
    "        node.children[i] = Node(node, state, len(self.actions), value=child_val, depth=node.depth+1)\n",
    "\n",
    "        # if new Node is terminal, take it as the tree's terminal if it takes less time to reach than current terminal\n",
    "        # if node.children[i].is_terminal:\n",
    "        #     # if terminal, add reward of ||start_vec||_2^2\n",
    "        #     node.children[i].value += torch.linalg.vector_norm(torch.square(self.root.state)).item()\n",
    "        #     if self.terminal is None or node.children[i].depth < self.terminal.depth:\n",
    "        #         self.terminal = node.children[i]\n",
    "\n",
    "        if node.children[i].depth > self.max_depth:\n",
    "            self.max_depth = node.children[i].depth\n",
    "        return node.children[i]\n",
    "\n",
    "    def tree_policy(self, node, computations):\n",
    "        while node.is_terminal is False:\n",
    "            explored = self.default_search(node)\n",
    "            if explored is not True:\n",
    "                return explored, computations + 1\n",
    "            node = node.children[self.pick_child(node)]\n",
    "            # node = random.choice(node.children)\n",
    "        return node, computations + 1\n",
    "\n",
    "    def mean_prop(self, node):\n",
    "        \"\"\"\n",
    "        Backprop up from a leaf, where subtree_value is the average of a node's rewards and its subtree's rewards\n",
    "\n",
    "        :param node: of subtree\n",
    "        \"\"\"\n",
    "        node.subtree_value = torch.zeros(1)\n",
    "        node.subtree_value += node.value\n",
    "        valid_children = 0\n",
    "        if not node.is_leaf():\n",
    "            for i in node.children:\n",
    "                if i is None:\n",
    "                    continue\n",
    "                node.subtree_value += self.k_weight * i.subtree_value\n",
    "                valid_children += 1\n",
    "        node.subtree_value /= valid_children + 1\n",
    "        node.visits += 1\n",
    "        if node.parent is None:\n",
    "            return\n",
    "        self.mean_prop(node.parent)\n",
    "\n",
    "    def run(self, root, comp_limit=10):\n",
    "        \"\"\"\n",
    "        Shoutout \"A Survey of MCTS Methods\"\n",
    "        :param root: the current state\n",
    "        :param comp_limit: max number of possible future scenarios to compute (carries over)\n",
    "        :return: index corresponding to best action\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        if self.root.is_terminal:\n",
    "            return True\n",
    "        comps = 0\n",
    "        while comps < comp_limit:\n",
    "            node, comps = self.tree_policy(self.root, comps)\n",
    "            self.mean_prop(node)\n",
    "\n",
    "        rv = self.pick_child(self.root)\n",
    "        \n",
    "        if False:\n",
    "            print(\"root state:\", root.state)\n",
    "            print(\"child states: \",end=\"\")\n",
    "            for child in root.children:\n",
    "                print(child.state, end=\",\")\n",
    "            print()\n",
    "        return rv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "438fdf1c04168299",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Loss, self).__init__()\n",
    "        self.v_loss_fn = torch.nn.MSELoss()\n",
    "        self.p_loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, v_out, v_target, p_out, p_target):\n",
    "        \"\"\"\n",
    "        Loss function designed to reward successful game completion while taking the least amount of steps possible\n",
    "        Adapted from:\n",
    "            - \"Mastering the game of Go without human knowledge\" (Silver et al)\n",
    "            - \"Discovering faster matrix multiplication algorithms with reinforcement learning\" (Fawzi et al)\n",
    "\n",
    "        :param v_out: the value outputed for the state by NN\n",
    "        :param p_out: the policy outputed for the state by NN\n",
    "        :param v_target: target value output\n",
    "        :return: total loss\n",
    "        \"\"\"\n",
    "        loss = self.v_loss_fn(v_out, v_target)\n",
    "        loss += self.p_loss_fn(p_out, p_target)\n",
    "        return loss\n",
    "\n",
    "\n",
    "        \n",
    "class ValueNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ValueNN, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.stack = nn.Sequential(\n",
    "            nn.Linear(2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "        )\n",
    "        self.value_activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.stack(x).flatten()\n",
    "        # value = self.value_activation(x[0:1]).unsqueeze(0)\n",
    "        value = x[0:1].reshape((1,1))\n",
    "        return value\n",
    "\n",
    "\n",
    "class PolicyNN(nn.Module):\n",
    "    def __init__(self, n_actions):\n",
    "        super(PolicyNN, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.stack = nn.Sequential(\n",
    "            nn.Linear(2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, n_actions),\n",
    "        )\n",
    "        self.policy_activation = nn.Softmax(dim=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.policy_activation(self.stack(x).flatten())\n",
    "        # value = self.value_activation(x[0:1]).unsqueeze(0)\n",
    "        policy = torch.clamp(x.unsqueeze(0),min=1e-8,max=1-(1e-8))\n",
    "        return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f19bdb93248565c7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 \t\tLoss: 427.5707092285156\n",
      "Epoch: 20 \t\tLoss: 51.128414154052734\n",
      "Epoch: 30 \t\tLoss: 49.24557876586914\n",
      "Epoch: 40 \t\tLoss: 47.562469482421875\n",
      "Epoch: 50 \t\tLoss: 48.49505615234375\n",
      "Epoch: 60 \t\tLoss: 48.519386291503906\n",
      "Epoch: 70 \t\tLoss: 67.93046569824219\n",
      "Epoch: 80 \t\tLoss: 83.16307067871094\n",
      "Epoch: 90 \t\tLoss: 90.97821044921875\n",
      "Epoch: 100 \t\tLoss: 101.3854751586914\n",
      "Epoch: 110 \t\tLoss: 115.19110870361328\n",
      "Epoch: 120 \t\tLoss: 98.37451934814453\n",
      "Epoch: 130 \t\tLoss: 65.6265640258789\n",
      "Epoch: 140 \t\tLoss: 207.3758544921875\n",
      "Epoch: 150 \t\tLoss: 69.74079895019531\n",
      "Epoch: 160 \t\tLoss: 55.3030891418457\n",
      "Epoch: 170 \t\tLoss: 33.931697845458984\n",
      "Epoch: 180 \t\tLoss: 26.16014289855957\n",
      "Epoch: 190 \t\tLoss: 25.88953399658203\n",
      "Epoch: 200 \t\tLoss: 15.620043754577637\n",
      "Epoch: 210 \t\tLoss: 166.25595092773438\n",
      "Epoch: 220 \t\tLoss: 25.160869598388672\n",
      "Epoch: 230 \t\tLoss: 19.127498626708984\n",
      "Epoch: 240 \t\tLoss: 6.065675735473633\n",
      "Epoch: 250 \t\tLoss: 24.917194366455078\n",
      "Epoch: 260 \t\tLoss: 55.45851516723633\n",
      "Epoch: 270 \t\tLoss: 27.791473388671875\n",
      "Epoch: 280 \t\tLoss: 14.547812461853027\n",
      "Epoch: 290 \t\tLoss: 8.943713188171387\n",
      "Epoch: 300 \t\tLoss: 0.7443438768386841\n",
      "Epoch: 310 \t\tLoss: 5.249520301818848\n",
      "Epoch: 320 \t\tLoss: 4.472096920013428\n",
      "Epoch: 330 \t\tLoss: 0.36160537600517273\n",
      "Epoch: 340 \t\tLoss: 16.101825714111328\n",
      "Epoch: 350 \t\tLoss: 358.6707458496094\n",
      "Epoch: 360 \t\tLoss: 6.98356294631958\n",
      "Epoch: 370 \t\tLoss: 3.118889093399048\n",
      "Epoch: 380 \t\tLoss: 1.1329046487808228\n",
      "Epoch: 390 \t\tLoss: 29.242324829101562\n",
      "Epoch: 400 \t\tLoss: 42.916961669921875\n",
      "Epoch: 410 \t\tLoss: 30.396560668945312\n",
      "Epoch: 420 \t\tLoss: 2.1808290481567383\n",
      "Epoch: 430 \t\tLoss: 4.347366809844971\n",
      "Epoch: 440 \t\tLoss: 0.43777742981910706\n",
      "Epoch: 450 \t\tLoss: 27.283782958984375\n",
      "Epoch: 460 \t\tLoss: 2.066505193710327\n",
      "Epoch: 470 \t\tLoss: 8.352314949035645\n",
      "Epoch: 480 \t\tLoss: 2.3691041469573975\n",
      "Epoch: 490 \t\tLoss: 0.35015884041786194\n",
      "Epoch: 500 \t\tLoss: 20.191959381103516\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "k_state_upper_lim = 30 # arbitrary\n",
    "k_C = 1 / math.sqrt(2)\n",
    "k_comp_limit = int(k_state_upper_lim ** (3/2))\n",
    "\n",
    "k_actions = (a_subtract, a_swap)\n",
    "\n",
    "value_fn = ValueNN()\n",
    "policy_fn = PolicyNN(len(k_actions))\n",
    "loss_fn = Loss()\n",
    "value_optim = optim.Adam(value_fn.parameters(), lr=0.0001)\n",
    "policy_optim = optim.Adam(policy_fn.parameters(), lr=0.0001)\n",
    "\n",
    "def gen_start_state_2a(limit):\n",
    "    return torch.round(torch.rand((1, 2)) * limit + 1).float()\n",
    "\n",
    "def train(epochs):\n",
    "    for t in range(epochs):\n",
    "        value_optim.zero_grad()\n",
    "        policy_optim.zero_grad()\n",
    "        # Repeat the following:\n",
    "        # 1) run the NN on some random initial state\n",
    "        # 2) update the NN based off performance in that game\n",
    "        mcts = MCTS(k_actions, C=k_C, weight=1, value_fn=value_fn)\n",
    "        start = gen_start_state_2a(k_state_upper_lim)\n",
    "\n",
    "        value = mcts.value_fn(start).flatten()\n",
    "        policy = policy_fn(start).flatten()\n",
    "\n",
    "        start_node = Node(None, start, len(k_actions), value, 0)\n",
    "\n",
    "        # play out a game\n",
    "        mcts.run(start_node, comp_limit=k_comp_limit)\n",
    "\n",
    "        # get attributes of game just played\n",
    "        v_out = start_node.subtree_value\n",
    "        v_target = -mcts.max_depth\n",
    "        if mcts.terminal is not None:\n",
    "            v_target = -mcts.terminal.depth + torch.linalg.norm(start)\n",
    "\n",
    "        visits = []\n",
    "        for i in start_node.children:\n",
    "            if i is None:\n",
    "                visits.append(0)\n",
    "            else:\n",
    "                visits.append(i.visits)\n",
    "        visits = torch.tensor(visits, dtype=torch.float)\n",
    "        p_sampled = visits / torch.sum(visits)\n",
    "        \n",
    "        loss = loss_fn(v_out, torch.tensor(v_target,dtype=v_out.dtype), policy.flatten(), p_sampled.flatten())\n",
    "        loss.backward()\n",
    "        value_optim.step()\n",
    "        policy_optim.step()\n",
    "\n",
    "        if (t+1) % 10 == 0:\n",
    "            print(\"Epoch:\", t+1,\"\\t\\tLoss:\",loss.item())\n",
    "\n",
    "train(500)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "save = False\n",
    "if save:\n",
    "    torch.save(value_fn.state_dict(), \"deep_mcts_v_weights.pth\")\n",
    "    torch.save(policy_fn.state_dict(), \"deep_mcts_p_weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c191e6c845a0bd24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T02:47:31.912513Z",
     "start_time": "2023-11-06T02:47:31.904073Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_data(fname):\n",
    "    x = torch.tensor(np.loadtxt(fname, delimiter=\",\"), dtype=torch.float)\n",
    "    return x[:,:-1], x[:,-1]\n",
    "\n",
    "def plot_db(policy_fn, actions, comp_limit, ranges):\n",
    "    X = ranges[0]\n",
    "    Y = ranges[1]\n",
    "    action_plot = [[] * actions]\n",
    "    for i in X:\n",
    "        for j in Y:\n",
    "            action_plot[policy_fn(torch.tensor(i,j))].append((i,j))\n",
    "    for i in range(len(action_plot)):\n",
    "        action = np.array(action_plot[i])\n",
    "        plt.scatter(action[:,0], action[:,1], color=(\"C\"+str(i)), label=action)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1368c1aa3c071299",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T02:48:10.179905Z",
     "start_time": "2023-11-06T02:48:10.175988Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test(x, y, C, policy_fn, weight=1., comp_limit=10, actions=(a_subtract, a_swap), zero_index=False, dbs=None):\n",
    "    correct = 0\n",
    "    mcts = MCTS(actions, C, weight, value_fn)\n",
    "    guess_dist = [0] * len(actions)\n",
    "    if zero_index:\n",
    "        y = y - np.ones(len(y))\n",
    "    for i in range(len(x)):\n",
    "        state = torch.tensor(x[i]).unsqueeze(0)\n",
    "        rv = policy_fn(state).flatten()                      # take the move distribution given by NN\n",
    "\n",
    "        # todo pick one way to select\n",
    "        # rv = rv.multinomial(num_samples=1, replacement=True)    # sample from the move distribution\n",
    "        rv = torch.argmax(rv)\n",
    "\n",
    "        if rv == y[i]:\n",
    "            correct += 1\n",
    "        guess_dist[rv] += 1\n",
    "        # if (i+1) % 100 == 0:\n",
    "            # print(\"epoch\", i+1, \":\", correct / (i+1))\n",
    "    # todo fix\n",
    "    # if dbs is not None:\n",
    "    #     # graphing decision boundary\n",
    "    #     plot_db(policy_fn, actions, comp_limit, ranges=dbs)\n",
    "    return correct / len(x), guess_dist\n",
    "\n",
    "\n",
    "def run_test(data_name, actions, C, policy_fn, cases=100, lookahead=100, weight=1., zero_index=False, dbs=None):\n",
    "    test_X, test_Y = get_data(data_name)\n",
    "    test_Y.reshape(-1, 1)\n",
    "\n",
    "    acc, guesses = test(test_X[:cases], test_Y[:cases],\n",
    "                        C, policy_fn, weight, comp_limit=lookahead, actions=actions, zero_index=zero_index, dbs=dbs)\n",
    "    print(\"Test Accuracy:\", acc)\n",
    "    print(\"Guess Distribution:\", guesses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d034573cc3f5050",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T02:45:36.106299Z",
     "start_time": "2023-11-06T02:45:36.096467Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k_C = 1 / math.sqrt(2)  # satisfies Hoeffding Ineq (Kocsis and Szepesvari)\n",
    "k_cases = 2000\n",
    "\n",
    "k_dbound_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "caa3d0f193cdda2a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t9/gxb6q8zj21dff090q066td740000gn/T/ipykernel_2763/1659889107.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(x[i]).unsqueeze(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9905\n",
      "Guess Distribution: [1039, 961]\n"
     ]
    }
   ],
   "source": [
    "dual_file = \"test_data/test_simple.csv\"\n",
    "\n",
    "db2 = np.linspace(2, k_dbound_size, k_dbound_size - 1)\n",
    "two_dbs =[db2, db2]\n",
    "\n",
    "run_test(dual_file, [a_subtract, a_swap], C=k_C, policy_fn=policy_fn, cases=k_cases, lookahead=10, dbs=two_dbs)\n",
    "# ~99% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb7c4b36c7d9707",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# quad_file = \"../Donald/four_step_euclidean/four_directions_cleaner_test.csv\"     # thanks, donald\n",
    "\n",
    "# k_C = 1 / math.sqrt(2)  # satisfies Hoeffding Ineq (Kocsis and Szepesvari)\n",
    "# k_cases = 10\n",
    "\n",
    "# k_dbound_size = 100\n",
    "\n",
    "# db4 = np.linspace(-k_dbound_size/2, k_dbound_size/2, k_dbound_size+1)\n",
    "# quad_dbs = [db4, db4]\n",
    "# run_test(quad_file, [a_plsy, a_suby, a_plsx, a_subx], k_C, k_cases, lookahead=100, zero_index=False)\n",
    "# # 8% accuracy on Donald test csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6aeaf873defc84",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-06T02:45:00.363403Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
