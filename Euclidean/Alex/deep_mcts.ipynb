{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T00:28:23.047407Z",
     "start_time": "2024-02-28T00:28:03.339254Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch import autograd\n",
    "import threading\n",
    "import concurrent\n",
    "\n",
    "import math\n",
    "\n",
    "from util import *\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "603bf730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nogil=True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nNOTE: RUN WITH NOGIL https://github.com/colesbury/nogil FOR MUCH BETTER PERFORMANCE\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "print(f\"nogil={getattr(sys.flags, 'nogil', False)}\")\n",
    "\n",
    "\"\"\"\n",
    "NOTE: RUN WITH NOGIL https://github.com/colesbury/nogil FOR MUCH BETTER PERFORMANCE\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d75afc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T00:29:36.547536Z",
     "start_time": "2024-02-28T00:29:36.538039Z"
    }
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, parent, state, n_children, value, depth=0):\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.visits = 0\n",
    "        self.depth = depth\n",
    "        self.children = [None] * n_children\n",
    "        self.is_terminal = terminal(self.state)\n",
    "        self.value = value\n",
    "        self.subtree_value = torch.zeros(1).to(device)\n",
    "\n",
    "        # for more on virtual loss/shared tree search, see \"Parallel MCTS\" by Chaslot et al. \n",
    "        # https://dke.maastrichtuniversity.nl/m.winands/documents/multithreadedMCTS2.pdf\n",
    "        self.active_threads = 0\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        return (\"State: \" + str(self.state) + \"; Value: \" + str(self.value)\n",
    "                + \"; Subtree Value: \" + str(self.subtree_value) + \"; Visits:\", str(self.visits))\n",
    "\n",
    "    def is_leaf(self):\n",
    "        for i in self.state:\n",
    "            if i is not None:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "# override the one in util.py\n",
    "def UCT_fn(child, C):\n",
    "    if child.visits == 0:\n",
    "        return math.inf\n",
    "    return child.subtree_value + 2 * C * math.sqrt(2 * math.log2(child.parent.visits) / child.visits) - child.active_threads\n",
    "    \n",
    "class MCTS:\n",
    "    def __init__(self, actions, C, weight, value_fn):\n",
    "        self.actions = actions\n",
    "        self.k_C = C\n",
    "        self.k_weight = weight\n",
    "        self.value_fn = value_fn\n",
    "        self.max_depth = 0\n",
    "        self.terminal = None    # None if no terminal state found; terminal Node if found\n",
    "        self.root = None\n",
    "        self.propagation_lock = threading.Lock()\n",
    "\n",
    "    def pick_child(self, node):\n",
    "        # UCT\n",
    "        t = []\n",
    "        for i in node.children:\n",
    "            if i is None:\n",
    "                continue\n",
    "            t.append(UCT_fn(i, self.k_C))\n",
    "\n",
    "        if len(t) == 0:\n",
    "            return random.randint(0, len(node.children)-1)\n",
    "        \n",
    "        t = torch.tensor(t)\n",
    "\n",
    "        rvs = torch.squeeze(torch.argwhere(t == torch.max(t)), axis=1)\n",
    "        return int(random.choice(rvs))\n",
    "\n",
    "    def default_search(self, node):\n",
    "        \"\"\"\n",
    "        If node is fully explored (neither child is None), return True\n",
    "        Otherwise, initialize value of a random unexplored next state\n",
    "\n",
    "        :param node: node to search from\n",
    "        :return: if fully explored, True. Else, value of the random unexplored next state\n",
    "        \"\"\"\n",
    "        possible = []\n",
    "        for i in range(len(node.children)):\n",
    "            if node.children[i] is None:\n",
    "                possible.append(i)\n",
    "        if len(possible) == 0:\n",
    "            return True\n",
    "\n",
    "        i = random.choice(possible)\n",
    "        # if unexplored or non-terminal, get value\n",
    "        state = self.actions[i](node.state.flatten()).float().to(device)\n",
    "        state = state.reshape(node.state.shape)\n",
    "        # child_val = self.value_fn(state) - node.depth - 1  # give penalty -1 for each additional step taken\n",
    "        child_val = self.value_fn(state)\n",
    "        child_val = child_val.flatten()[0]\n",
    "\n",
    "        with node.lock:\n",
    "            node.children[i] = Node(node, state, len(self.actions), value=child_val, depth=node.depth+1)\n",
    "\n",
    "        # if new Node is terminal, take it as the tree's terminal if it takes less time to reach than current terminal\n",
    "        # if node.children[i].is_terminal:\n",
    "        #     # if terminal, add reward of ||start_vec||_2^2\n",
    "        #     node.children[i].value += torch.linalg.vector_norm(torch.square(self.root.state)).item()\n",
    "        #     if self.terminal is None or node.children[i].depth < self.terminal.depth:\n",
    "        #         self.terminal = node.children[i]\n",
    "\n",
    "        with self.propagation_lock:\n",
    "            if node.children[i].depth > self.max_depth:\n",
    "                self.max_depth = node.children[i].depth\n",
    "        return node.children[i]\n",
    "\n",
    "    def tree_policy(self, node):\n",
    "        prev = None\n",
    "        while node.is_terminal is False:\n",
    "            # add some virtual loss to the node for each thread that's exploring (released after back propagating)\n",
    "            with node.lock:\n",
    "                node.active_threads += 1\n",
    "\n",
    "            explored = self.default_search(node)\n",
    "            if explored is not True:\n",
    "                return explored\n",
    "            node = node.children[self.pick_child(node)]\n",
    "            prev = node\n",
    "            # node = random.choice(node.children)\n",
    "        return prev\n",
    "\n",
    "    def mean_prop(self, node):\n",
    "        \"\"\"\n",
    "        Backprop up from a leaf, where subtree_value is the average of a node's rewards and its subtree's rewards\n",
    "\n",
    "        :param node: of subtree\n",
    "        \"\"\"\n",
    "        with node.lock:\n",
    "            node.subtree_value = torch.zeros(1).to(device)\n",
    "            node.subtree_value += node.value\n",
    "            valid_children = 0\n",
    "            if not node.is_leaf():\n",
    "                for i in node.children:\n",
    "                    if i is None:\n",
    "                        continue\n",
    "                    node.subtree_value += self.k_weight * i.subtree_value\n",
    "                    valid_children += 1\n",
    "            node.subtree_value /= valid_children + 1\n",
    "            node.visits += 1\n",
    "\n",
    "            # remove virtual loss from node after thread done exploring its subtree\n",
    "            node.active_threads -= 1\n",
    "\n",
    "            if node.parent is None:\n",
    "                return\n",
    "        self.mean_prop(node.parent)\n",
    "\n",
    "    def explore_once(self, number):\n",
    "        node = self.tree_policy(self.root)\n",
    "        with self.propagation_lock:\n",
    "            self.mean_prop(node)\n",
    "        return number\n",
    "\n",
    "    def run(self, root, comp_limit=10, max_threads=5):\n",
    "        \"\"\"\n",
    "        Shoutout \"A Survey of MCTS Methods\"\n",
    "        :param root: the current state\n",
    "        :param comp_limit: max number of possible future scenarios to compute (carries over)\n",
    "        :return: index corresponding to best action\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        if self.root.is_terminal:\n",
    "            return True\n",
    "\n",
    "        # spawn new thread for each computation\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_threads) as executor:\n",
    "            executor.map(self.explore_once, range(comp_limit))\n",
    "\n",
    "        rv = self.pick_child(self.root)\n",
    "\n",
    "        if False:\n",
    "            print(\"root state:\", root.state)\n",
    "            print(\"child states: \",end=\"\")\n",
    "            for child in root.children:\n",
    "                print(child.state, end=\",\")\n",
    "            print()\n",
    "        return rv\n",
    "    \n",
    "    def generate(self, init_state, actions):\n",
    "        self.root = Node(None, init_state, n_children=len(self.actions), value=self.value_fn(init_state), depth=0)\n",
    "        curr = self.root\n",
    "        r_nodes = []\n",
    "        for i in actions:\n",
    "            newstate = self.actions[i](curr.state)\n",
    "            n = Node(parent=curr,\n",
    "                     state=newstate,\n",
    "                     n_children=len(self.actions),\n",
    "                     value=self.value_fn(newstate),\n",
    "                     depth=curr.depth + 1)\n",
    "            curr.children[i] = n\n",
    "            curr = n            \n",
    "            r_nodes.append(n)\n",
    "        return r_nodes\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "438fdf1c04168299",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T00:29:37.378811Z",
     "start_time": "2024-02-28T00:29:37.370384Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Loss, self).__init__()\n",
    "        self.v_loss_fn = torch.nn.MSELoss()\n",
    "        self.p_loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, v_out, v_target, p_out, p_target):\n",
    "        \"\"\"\n",
    "        Loss function designed to reward successful game completion while taking the least amount of steps possible\n",
    "        Adapted from:\n",
    "            - \"Mastering the game of Go without human knowledge\" (Silver et al)\n",
    "            - \"Discovering faster matrix multiplication algorithms with reinforcement learning\" (Fawzi et al)\n",
    "\n",
    "        :param v_out: the value outputed for the state by NN\n",
    "        :param p_out: the policy outputed for the state by NN\n",
    "        :param v_target: target value output\n",
    "        :return: total loss\n",
    "        \"\"\"\n",
    "        loss = self.v_loss_fn(v_out, v_target)\n",
    "        loss += self.p_loss_fn(p_out, p_target).sum()\n",
    "        return loss\n",
    "\n",
    "\n",
    "class ValueNN(nn.Module):\n",
    "    def __init__(self, state_size):\n",
    "        super(ValueNN, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.stack = nn.Sequential(\n",
    "            nn.Linear(state_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "        )\n",
    "        self.value_activation = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "#        x = self.flatten(x)\n",
    "#        x = self.stack(x).flatten()\n",
    "#        value = x[0:1].reshape((1,1))\n",
    "#        return value\n",
    "        return self.stack(x)\n",
    "\n",
    "\n",
    "class PolicyNN(nn.Module):\n",
    "    def __init__(self, state_size, n_actions):\n",
    "        super(PolicyNN, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.stack = nn.Sequential(\n",
    "            nn.Linear(state_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, n_actions),\n",
    "        )\n",
    "        self.policy_activation = nn.Softmax(dim=1)\n",
    "    def forward(self, x):\n",
    "        x = self.policy_activation(self.stack(x))#.flatten())\n",
    "        policy = torch.clamp(x,min=1e-8,max=1-(1e-7))\n",
    "        return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05e3ddf7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T00:29:38.334723Z",
     "start_time": "2024-02-28T00:29:38.325718Z"
    }
   },
   "outputs": [],
   "source": [
    "# Memory for better batching\n",
    "\n",
    "class Memory:\n",
    "    def __init__(self, width) -> None:\n",
    "        self.mem_ = None\n",
    "        self.len_ = 0\n",
    "    def record(self, obs):\n",
    "        if self.len_ == 0:\n",
    "            self.mem_ = obs\n",
    "        else:\n",
    "            self.mem_ = torch.cat((self.mem_, obs), dim=0)\n",
    "        self.len_ += 1\n",
    "    def recall(self, n_samples):\n",
    "        if self.len_ == 0:\n",
    "            return None\n",
    "        des_len = min(n_samples, self.len_)\n",
    "        indices = torch.ones(self.mem_.shape[0]).multinomial(des_len, replacement=False)\n",
    "        return self.mem_[indices]\n",
    "    def size(self):\n",
    "        return self.len_\n",
    "    def clear(self):\n",
    "        self.len_ = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f19bdb93248565c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T00:29:39.699250Z",
     "start_time": "2024-02-28T00:29:39.688411Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "k_C = 1 / math.sqrt(2)\n",
    "k_thread_count_limit = 20\n",
    "\n",
    "def get_train_data(fname):\n",
    "    x = np.loadtxt(fname, delimiter=\",\")\n",
    "    return torch.tensor([x[:,2], x[:,2:]], dtype=torch.float)\n",
    "\n",
    "def get_nonterm_rwd(mcts):\n",
    "    return -mcts.max_depth\n",
    "\n",
    "def get_terminal_rwd(terminal_depth, start):\n",
    "    return -terminal_depth + torch.linalg.norm(start)\n",
    "\n",
    "def train_sv(epochs, actions, policy_fn, value_fn, optimizers, fname, batch_size=10):\n",
    "    k_mem_width = 4    # statex,statey,action,subtree_value\n",
    "    memory = Memory(k_mem_width)\n",
    "    loss_fn = Loss()\n",
    "    # load data into memory\n",
    "    with open(fname, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            entry = torch.tensor(list(map(int, line.split(','))))\n",
    "            tree = MCTS(actions, C=k_C, weight=1, value_fn=value_fn)\n",
    "            g_nodes = tree.generate(entry[0:2].unsqueeze(0).float(), entry[2:])\n",
    "            for i in range(len(entry) - 2): # go by actions (so we disregard the terminal node)\n",
    "                memory.record(torch.cat((g_nodes[i].state.reshape(1,2), \n",
    "                                         entry[2+i].reshape((1,1)), \n",
    "                                         g_nodes[i].subtree_value.reshape(1,1)), dim=1))\n",
    "    # train off memory\n",
    "    for t in range(epochs):\n",
    "        for o in optimizers:\n",
    "            o.zero_grad()\n",
    "        batch = memory.recall(batch_size)\n",
    "        v_out = value_fn(batch[:,:2])\n",
    "        p_out = policy_fn(batch[:,:2])\n",
    "\n",
    "        # one-hot encode actions; e.g. convert 3 -> (0,0,0,1)\n",
    "        action_indices = batch[:,2:-1].to(torch.int64)\n",
    "        p_target = oh_encode(action_indices, len(actions))\n",
    "        # p_target = torch.zeros(action_indices.shape[0],len(actions))\n",
    "        # p_target.scatter_(1, action_indices,1)\n",
    "\n",
    "        v_target = batch[:,-1]\n",
    "\n",
    "        loss = loss_fn(v_out.view(v_target.shape), v_target, p_out.view(p_target.shape), p_target)\n",
    "        loss.backward()\n",
    "\n",
    "        for o in optimizers:\n",
    "            o.step()\n",
    "    \n",
    "        if (t+1) % 10 == 0:\n",
    "            print(\"Epoch:\", t+1,\"\\t\\tLoss:\",loss.item())\n",
    "\n",
    "\n",
    "def train_play(epochs, actions, policy_fn, value_fn, optimizers, rand_start_state_fn, comp_limit, batch_size=16):\n",
    "    history = Memory(2+len(actions))    # [stateX,stateY,pr1,pr2,...pr(len(actions))] (probs are sampled probs)\n",
    "    loss_fn = Loss()\n",
    "    for t in range(epochs):\n",
    "        for o in optimizers:\n",
    "            o.zero_grad()\n",
    "        # Repeat the following:\n",
    "        # 1) run the NN on some random initial state\n",
    "        # 2) update the NN based off performance in that game\n",
    "            \n",
    "        # play out some games\n",
    "        k_comp_limit = comp_limit(t / epochs)\n",
    "        for game in range(batch_size):\n",
    "            mcts = MCTS(actions, C=k_C, weight=1, value_fn=value_fn)\n",
    "            start = rand_start_state_fn().to(device)\n",
    "\n",
    "            value = mcts.value_fn(start).flatten().to(device)\n",
    "            policy = policy_fn(start).flatten().to(device)\n",
    "\n",
    "            start_node = Node(None, start, len(actions), value, 0)\n",
    "\n",
    "            mcts.run(start_node, comp_limit=k_comp_limit, max_threads=k_thread_count_limit)\n",
    "\n",
    "\n",
    "            # get attributes of game just played\n",
    "            v_out = start_node.subtree_value.to(device)\n",
    "            v_target = get_nonterm_rwd(mcts)\n",
    "            if mcts.terminal is not None:\n",
    "                v_target = get_terminal_rwd(mcts.terminal.depth, start)\n",
    "            v_target = torch.tensor(v_target,dtype=v_out.dtype).to(device)\n",
    "\n",
    "\n",
    "            visits = []\n",
    "            for i in start_node.children:\n",
    "                if i is None:\n",
    "                    visits.append(0)\n",
    "                else:\n",
    "                    visits.append(i.visits)\n",
    "            visits = torch.tensor(visits, dtype=torch.float).to(device)\n",
    "            p_sampled = visits / torch.sum(visits)\n",
    "\n",
    "            curr_batch_entry = torch.cat((start,p_sampled.flatten().unsqueeze(0)),dim=1)\n",
    "            history.record(curr_batch_entry)\n",
    "\n",
    "        # train NN on games just played\n",
    "        batch = history.recall(batch_size)\n",
    "        batch_states = batch[:,:2]\n",
    "        batch_psampled = batch[:,2:]\n",
    "        \n",
    "        loss = loss_fn(v_out, v_target,\n",
    "                       policy_fn(batch_states).reshape(batch_psampled.shape), batch_psampled)\n",
    "        loss.backward()\n",
    "\n",
    "        history.clear()\n",
    "\n",
    "        for o in optimizers:\n",
    "            o.step()\n",
    "\n",
    "\n",
    "        if (t+1) % 10 == 0:\n",
    "            print(\"Epoch:\", t+1,\"\\t\\tLoss:\",loss.item())\n",
    "            # if torch.isnan(p_loss):\n",
    "            #     print(\"value\",v_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e004f8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T00:29:40.566311Z",
     "start_time": "2024-02-28T00:29:40.556759Z"
    }
   },
   "outputs": [],
   "source": [
    "k_state_upper_lim = 30 # arbitrary\n",
    "k_comp_limit = int(k_state_upper_lim ** (3/2))\n",
    "k_min_comps = int(k_state_upper_lim ** (1.1))\n",
    "\n",
    "value_fn_2 = ValueNN(2).to(device)\n",
    "policy_fn_2 = PolicyNN(2,len(k_2actions)).to(device)\n",
    "value_optim = optim.Adam(value_fn_2.parameters(), lr=0.00005)\n",
    "policy_optim = optim.Adam(policy_fn_2.parameters(), lr=0.000005)\n",
    "\n",
    "def gen_start_state_2a():\n",
    "    limit = k_state_upper_lim\n",
    "    return torch.round(torch.rand((1, 2)) * limit + 1).float()\n",
    "\n",
    "def adaptive_comp_limit(frac_epochs):\n",
    "    # linearly decrease computation limit as model becomes better over time\n",
    "    rv = k_comp_limit - (k_comp_limit - k_min_comps) * frac_epochs\n",
    "    return int(rv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41211b20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T02:48:18.942268Z",
     "start_time": "2024-02-28T02:48:12.374093Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayun/.pyenv/versions/nogil-3.9.10-1/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 \t\tLoss: 61.50035858154297\n",
      "Epoch: 20 \t\tLoss: 59.00404739379883\n",
      "Epoch: 30 \t\tLoss: 40.618141174316406\n",
      "Epoch: 40 \t\tLoss: 44.178367614746094\n",
      "Epoch: 50 \t\tLoss: 47.50703811645508\n",
      "Epoch: 60 \t\tLoss: 31.140111923217773\n",
      "Epoch: 70 \t\tLoss: 40.70294952392578\n",
      "Epoch: 80 \t\tLoss: 32.138755798339844\n",
      "Epoch: 90 \t\tLoss: 20.571725845336914\n",
      "Epoch: 100 \t\tLoss: 37.226905822753906\n",
      "Epoch: 110 \t\tLoss: 21.974000930786133\n",
      "Epoch: 120 \t\tLoss: 26.82094955444336\n",
      "Epoch: 130 \t\tLoss: 20.364957809448242\n",
      "Epoch: 140 \t\tLoss: 39.23075485229492\n",
      "Epoch: 150 \t\tLoss: 20.486770629882812\n",
      "Epoch: 160 \t\tLoss: 22.066242218017578\n",
      "Epoch: 170 \t\tLoss: 10.248380661010742\n",
      "Epoch: 180 \t\tLoss: 7.751731872558594\n",
      "Epoch: 190 \t\tLoss: 27.434144973754883\n",
      "Epoch: 200 \t\tLoss: 6.5898308753967285\n",
      "Epoch: 210 \t\tLoss: 5.702095985412598\n",
      "Epoch: 220 \t\tLoss: 25.708667755126953\n",
      "Epoch: 230 \t\tLoss: 30.828908920288086\n",
      "Epoch: 240 \t\tLoss: 0.9221897721290588\n",
      "Epoch: 250 \t\tLoss: 23.344585418701172\n",
      "Epoch: 260 \t\tLoss: 29.399446487426758\n",
      "Epoch: 270 \t\tLoss: 7.5988450050354\n",
      "Epoch: 280 \t\tLoss: 4.917426109313965\n",
      "Epoch: 290 \t\tLoss: 4.099639892578125\n",
      "Epoch: 300 \t\tLoss: 0.7293728590011597\n",
      "Epoch: 310 \t\tLoss: 1.362203598022461\n",
      "Epoch: 320 \t\tLoss: 7.982805252075195\n",
      "Epoch: 330 \t\tLoss: 6.464631080627441\n",
      "Epoch: 340 \t\tLoss: 4.816525936126709\n",
      "Epoch: 350 \t\tLoss: 4.354920387268066\n",
      "Epoch: 360 \t\tLoss: 0.9432228803634644\n",
      "Epoch: 370 \t\tLoss: 3.791017770767212\n",
      "Epoch: 380 \t\tLoss: 0.6942108869552612\n",
      "Epoch: 390 \t\tLoss: 2.6249730587005615\n",
      "Epoch: 400 \t\tLoss: 1.829876184463501\n",
      "Epoch: 410 \t\tLoss: 2.9840457439422607\n",
      "Epoch: 420 \t\tLoss: 10.115006446838379\n",
      "Epoch: 430 \t\tLoss: 1.5356870889663696\n",
      "Epoch: 440 \t\tLoss: 1.4590753316879272\n",
      "Epoch: 450 \t\tLoss: 2.5703845024108887\n",
      "Epoch: 460 \t\tLoss: 1.1253893375396729\n",
      "Epoch: 470 \t\tLoss: 2.0734171867370605\n",
      "Epoch: 480 \t\tLoss: 0.6965785026550293\n",
      "Epoch: 490 \t\tLoss: 4.164518356323242\n",
      "Epoch: 500 \t\tLoss: 1.1194591522216797\n"
     ]
    }
   ],
   "source": [
    "# synthetic data\n",
    "train_play(epochs=500, actions=k_2actions, policy_fn=policy_fn_2, value_fn=value_fn_2, \n",
    "           optimizers=[value_optim, policy_optim], rand_start_state_fn=gen_start_state_2a, \n",
    "           comp_limit=adaptive_comp_limit, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "df2ff0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "save = False\n",
    "if save:\n",
    "    torch.save(value_fn_2.state_dict(), \"trained_weights/deep_mcts_2_v_weights.pth\")\n",
    "    torch.save(policy_fn_2.state_dict(), \"trained_weights/deep_mcts_2_p_weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57f7b9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_state_upper_lim = 30 # arbitrary\n",
    "k_comp_limit = int(k_state_upper_lim ** (7/2))\n",
    "value_fn_4 = ValueNN(2).to(device)\n",
    "policy_fn_4 = PolicyNN(2,len(k_4actions)).to(device)\n",
    "value_optim_4 = optim.Adam(value_fn_4.parameters(), lr=0.00005)\n",
    "policy_optim_4 = optim.Adam(policy_fn_4.parameters(), lr=0.000005)\n",
    "\n",
    "def gen_start_state_4a():\n",
    "    limit = k_state_upper_lim\n",
    "    return torch.round( (torch.rand((1, 2)) - 0.5) * 2 * limit).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e2b8d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 \t\tLoss: 2.1033401489257812\n",
      "Epoch: 20 \t\tLoss: 1.556889295578003\n",
      "Epoch: 30 \t\tLoss: 1.3582837581634521\n",
      "Epoch: 40 \t\tLoss: 1.4088467359542847\n",
      "Epoch: 50 \t\tLoss: 1.436020016670227\n",
      "Epoch: 60 \t\tLoss: 1.379128336906433\n",
      "Epoch: 70 \t\tLoss: 1.4841262102127075\n",
      "Epoch: 80 \t\tLoss: 1.3614771366119385\n",
      "Epoch: 90 \t\tLoss: 1.3785827159881592\n",
      "Epoch: 100 \t\tLoss: 1.3600499629974365\n",
      "Epoch: 110 \t\tLoss: 1.389901041984558\n",
      "Epoch: 120 \t\tLoss: 1.383531928062439\n",
      "Epoch: 130 \t\tLoss: 1.3663393259048462\n",
      "Epoch: 140 \t\tLoss: 1.3774583339691162\n",
      "Epoch: 150 \t\tLoss: 1.2760998010635376\n",
      "Epoch: 160 \t\tLoss: 1.3536250591278076\n",
      "Epoch: 170 \t\tLoss: 1.3584808111190796\n",
      "Epoch: 180 \t\tLoss: 1.3749842643737793\n",
      "Epoch: 190 \t\tLoss: 1.3323438167572021\n",
      "Epoch: 200 \t\tLoss: 1.3520457744598389\n",
      "Epoch: 210 \t\tLoss: 1.310624599456787\n",
      "Epoch: 220 \t\tLoss: 1.3285794258117676\n",
      "Epoch: 230 \t\tLoss: 1.3432512283325195\n",
      "Epoch: 240 \t\tLoss: 1.3051739931106567\n",
      "Epoch: 250 \t\tLoss: 1.3170291185379028\n"
     ]
    }
   ],
   "source": [
    "train_sv(250, k_4actions, policy_fn=policy_fn_4, value_fn=value_fn_4, optimizers=[value_optim_4,policy_optim_4], fname='train_data/train_mcts.csv', batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e933c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_policy_fp = \"start_weights/deep_mcts_4_p_weights.pth\"\n",
    "init_value_fp = \"start_weights/deep_mcts_4_v_weights.pth\"\n",
    "trained_policy_fp = \"trained_weights/deep_mcts_4_p_weights_f.pth\"\n",
    "trained_value_fp = \"trained_weights/deep_mcts_4_v_weights_f.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4fba2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if False:\n",
    "    torch.save(value_fn_4.state_dict(), init_value_fp)\n",
    "    torch.save(policy_fn_4.state_dict(), init_policy_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fc7fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_play(epochs=100, actions=k_4actions, policy_fn=policy_fn_4, value_fn=value_fn_4, optimizers=[value_optim_4, policy_optim_4], rand_start_state_fn=gen_start_state_4a, comp_limit=k_comp_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bcbd88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test trained weights\n",
    "\n",
    "\n",
    "# value_fn_4.load_state_dict(torch.load(trained_value_fp))\n",
    "# policy_fn_4.load_state_dict(torch.load(trained_policy_fp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c191e6c845a0bd24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T02:47:31.912513Z",
     "start_time": "2023-11-06T02:47:31.904073Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_test_data(fname):\n",
    "    x = torch.tensor(np.loadtxt(fname, delimiter=\",\"), dtype=torch.float)\n",
    "    return x[:,:-1], x[:,-1]\n",
    "\n",
    "def plot_db(policy_fn, actions, ranges):\n",
    "    X = ranges[0]\n",
    "    Y = ranges[1]\n",
    "    action_plot = []\n",
    "    for i in actions:\n",
    "        action_plot.append([])\n",
    "    for i in X:\n",
    "        for j in Y:\n",
    "            rv = policy_fn(torch.tensor([i,j],dtype=torch.float).unsqueeze(0)).flatten().to(device)\n",
    "            action_plot[torch.argmax(rv)].append((i.cpu(),j.cpu()))\n",
    "    for i in range(len(action_plot)):\n",
    "        action = np.array(action_plot[i])\n",
    "        plt.scatter(action[:,0], action[:,1], color=(\"C\"+str(i)), label=action)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1368c1aa3c071299",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T02:48:10.179905Z",
     "start_time": "2023-11-06T02:48:10.175988Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test(x, y, policy_fn, actions=k_2actions, dbs=None):\n",
    "    correct = 0\n",
    "    guess_dist = [0] * len(actions)\n",
    "    for i in range(len(x)):\n",
    "        state = torch.tensor(x[i]).unsqueeze(0).to(device)\n",
    "        rv = policy_fn(state).flatten()                      # take the move distribution given by NN\n",
    "\n",
    "        # todo pick one way to select\n",
    "        # rv = rv.multinomial(num_samples=1, replacement=True)    # sample from the move distribution\n",
    "        rv = torch.argmax(rv)\n",
    "\n",
    "        if rv == y[i]:\n",
    "            correct += 1\n",
    "        guess_dist[rv] += 1\n",
    "    # todo fix\n",
    "    if dbs is not None:\n",
    "        # graphing decision boundary\n",
    "        plot_db(policy_fn, actions, ranges=dbs)\n",
    "    return correct / len(x), guess_dist\n",
    "\n",
    "\n",
    "def run_test(data_name, actions, policy_fn, cases=100, dbs=None):\n",
    "    test_X, test_Y = get_test_data(data_name)\n",
    "    test_X = test_X.to(device)\n",
    "    test_Y.reshape(-1, 1)\n",
    "    test_Y = test_Y.to(device)\n",
    "\n",
    "    acc, guesses = test(x=test_X[:cases], y=test_Y[:cases],\n",
    "                        policy_fn=policy_fn, actions=actions, dbs=dbs)\n",
    "    print(\"Test Accuracy:\", acc)\n",
    "    print(\"Guess Distribution:\", guesses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "caa3d0f193cdda2a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t9/gxb6q8zj21dff090q066td740000gn/T/ipykernel_88706/450618715.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(x[i]).unsqueeze(0).to(device)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8545\n",
      "Guess Distribution: [1311, 689]\n"
     ]
    }
   ],
   "source": [
    "k_cases = 2000\n",
    "k_dbound_size = 100\n",
    "\n",
    "dual_file = \"test_data/test_simple.csv\"\n",
    "\n",
    "db2 = torch.linspace(2, k_dbound_size, k_dbound_size - 1).to(device)\n",
    "two_dbs = [db2, db2]\n",
    "\n",
    "run_test(dual_file, k_2actions, policy_fn=policy_fn_2, cases=k_cases, dbs=two_dbs)\n",
    "# ~99% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "77fce074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[nan, nan]], grad_fn=<ClampBackward1>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_fn_2(gen_start_state_2a())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bb7c4b36c7d9707",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.261\n",
      "Guess Distribution: [1000, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t9/gxb6q8zj21dff090q066td740000gn/T/ipykernel_8108/450618715.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(x[i]).unsqueeze(0).to(device)\n"
     ]
    }
   ],
   "source": [
    "quad_file = \"test_data/four_directions_cleaner_test.csv\"     # thanks, donald\n",
    "\n",
    "k_cases = 1000\n",
    "\n",
    "k_dbound_size = 200\n",
    "\n",
    "db4 = torch.linspace(-k_dbound_size/2, k_dbound_size/2, k_dbound_size+1).to(device)\n",
    "quad_dbs = [db4, db4]\n",
    "run_test(quad_file, k_4actions, policy_fn=lambda a: torch.argmax(value_fn_4(a)), cases=k_cases)\n",
    "            \n",
    "# run_test(quad_file, k_4actions, policy_fn=lambda a: oh_encode(torch.tensor(determine_action(a.flatten())).view((1,1)),4), cases=k_cases, dbs=quad_dbs)\n",
    "# # 8% accuracy on Donald test csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636c81ea1aa8fc32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
