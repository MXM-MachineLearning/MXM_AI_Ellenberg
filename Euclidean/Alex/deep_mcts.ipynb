{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T00:28:23.047407Z",
     "start_time": "2024-02-28T00:28:03.339254Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch import autograd\n",
    "import threading\n",
    "import concurrent\n",
    "\n",
    "import math\n",
    "\n",
    "from util import *\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "603bf730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nogil=True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nNOTE: RUN WITH NOGIL https://github.com/colesbury/nogil FOR MUCH BETTER PERFORMANCE\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "print(f\"nogil={getattr(sys.flags, 'nogil', False)}\")\n",
    "\n",
    "\"\"\"\n",
    "NOTE: RUN WITH NOGIL https://github.com/colesbury/nogil FOR MUCH BETTER PERFORMANCE\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d75afc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T00:29:36.547536Z",
     "start_time": "2024-02-28T00:29:36.538039Z"
    }
   },
   "outputs": [],
   "source": [
    "# class Node:\n",
    "#     def __init__(self, parent, state, n_children, value, depth=0):\n",
    "#         self.state = state\n",
    "#         self.parent = parent\n",
    "#         self.visits = 0\n",
    "#         self.depth = depth\n",
    "#         self.children = [None] * n_children\n",
    "#         self.is_terminal = terminal(self.state)\n",
    "#         self.value = value\n",
    "#         self.subtree_value = torch.zeros(1).to(device)\n",
    "\n",
    "#         # for more on virtual loss/shared tree search, see \"Parallel MCTS\" by Chaslot et al. \n",
    "#         # https://dke.maastrichtuniversity.nl/m.winands/documents/multithreadedMCTS2.pdf\n",
    "#         self.active_threads = 0\n",
    "#         self.lock = threading.Lock()\n",
    "\n",
    "\n",
    "#     def __str__(self):\n",
    "#         return (\"State: \" + str(self.state) + \"; Value: \" + str(self.value)\n",
    "#                 + \"; Subtree Value: \" + str(self.subtree_value) + \"; Visits:\", str(self.visits))\n",
    "\n",
    "#     def is_leaf(self):\n",
    "#         for i in self.state:\n",
    "#             if i is not None:\n",
    "#                 return False\n",
    "#         return True\n",
    "    \n",
    "# # override the one in util.py\n",
    "# def UCT_fn(child, C):\n",
    "#     if child.visits == 0:\n",
    "#         return math.inf\n",
    "#     uct = child.subtree_value + 2 * C * math.sqrt(2 * math.log2(child.parent.visits) / child.visits)\n",
    "#     return uct * (-1 - child.active_threads)\n",
    "    \n",
    "# class MCTS:\n",
    "#     def __init__(self, actions, C, weight, value_fn):\n",
    "#         self.actions = actions\n",
    "#         self.k_C = C\n",
    "#         self.k_weight = weight\n",
    "#         self.value_fn = value_fn\n",
    "#         self.max_depth = 0\n",
    "#         self.terminal = None    # None if no terminal state found; terminal Node if found\n",
    "#         self.root = None\n",
    "#         self.propagation_lock = threading.Lock()\n",
    "\n",
    "#     def pick_child(self, node):\n",
    "#         # UCT\n",
    "#         t = []\n",
    "#         for i in node.children:\n",
    "#             if i is None:\n",
    "#                 continue\n",
    "#             t.append(UCT_fn(i, self.k_C))\n",
    "\n",
    "#         if len(t) == 0:\n",
    "#             return random.randint(0, len(node.children)-1)\n",
    "        \n",
    "#         t = torch.tensor(t)\n",
    "\n",
    "#         rvs = torch.squeeze(torch.argwhere(t == torch.max(t)), axis=1)\n",
    "#         return int(random.choice(rvs))\n",
    "\n",
    "#     def default_search(self, node):\n",
    "#         \"\"\"\n",
    "#         If node is fully explored (neither child is None), return True\n",
    "#         Otherwise, initialize value of a random unexplored next state\n",
    "\n",
    "#         :param node: node to search from\n",
    "#         :return: if fully explored, True. Else, value of the random unexplored next state\n",
    "#         \"\"\"\n",
    "#         possible = []\n",
    "#         for i in range(len(node.children)):\n",
    "#             if node.children[i] is None:\n",
    "#                 possible.append(i)\n",
    "#         if len(possible) == 0:\n",
    "#             return True\n",
    "\n",
    "#         i = random.choice(possible)\n",
    "#         # if unexplored or non-terminal, get value\n",
    "#         state = self.actions[i](node.state.flatten()).float().to(device)\n",
    "#         state = state.reshape(node.state.shape)\n",
    "#         # child_val = self.value_fn(state) - node.depth - 1  # give penalty -1 for each additional step taken\n",
    "#         child_val = self.value_fn(state)\n",
    "#         child_val = child_val.flatten()[0]\n",
    "\n",
    "#         with node.lock:\n",
    "#             node.children[i] = Node(node, state, len(self.actions), value=child_val, depth=node.depth+1)\n",
    "\n",
    "#         # if new Node is terminal, take it as the tree's terminal if it takes less time to reach than current terminal\n",
    "#         # if node.children[i].is_terminal:\n",
    "#         #     # if terminal, add reward of ||start_vec||_2^2\n",
    "#         #     node.children[i].value += torch.linalg.vector_norm(torch.square(self.root.state)).item()\n",
    "#         #     if self.terminal is None or node.children[i].depth < self.terminal.depth:\n",
    "#         #         self.terminal = node.children[i]\n",
    "\n",
    "#         with self.propagation_lock:\n",
    "#             if node.children[i].depth > self.max_depth:\n",
    "#                 self.max_depth = node.children[i].depth\n",
    "#         return node.children[i]\n",
    "\n",
    "#     def tree_policy(self, node):\n",
    "#         prev = None\n",
    "#         while node.is_terminal is False:\n",
    "#             # add some virtual loss to the node for each thread that's exploring (released after back propagating)\n",
    "#             with node.lock:\n",
    "#                 node.active_threads += 1\n",
    "\n",
    "#             explored = self.default_search(node)\n",
    "#             if explored is not True:\n",
    "#                 return explored\n",
    "#             node = node.children[self.pick_child(node)]\n",
    "#             prev = node\n",
    "#             # node = random.choice(node.children)\n",
    "#         return prev\n",
    "\n",
    "#     def mean_prop(self, node):\n",
    "#         \"\"\"\n",
    "#         Backprop up from a leaf, where subtree_value is the average of a node's rewards and its subtree's rewards\n",
    "\n",
    "#         :param node: of subtree\n",
    "#         \"\"\"\n",
    "#         with node.lock:\n",
    "#             node.subtree_value = torch.zeros(1).to(device)\n",
    "#             node.subtree_value += node.value\n",
    "#             valid_children = 0\n",
    "#             if not node.is_leaf():\n",
    "#                 for i in node.children:\n",
    "#                     if i is None:\n",
    "#                         continue\n",
    "#                     node.subtree_value += self.k_weight * i.subtree_value\n",
    "#                     valid_children += 1\n",
    "#             node.subtree_value /= valid_children + 1\n",
    "#             node.visits += 1\n",
    "\n",
    "#             # remove virtual loss from node after thread done exploring its subtree\n",
    "#             node.active_threads -= 1\n",
    "\n",
    "#             if node.parent is None:\n",
    "#                 return\n",
    "#         self.mean_prop(node.parent)\n",
    "\n",
    "#     def explore_once(self, number):\n",
    "#         node = self.tree_policy(self.root)\n",
    "#         with self.propagation_lock:\n",
    "#             self.mean_prop(node)\n",
    "#         return number\n",
    "\n",
    "#     def run(self, root, comp_limit=10, max_threads=5):\n",
    "#         \"\"\"\n",
    "#         Shoutout \"A Survey of MCTS Methods\"\n",
    "#         :param root: the current state\n",
    "#         :param comp_limit: max number of possible future scenarios to compute (carries over)\n",
    "#         :return: index corresponding to best action\n",
    "#         \"\"\"\n",
    "#         self.root = root\n",
    "#         if self.root.is_terminal:\n",
    "#             return True\n",
    "\n",
    "#         # spawn new thread for each computation\n",
    "#         with concurrent.futures.ThreadPoolExecutor(max_workers=max_threads) as executor:\n",
    "#             executor.map(self.explore_once, range(comp_limit))\n",
    "\n",
    "#         rv = self.pick_child(self.root)\n",
    "\n",
    "#         if False:\n",
    "#             print(\"root state:\", root.state)\n",
    "#             print(\"child states: \",end=\"\")\n",
    "#             for child in root.children:\n",
    "#                 print(child.state, end=\",\")\n",
    "#             print()\n",
    "#         return rv\n",
    "    \n",
    "#     def generate(self, init_state, actions):\n",
    "#         self.root = Node(None, init_state, n_children=len(self.actions), value=self.value_fn(init_state), depth=0)\n",
    "#         curr = self.root\n",
    "#         r_nodes = []\n",
    "#         for i in actions:\n",
    "#             newstate = self.actions[i](curr.state)\n",
    "#             n = Node(parent=curr,\n",
    "#                      state=newstate,\n",
    "#                      n_children=len(self.actions),\n",
    "#                      value=self.value_fn(newstate),\n",
    "#                      depth=curr.depth + 1)\n",
    "#             curr.children[i] = n\n",
    "#             curr = n            \n",
    "#             r_nodes.append(n)\n",
    "#         return r_nodes\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "438fdf1c04168299",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T00:29:37.378811Z",
     "start_time": "2024-02-28T00:29:37.370384Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# class Loss(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Loss, self).__init__()\n",
    "#         self.v_loss_fn = torch.nn.MSELoss()\n",
    "#         self.p_loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "#     def forward(self, v_out, v_target, p_out, p_target):\n",
    "#         \"\"\"\n",
    "#         Loss function designed to reward successful game completion while taking the least amount of steps possible\n",
    "#         Adapted from:\n",
    "#             - \"Mastering the game of Go without human knowledge\" (Silver et al)\n",
    "#             - \"Discovering faster matrix multiplication algorithms with reinforcement learning\" (Fawzi et al)\n",
    "\n",
    "#         :param v_out: the value outputed for the state by NN\n",
    "#         :param p_out: the policy outputed for the state by NN\n",
    "#         :param v_target: target value output\n",
    "#         :return: total loss\n",
    "#         \"\"\"\n",
    "#         loss = self.v_loss_fn(v_out, v_target)\n",
    "#         loss += self.p_loss_fn(p_out, p_target).sum()\n",
    "#         return loss\n",
    "\n",
    "\n",
    "# class ValueNN(nn.Module):\n",
    "#     def __init__(self, state_size):\n",
    "#         super(ValueNN, self).__init__()\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.stack = nn.Sequential(\n",
    "#             nn.Linear(state_size, 128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(128, 64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(64, 16),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(16, 1),\n",
    "#         )\n",
    "#         self.value_activation = nn.ReLU()\n",
    "#     def forward(self, x):\n",
    "# #        x = self.flatten(x)\n",
    "# #        x = self.stack(x).flatten()\n",
    "# #        value = x[0:1].reshape((1,1))\n",
    "# #        return value\n",
    "#         return self.stack(x)\n",
    "\n",
    "\n",
    "# class PolicyNN(nn.Module):\n",
    "#     def __init__(self, state_size, n_actions):\n",
    "#         super(PolicyNN, self).__init__()\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.stack = nn.Sequential(\n",
    "#             nn.Linear(state_size, 128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(128, 64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(64, 16),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(16, n_actions),\n",
    "#         )\n",
    "#         self.policy_activation = nn.Softmax(dim=1)\n",
    "#     def forward(self, x):\n",
    "#         x = self.policy_activation(self.stack(x))#.flatten())\n",
    "#         policy = torch.clamp(x,min=1e-8,max=1-(1e-7))\n",
    "#         return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05e3ddf7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T00:29:38.334723Z",
     "start_time": "2024-02-28T00:29:38.325718Z"
    }
   },
   "outputs": [],
   "source": [
    "# Memory for better batching\n",
    "\n",
    "class Memory:\n",
    "    def __init__(self, width) -> None:\n",
    "        self.mem_ = None\n",
    "        self.len_ = 0\n",
    "    def record(self, obs):\n",
    "        if self.len_ == 0:\n",
    "            self.mem_ = obs\n",
    "        else:\n",
    "            self.mem_ = torch.cat((self.mem_, obs), dim=0)\n",
    "        self.len_ += 1\n",
    "    def recall(self, n_samples):\n",
    "        if self.len_ == 0:\n",
    "            return None\n",
    "        des_len = min(n_samples, self.len_)\n",
    "        indices = torch.ones(self.mem_.shape[0]).multinomial(des_len, replacement=False)\n",
    "        return self.mem_[indices]\n",
    "    def size(self):\n",
    "        return self.len_\n",
    "    def clear(self):\n",
    "        self.len_ = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f19bdb93248565c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T00:29:39.699250Z",
     "start_time": "2024-02-28T00:29:39.688411Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "k_C = 1 / math.sqrt(2)\n",
    "k_thread_count_limit = 20\n",
    "k_core_limit = 5\n",
    "\n",
    "# def get_train_data(fname):\n",
    "#     x = np.loadtxt(fname, delimiter=\",\")\n",
    "#     return torch.tensor([x[:,2], x[:,2:]], dtype=torch.float)\n",
    "\n",
    "# def get_nonterm_rwd(mcts):\n",
    "#     return -mcts.max_depth\n",
    "\n",
    "# def get_terminal_rwd(terminal_depth, start):\n",
    "#     return -terminal_depth + torch.linalg.norm(start)\n",
    "\n",
    "def train_sv(epochs, actions, policy_fn, value_fn, optimizers, fname, batch_size=10):\n",
    "    k_mem_width = 4    # statex,statey,action,subtree_value\n",
    "    memory = Memory(k_mem_width)\n",
    "    loss_fn = Loss()\n",
    "    # load data into memory\n",
    "    with open(fname, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            entry = torch.tensor(list(map(int, line.split(','))))\n",
    "            tree = MCTS(actions, C=k_C, weight=1, value_fn=value_fn)\n",
    "            g_nodes = tree.generate(entry[0:2].unsqueeze(0).float(), entry[2:])\n",
    "            for i in range(len(entry) - 2): # go by actions (so we disregard the terminal node)\n",
    "                memory.record(torch.cat((g_nodes[i].state.reshape(1,2), \n",
    "                                         entry[2+i].reshape((1,1)), \n",
    "                                         g_nodes[i].subtree_value.reshape(1,1)), dim=1))\n",
    "    # train off memory\n",
    "    for t in range(epochs):\n",
    "        for o in optimizers:\n",
    "            o.zero_grad()\n",
    "        batch = memory.recall(batch_size)\n",
    "        v_out = value_fn(batch[:,:2])\n",
    "        p_out = policy_fn(batch[:,:2])\n",
    "\n",
    "        # one-hot encode actions; e.g. convert 3 -> (0,0,0,1)\n",
    "        action_indices = batch[:,2:-1].to(torch.int64)\n",
    "        p_target = oh_encode(action_indices, len(actions))\n",
    "        # p_target = torch.zeros(action_indices.shape[0],len(actions))\n",
    "        # p_target.scatter_(1, action_indices,1)\n",
    "\n",
    "        v_target = batch[:,-1]\n",
    "\n",
    "        loss = loss_fn(v_out.view(v_target.shape), v_target, p_out.view(p_target.shape), p_target)\n",
    "        loss.backward()\n",
    "\n",
    "        for o in optimizers:\n",
    "            o.step()\n",
    "    \n",
    "        if (t+1) % 10 == 0:\n",
    "            print(\"Epoch:\", t+1,\"\\t\\tLoss:\",loss.item())\n",
    "\n",
    "\n",
    "# def one_batch(start, actions, value_fn, comp_limit):\n",
    "#         mcts = MCTS(actions, C=k_C, weight=1, value_fn=value_fn)\n",
    "\n",
    "#         value = mcts.value_fn(start).flatten().to(device)\n",
    "\n",
    "#         start_node = Node(None, start, len(actions), value, 0)\n",
    "\n",
    "#         mcts.run(start_node, comp_limit=comp_limit, max_threads=k_thread_count_limit)\n",
    "\n",
    "\n",
    "#         # get attributes of game just played\n",
    "#         v_out = start_node.subtree_value.to(device)\n",
    "#         v_target = get_nonterm_rwd(mcts)\n",
    "#         if mcts.terminal is not None:\n",
    "#             v_target = get_terminal_rwd(mcts.terminal.depth, start)\n",
    "#         v_target = torch.tensor(v_target,dtype=v_out.dtype).to(device)\n",
    "\n",
    "\n",
    "#         visits = []\n",
    "#         for i in start_node.children:\n",
    "#             if i is None:\n",
    "#                 visits.append(0)\n",
    "#             else:\n",
    "#                 visits.append(i.visits)\n",
    "#         visits = torch.tensor(visits, dtype=torch.float).to(device)\n",
    "#         p_sampled = visits / torch.sum(visits)\n",
    "#         return torch.cat((start,v_target.flatten().unsqueeze(0), p_sampled.flatten().unsqueeze(0)),dim=1)\n",
    "\n",
    "\n",
    "def train_play(epochs, actions, policy_fn, value_fn, optimizers, rand_start_state_fn, comp_limit, batch_size=16):\n",
    "    history = Memory(3+len(actions))    # [stateX,stateY,pr1,pr2,...pr(len(actions))] (probs are sampled probs)\n",
    "    loss_fn = Loss()\n",
    "    tot_loss = 0\n",
    "    for t in range(epochs):\n",
    "\n",
    "        for o in optimizers:\n",
    "            o.zero_grad()\n",
    "        # Repeat the following:\n",
    "        # 1) run the NN on some random initial state\n",
    "        # 2) update the NN based off performance in that game\n",
    "            \n",
    "        # play out some games\n",
    "        k_comp_limit = comp_limit(t / epochs)\n",
    "\n",
    "        # NOTE: ProcessPoolExecutor requires placing called functions/data structs in separate imported file\n",
    "        payload = [(rand_start_state_fn(), k_2actions, value_fn, k_comp_limit, k_C, k_thread_count_limit) for i in range(5)]\n",
    "        with concurrent.futures.ProcessPoolExecutor(max_workers=k_core_limit) as executor:\n",
    "            for rv in zip(executor.map(one_batch, iter(payload))):\n",
    "                history.record(rv[0])\n",
    "        \n",
    "        # for game in range(batch_size):\n",
    "        #     history.record(one_batch(rand_start_state_fn(), actions, value_fn, k_comp_limit))\n",
    "\n",
    "        # train NN on games just played\n",
    "        batch = history.recall(batch_size)\n",
    "        batch_states = batch[:,:2]\n",
    "        batch_vsampled = batch[:,2]\n",
    "        batch_psampled = batch[:,3:]\n",
    "        \n",
    "        loss = loss_fn(value_fn(batch_states).view(batch_vsampled.shape), batch_vsampled,\n",
    "                       policy_fn(batch_states).view(batch_psampled.shape), batch_psampled)\n",
    "        loss.backward()\n",
    "        tot_loss += loss.item() / batch_size\n",
    "\n",
    "        history.clear()\n",
    "\n",
    "        for o in optimizers:\n",
    "            o.step()\n",
    "\n",
    "\n",
    "        if (t+1) % 10 == 0:\n",
    "            print(\"Epoch:\", t+1,\"\\t\\tLoss:\",tot_loss/10)\n",
    "            tot_loss = 0\n",
    "            # if torch.isnan(p_loss):\n",
    "            #     print(\"value\",v_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e004f8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T00:29:40.566311Z",
     "start_time": "2024-02-28T00:29:40.556759Z"
    }
   },
   "outputs": [],
   "source": [
    "k_state_upper_lim = 30 # arbitrary\n",
    "k_comp_limit = int(k_state_upper_lim ** (3/2))\n",
    "k_min_comps = int(k_state_upper_lim ** (1.1))\n",
    "\n",
    "value_fn_2 = ValueNN(2).to(device)\n",
    "policy_fn_2 = PolicyNN(2,len(k_2actions)).to(device)\n",
    "value_optim = optim.Adam(value_fn_2.parameters(), lr=0.00005)\n",
    "policy_optim = optim.Adam(policy_fn_2.parameters(), lr=0.000005)\n",
    "\n",
    "def gen_start_state_2a():\n",
    "    limit = k_state_upper_lim\n",
    "    return torch.round(torch.rand((1, 2)) * limit + 1).float()\n",
    "\n",
    "def adaptive_comp_limit(frac_epochs):\n",
    "    # linearly decrease computation limit as model becomes better over time\n",
    "    rv = k_comp_limit - (k_comp_limit - k_min_comps) * frac_epochs\n",
    "    return int(rv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41211b20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T02:48:18.942268Z",
     "start_time": "2024-02-28T02:48:12.374093Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 \t\tLoss: 11.608506846427918\n",
      "Epoch: 20 \t\tLoss: 12.267288398742675\n",
      "Epoch: 30 \t\tLoss: 7.620764726400376\n",
      "Epoch: 40 \t\tLoss: 4.146345758438111\n",
      "Epoch: 50 \t\tLoss: 2.787030404806137\n",
      "Epoch: 60 \t\tLoss: 3.6339794158935548\n",
      "Epoch: 70 \t\tLoss: 3.091897279024124\n",
      "Epoch: 80 \t\tLoss: 2.4989698052406313\n",
      "Epoch: 90 \t\tLoss: 3.218223679065704\n",
      "Epoch: 100 \t\tLoss: 3.2597360849380492\n"
     ]
    }
   ],
   "source": [
    "train_play(epochs=100, actions=k_2actions, policy_fn=policy_fn_2, value_fn=value_fn_2, \n",
    "           optimizers=[value_optim, policy_optim], rand_start_state_fn=gen_start_state_2a, \n",
    "           comp_limit=adaptive_comp_limit, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "df2ff0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "save = False\n",
    "if save:\n",
    "    torch.save(value_fn_2.state_dict(), \"trained_weights/deep_mcts_2_v_weights.pth\")\n",
    "    torch.save(policy_fn_2.state_dict(), \"trained_weights/deep_mcts_2_p_weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57f7b9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_state_upper_lim = 30 # arbitrary\n",
    "k_comp_limit = int(k_state_upper_lim ** (7/2))\n",
    "value_fn_4 = ValueNN(2).to(device)\n",
    "policy_fn_4 = PolicyNN(2,len(k_4actions)).to(device)\n",
    "value_optim_4 = optim.Adam(value_fn_4.parameters(), lr=0.00005)\n",
    "policy_optim_4 = optim.Adam(policy_fn_4.parameters(), lr=0.000005)\n",
    "\n",
    "def gen_start_state_4a():\n",
    "    limit = k_state_upper_lim\n",
    "    return torch.round( (torch.rand((1, 2)) - 0.5) * 2 * limit).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e2b8d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 \t\tLoss: 2.1033401489257812\n",
      "Epoch: 20 \t\tLoss: 1.556889295578003\n",
      "Epoch: 30 \t\tLoss: 1.3582837581634521\n",
      "Epoch: 40 \t\tLoss: 1.4088467359542847\n",
      "Epoch: 50 \t\tLoss: 1.436020016670227\n",
      "Epoch: 60 \t\tLoss: 1.379128336906433\n",
      "Epoch: 70 \t\tLoss: 1.4841262102127075\n",
      "Epoch: 80 \t\tLoss: 1.3614771366119385\n",
      "Epoch: 90 \t\tLoss: 1.3785827159881592\n",
      "Epoch: 100 \t\tLoss: 1.3600499629974365\n",
      "Epoch: 110 \t\tLoss: 1.389901041984558\n",
      "Epoch: 120 \t\tLoss: 1.383531928062439\n",
      "Epoch: 130 \t\tLoss: 1.3663393259048462\n",
      "Epoch: 140 \t\tLoss: 1.3774583339691162\n",
      "Epoch: 150 \t\tLoss: 1.2760998010635376\n",
      "Epoch: 160 \t\tLoss: 1.3536250591278076\n",
      "Epoch: 170 \t\tLoss: 1.3584808111190796\n",
      "Epoch: 180 \t\tLoss: 1.3749842643737793\n",
      "Epoch: 190 \t\tLoss: 1.3323438167572021\n",
      "Epoch: 200 \t\tLoss: 1.3520457744598389\n",
      "Epoch: 210 \t\tLoss: 1.310624599456787\n",
      "Epoch: 220 \t\tLoss: 1.3285794258117676\n",
      "Epoch: 230 \t\tLoss: 1.3432512283325195\n",
      "Epoch: 240 \t\tLoss: 1.3051739931106567\n",
      "Epoch: 250 \t\tLoss: 1.3170291185379028\n"
     ]
    }
   ],
   "source": [
    "train_sv(250, k_4actions, policy_fn=policy_fn_4, value_fn=value_fn_4, optimizers=[value_optim_4,policy_optim_4], fname='train_data/train_mcts.csv', batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e933c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_policy_fp = \"start_weights/deep_mcts_4_p_weights.pth\"\n",
    "init_value_fp = \"start_weights/deep_mcts_4_v_weights.pth\"\n",
    "trained_policy_fp = \"trained_weights/deep_mcts_4_p_weights_f.pth\"\n",
    "trained_value_fp = \"trained_weights/deep_mcts_4_v_weights_f.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4fba2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if False:\n",
    "    torch.save(value_fn_4.state_dict(), init_value_fp)\n",
    "    torch.save(policy_fn_4.state_dict(), init_policy_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fc7fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_play(epochs=100, actions=k_4actions, policy_fn=policy_fn_4, value_fn=value_fn_4, optimizers=[value_optim_4, policy_optim_4], rand_start_state_fn=gen_start_state_4a, comp_limit=k_comp_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bcbd88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test trained weights\n",
    "\n",
    "\n",
    "# value_fn_4.load_state_dict(torch.load(trained_value_fp))\n",
    "# policy_fn_4.load_state_dict(torch.load(trained_policy_fp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c191e6c845a0bd24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T02:47:31.912513Z",
     "start_time": "2023-11-06T02:47:31.904073Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_test_data(fname):\n",
    "    x = torch.tensor(np.loadtxt(fname, delimiter=\",\"), dtype=torch.float)\n",
    "    return x[:,:-1], x[:,-1]\n",
    "\n",
    "def plot_db(policy_fn, actions, ranges):\n",
    "    X = ranges[0]\n",
    "    Y = ranges[1]\n",
    "    action_plot = []\n",
    "    for i in actions:\n",
    "        action_plot.append([])\n",
    "    for i in X:\n",
    "        for j in Y:\n",
    "            rv = policy_fn(torch.tensor([i,j],dtype=torch.float).unsqueeze(0)).flatten().to(device)\n",
    "            action_plot[torch.argmax(rv)].append((i.cpu(),j.cpu()))\n",
    "    for i in range(len(action_plot)):\n",
    "        action = np.array(action_plot[i])\n",
    "        if len(action) == 0:\n",
    "            continue\n",
    "        plt.scatter(action[:,0], action[:,1], color=(\"C\"+str(i)), label=action)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1368c1aa3c071299",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T02:48:10.179905Z",
     "start_time": "2023-11-06T02:48:10.175988Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test(x, y, policy_fn, actions=k_2actions, dbs=None):\n",
    "    correct = 0\n",
    "    guess_dist = [0] * len(actions)\n",
    "    for i in range(len(x)):\n",
    "        state = torch.tensor(x[i]).unsqueeze(0).to(device)\n",
    "        rv = policy_fn(state).flatten()                      # take the move distribution given by NN\n",
    "\n",
    "        # todo pick one way to select\n",
    "        # rv = rv.multinomial(num_samples=1, replacement=True)    # sample from the move distribution\n",
    "        rv = torch.argmax(rv)\n",
    "\n",
    "        if rv == y[i]:\n",
    "            correct += 1\n",
    "        guess_dist[rv] += 1\n",
    "    # todo fix\n",
    "    if dbs is not None:\n",
    "        # graphing decision boundary\n",
    "        plot_db(policy_fn, actions, ranges=dbs)\n",
    "    return correct / len(x), guess_dist\n",
    "\n",
    "\n",
    "def run_test(data_name, actions, policy_fn, cases=100, dbs=None):\n",
    "    test_X, test_Y = get_test_data(data_name)\n",
    "    test_X = test_X.to(device)\n",
    "    test_Y.reshape(-1, 1)\n",
    "    test_Y = test_Y.to(device)\n",
    "\n",
    "    acc, guesses = test(x=test_X[:cases], y=test_Y[:cases],\n",
    "                        policy_fn=policy_fn, actions=actions, dbs=dbs)\n",
    "    print(\"Test Accuracy:\", acc)\n",
    "    print(\"Guess Distribution:\", guesses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "caa3d0f193cdda2a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t9/gxb6q8zj21dff090q066td740000gn/T/ipykernel_19315/450618715.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(x[i]).unsqueeze(0).to(device)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.933\n",
      "Guess Distribution: [888, 1112]\n"
     ]
    }
   ],
   "source": [
    "k_cases = 2000\n",
    "k_dbound_size = 100\n",
    "\n",
    "dual_file = \"test_data/test_simple.csv\"\n",
    "\n",
    "db2 = torch.linspace(2, k_dbound_size, k_dbound_size - 1).to(device)\n",
    "two_dbs = [db2, db2]\n",
    "\n",
    "def alt_policy(state, value_fn):\n",
    "    x = [i(state) for i in k_2actions]\n",
    "    x = [value_fn(i) for i in x]\n",
    "    x = torch.tensor(x)\n",
    "    return x\n",
    "\n",
    "run_test(dual_file, k_2actions, policy_fn=lambda a: alt_policy(a, value_fn_2), cases=k_cases, dbs=two_dbs)\n",
    "# ~99% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bb7c4b36c7d9707",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.261\n",
      "Guess Distribution: [1000, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t9/gxb6q8zj21dff090q066td740000gn/T/ipykernel_8108/450618715.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(x[i]).unsqueeze(0).to(device)\n"
     ]
    }
   ],
   "source": [
    "quad_file = \"test_data/four_directions_cleaner_test.csv\"     # thanks, donald\n",
    "\n",
    "k_cases = 1000\n",
    "\n",
    "k_dbound_size = 200\n",
    "\n",
    "db4 = torch.linspace(-k_dbound_size/2, k_dbound_size/2, k_dbound_size+1).to(device)\n",
    "quad_dbs = [db4, db4]\n",
    "run_test(quad_file, k_4actions, policy_fn=lambda a: torch.argmax(value_fn_4(a)), cases=k_cases)\n",
    "            \n",
    "# run_test(quad_file, k_4actions, policy_fn=lambda a: oh_encode(torch.tensor(determine_action(a.flatten())).view((1,1)),4), cases=k_cases, dbs=quad_dbs)\n",
    "# # 8% accuracy on Donald test csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636c81ea1aa8fc32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
