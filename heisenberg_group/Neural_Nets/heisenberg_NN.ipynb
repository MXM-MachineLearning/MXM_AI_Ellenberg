{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code adapted from ChatGPT\n",
    "\n",
    "# Define your neural network model\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "\n",
    "        self.step1 = nn.Linear(3, 128, bias=True)\n",
    "        self.step2 = nn.ReLU()\n",
    "        self.step3 = nn.Linear(128, 64, bias=True)\n",
    "        self.step4 = nn.ReLU()\n",
    "        self.step5 = nn.Linear(64, 16, bias=True)\n",
    "        self.step6 = nn.ReLU()\n",
    "        self.step7 = nn.Linear(16, 4, bias=True)\n",
    "\n",
    "        # multi-class classification adapted from ChatGPT\n",
    "        self.step8 = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # RUN IT ON A GPU if it exists\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.to(\"cuda\")\n",
    "\n",
    "        x = self.step1(x)\n",
    "        x = self.step2(x)\n",
    "        x = self.step3(x)\n",
    "        x = self.step4(x)\n",
    "        x = self.step5(x)\n",
    "        x = self.step6(x)\n",
    "        x = self.step7(x)\n",
    "        x = self.step8(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def train_model(inputs, desired_outputs, num_epochs=100, learning_rate=0.01, viz_accuracy=False):\n",
    "    # Convert inputs and desired_outputs to PyTorch tensors\n",
    "    inputs = torch.tensor(inputs, dtype=torch.float32)\n",
    "    desired_outputs = torch.tensor(desired_outputs, dtype=torch.float32)\n",
    "    \n",
    "    # Create a DataLoader to handle batching (if needed)\n",
    "    dataset = TensorDataset(inputs, desired_outputs)\n",
    "    dataloader = DataLoader(dataset, batch_size=1000, shuffle=True)  # Adjust batch_size as needed\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = SimpleModel()\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()  # Mean Squared Error loss\n",
    "    # optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "    # criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.995)\n",
    "    losses = []\n",
    "\n",
    "    inputs = inputs.float()\n",
    "    desired_outputs = desired_outputs.long()\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for batch_inputs, batch_desired_outputs in dataloader:\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            outputs = model(batch_inputs)  # Forward pass\n",
    "\n",
    "            batch_desired_outputs = batch_desired_outputs.long()\n",
    "\n",
    "            loss = criterion(outputs, batch_desired_outputs)  # Compute the loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update the model's parameters\n",
    "            cur_item = loss.item()\n",
    "            total_loss += cur_item\n",
    "        \n",
    "        # Print the average loss for this epoch\n",
    "        scheduler.step()\n",
    "        print(f\"total loss: {total_loss}\")\n",
    "        average_loss = total_loss / len(dataloader)\n",
    "        if viz_accuracy:\n",
    "            losses.append(average_loss)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.4f}')\n",
    "    \n",
    "    if viz_accuracy:\n",
    "        plt.scatter(x=range(1, len(losses)+1), y=losses)\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Average loss\")\n",
    "        plt.show()\n",
    "\n",
    "    # Return the trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Data_Generation/Data_files/heisenberg_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "just_input = df.drop('last_matrix', axis=1)\n",
    "\n",
    "# adapted from https://stackoverflow.com/questions/43898035/pandas-combine-column-values-into-a-list-in-a-new-column\n",
    "input_data = np.array(just_input.values.tolist())\n",
    "\n",
    "df['last_matrix'] = df['last_matrix'] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_output = torch.tensor(df['last_matrix'].tolist(), dtype=torch.float32).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16089\\AppData\\Local\\Temp\\ipykernel_12212\\2668634726.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  desired_outputs = torch.tensor(desired_outputs, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total loss: 13.854714632034302\n",
      "Epoch [1/500], Loss: 1.3855\n",
      "total loss: 13.762439012527466\n",
      "Epoch [2/500], Loss: 1.3762\n",
      "total loss: 13.691674828529358\n",
      "Epoch [3/500], Loss: 1.3692\n",
      "total loss: 13.637699484825134\n",
      "Epoch [4/500], Loss: 1.3638\n",
      "total loss: 13.590192556381226\n",
      "Epoch [5/500], Loss: 1.3590\n",
      "total loss: 13.54334008693695\n",
      "Epoch [6/500], Loss: 1.3543\n",
      "total loss: 13.49469780921936\n",
      "Epoch [7/500], Loss: 1.3495\n",
      "total loss: 13.45004415512085\n",
      "Epoch [8/500], Loss: 1.3450\n",
      "total loss: 13.413320064544678\n",
      "Epoch [9/500], Loss: 1.3413\n",
      "total loss: 13.379883050918579\n",
      "Epoch [10/500], Loss: 1.3380\n",
      "total loss: 13.357702374458313\n",
      "Epoch [11/500], Loss: 1.3358\n",
      "total loss: 13.345086455345154\n",
      "Epoch [12/500], Loss: 1.3345\n",
      "total loss: 13.333186626434326\n",
      "Epoch [13/500], Loss: 1.3333\n",
      "total loss: 13.320457339286804\n",
      "Epoch [14/500], Loss: 1.3320\n",
      "total loss: 13.316146492958069\n",
      "Epoch [15/500], Loss: 1.3316\n",
      "total loss: 13.30613660812378\n",
      "Epoch [16/500], Loss: 1.3306\n",
      "total loss: 13.30128800868988\n",
      "Epoch [17/500], Loss: 1.3301\n",
      "total loss: 13.29647409915924\n",
      "Epoch [18/500], Loss: 1.3296\n",
      "total loss: 13.288631319999695\n",
      "Epoch [19/500], Loss: 1.3289\n",
      "total loss: 13.290268540382385\n",
      "Epoch [20/500], Loss: 1.3290\n",
      "total loss: 13.284458994865417\n",
      "Epoch [21/500], Loss: 1.3284\n",
      "total loss: 13.282044172286987\n",
      "Epoch [22/500], Loss: 1.3282\n",
      "total loss: 13.276902318000793\n",
      "Epoch [23/500], Loss: 1.3277\n",
      "total loss: 13.27122151851654\n",
      "Epoch [24/500], Loss: 1.3271\n",
      "total loss: 13.274950981140137\n",
      "Epoch [25/500], Loss: 1.3275\n",
      "total loss: 13.267284274101257\n",
      "Epoch [26/500], Loss: 1.3267\n",
      "total loss: 13.270168781280518\n",
      "Epoch [27/500], Loss: 1.3270\n",
      "total loss: 13.261894941329956\n",
      "Epoch [28/500], Loss: 1.3262\n",
      "total loss: 13.264662265777588\n",
      "Epoch [29/500], Loss: 1.3265\n",
      "total loss: 13.262142300605774\n",
      "Epoch [30/500], Loss: 1.3262\n",
      "total loss: 13.260614395141602\n",
      "Epoch [31/500], Loss: 1.3261\n",
      "total loss: 13.255666971206665\n",
      "Epoch [32/500], Loss: 1.3256\n",
      "total loss: 13.248953819274902\n",
      "Epoch [33/500], Loss: 1.3249\n",
      "total loss: 13.250884890556335\n",
      "Epoch [34/500], Loss: 1.3251\n",
      "total loss: 13.24917984008789\n",
      "Epoch [35/500], Loss: 1.3249\n",
      "total loss: 13.2433180809021\n",
      "Epoch [36/500], Loss: 1.3243\n",
      "total loss: 13.246886134147644\n",
      "Epoch [37/500], Loss: 1.3247\n",
      "total loss: 13.247053503990173\n",
      "Epoch [38/500], Loss: 1.3247\n",
      "total loss: 13.245063781738281\n",
      "Epoch [39/500], Loss: 1.3245\n",
      "total loss: 13.242188334465027\n",
      "Epoch [40/500], Loss: 1.3242\n",
      "total loss: 13.240793585777283\n",
      "Epoch [41/500], Loss: 1.3241\n",
      "total loss: 13.246151566505432\n",
      "Epoch [42/500], Loss: 1.3246\n",
      "total loss: 13.238765835762024\n",
      "Epoch [43/500], Loss: 1.3239\n",
      "total loss: 13.23566722869873\n",
      "Epoch [44/500], Loss: 1.3236\n",
      "total loss: 13.2379629611969\n",
      "Epoch [45/500], Loss: 1.3238\n",
      "total loss: 13.236624002456665\n",
      "Epoch [46/500], Loss: 1.3237\n",
      "total loss: 13.232195734977722\n",
      "Epoch [47/500], Loss: 1.3232\n",
      "total loss: 13.235144019126892\n",
      "Epoch [48/500], Loss: 1.3235\n",
      "total loss: 13.22939383983612\n",
      "Epoch [49/500], Loss: 1.3229\n",
      "total loss: 13.229695200920105\n",
      "Epoch [50/500], Loss: 1.3230\n",
      "total loss: 13.22778582572937\n",
      "Epoch [51/500], Loss: 1.3228\n",
      "total loss: 13.225377917289734\n",
      "Epoch [52/500], Loss: 1.3225\n",
      "total loss: 13.226335048675537\n",
      "Epoch [53/500], Loss: 1.3226\n",
      "total loss: 13.224800825119019\n",
      "Epoch [54/500], Loss: 1.3225\n",
      "total loss: 13.22576653957367\n",
      "Epoch [55/500], Loss: 1.3226\n",
      "total loss: 13.221643924713135\n",
      "Epoch [56/500], Loss: 1.3222\n",
      "total loss: 13.22119677066803\n",
      "Epoch [57/500], Loss: 1.3221\n",
      "total loss: 13.223230957984924\n",
      "Epoch [58/500], Loss: 1.3223\n",
      "total loss: 13.223890542984009\n",
      "Epoch [59/500], Loss: 1.3224\n",
      "total loss: 13.2342609167099\n",
      "Epoch [60/500], Loss: 1.3234\n",
      "total loss: 13.225881338119507\n",
      "Epoch [61/500], Loss: 1.3226\n",
      "total loss: 13.220731854438782\n",
      "Epoch [62/500], Loss: 1.3221\n",
      "total loss: 13.223242163658142\n",
      "Epoch [63/500], Loss: 1.3223\n",
      "total loss: 13.217800259590149\n",
      "Epoch [64/500], Loss: 1.3218\n",
      "total loss: 13.218080163002014\n",
      "Epoch [65/500], Loss: 1.3218\n",
      "total loss: 13.215698719024658\n",
      "Epoch [66/500], Loss: 1.3216\n",
      "total loss: 13.214858055114746\n",
      "Epoch [67/500], Loss: 1.3215\n",
      "total loss: 13.216688632965088\n",
      "Epoch [68/500], Loss: 1.3217\n",
      "total loss: 13.214271903038025\n",
      "Epoch [69/500], Loss: 1.3214\n",
      "total loss: 13.213266015052795\n",
      "Epoch [70/500], Loss: 1.3213\n",
      "total loss: 13.212337255477905\n",
      "Epoch [71/500], Loss: 1.3212\n",
      "total loss: 13.213389158248901\n",
      "Epoch [72/500], Loss: 1.3213\n",
      "total loss: 13.21471083164215\n",
      "Epoch [73/500], Loss: 1.3215\n",
      "total loss: 13.210135698318481\n",
      "Epoch [74/500], Loss: 1.3210\n",
      "total loss: 13.212112307548523\n",
      "Epoch [75/500], Loss: 1.3212\n",
      "total loss: 13.211739659309387\n",
      "Epoch [76/500], Loss: 1.3212\n",
      "total loss: 13.211888551712036\n",
      "Epoch [77/500], Loss: 1.3212\n",
      "total loss: 13.20989203453064\n",
      "Epoch [78/500], Loss: 1.3210\n",
      "total loss: 13.211772203445435\n",
      "Epoch [79/500], Loss: 1.3212\n",
      "total loss: 13.20680844783783\n",
      "Epoch [80/500], Loss: 1.3207\n",
      "total loss: 13.208733916282654\n",
      "Epoch [81/500], Loss: 1.3209\n",
      "total loss: 13.204554438591003\n",
      "Epoch [82/500], Loss: 1.3205\n",
      "total loss: 13.206307888031006\n",
      "Epoch [83/500], Loss: 1.3206\n",
      "total loss: 13.204057097434998\n",
      "Epoch [84/500], Loss: 1.3204\n",
      "total loss: 13.204914569854736\n",
      "Epoch [85/500], Loss: 1.3205\n",
      "total loss: 13.205202579498291\n",
      "Epoch [86/500], Loss: 1.3205\n",
      "total loss: 13.207372903823853\n",
      "Epoch [87/500], Loss: 1.3207\n",
      "total loss: 13.206588864326477\n",
      "Epoch [88/500], Loss: 1.3207\n",
      "total loss: 13.205771088600159\n",
      "Epoch [89/500], Loss: 1.3206\n",
      "total loss: 13.205263376235962\n",
      "Epoch [90/500], Loss: 1.3205\n",
      "total loss: 13.204718351364136\n",
      "Epoch [91/500], Loss: 1.3205\n",
      "total loss: 13.203742861747742\n",
      "Epoch [92/500], Loss: 1.3204\n",
      "total loss: 13.20561969280243\n",
      "Epoch [93/500], Loss: 1.3206\n",
      "total loss: 13.204593658447266\n",
      "Epoch [94/500], Loss: 1.3205\n",
      "total loss: 13.199182868003845\n",
      "Epoch [95/500], Loss: 1.3199\n",
      "total loss: 13.197959780693054\n",
      "Epoch [96/500], Loss: 1.3198\n",
      "total loss: 13.200539827346802\n",
      "Epoch [97/500], Loss: 1.3201\n",
      "total loss: 13.202722549438477\n",
      "Epoch [98/500], Loss: 1.3203\n",
      "total loss: 13.199393630027771\n",
      "Epoch [99/500], Loss: 1.3199\n",
      "total loss: 13.199685215950012\n",
      "Epoch [100/500], Loss: 1.3200\n",
      "total loss: 13.197260975837708\n",
      "Epoch [101/500], Loss: 1.3197\n",
      "total loss: 13.199165940284729\n",
      "Epoch [102/500], Loss: 1.3199\n",
      "total loss: 13.199230670928955\n",
      "Epoch [103/500], Loss: 1.3199\n",
      "total loss: 13.197588682174683\n",
      "Epoch [104/500], Loss: 1.3198\n",
      "total loss: 13.194162130355835\n",
      "Epoch [105/500], Loss: 1.3194\n",
      "total loss: 13.195541381835938\n",
      "Epoch [106/500], Loss: 1.3196\n",
      "total loss: 13.198704838752747\n",
      "Epoch [107/500], Loss: 1.3199\n",
      "total loss: 13.192310214042664\n",
      "Epoch [108/500], Loss: 1.3192\n",
      "total loss: 13.195074796676636\n",
      "Epoch [109/500], Loss: 1.3195\n",
      "total loss: 13.195880889892578\n",
      "Epoch [110/500], Loss: 1.3196\n",
      "total loss: 13.194075465202332\n",
      "Epoch [111/500], Loss: 1.3194\n",
      "total loss: 13.196260213851929\n",
      "Epoch [112/500], Loss: 1.3196\n",
      "total loss: 13.193780660629272\n",
      "Epoch [113/500], Loss: 1.3194\n",
      "total loss: 13.191638469696045\n",
      "Epoch [114/500], Loss: 1.3192\n",
      "total loss: 13.192804455757141\n",
      "Epoch [115/500], Loss: 1.3193\n",
      "total loss: 13.193944215774536\n",
      "Epoch [116/500], Loss: 1.3194\n",
      "total loss: 13.190656185150146\n",
      "Epoch [117/500], Loss: 1.3191\n",
      "total loss: 13.19668424129486\n",
      "Epoch [118/500], Loss: 1.3197\n",
      "total loss: 13.194436073303223\n",
      "Epoch [119/500], Loss: 1.3194\n",
      "total loss: 13.189642310142517\n",
      "Epoch [120/500], Loss: 1.3190\n",
      "total loss: 13.193372130393982\n",
      "Epoch [121/500], Loss: 1.3193\n",
      "total loss: 13.192165851593018\n",
      "Epoch [122/500], Loss: 1.3192\n",
      "total loss: 13.191344499588013\n",
      "Epoch [123/500], Loss: 1.3191\n",
      "total loss: 13.18966281414032\n",
      "Epoch [124/500], Loss: 1.3190\n",
      "total loss: 13.188994526863098\n",
      "Epoch [125/500], Loss: 1.3189\n",
      "total loss: 13.190864324569702\n",
      "Epoch [126/500], Loss: 1.3191\n",
      "total loss: 13.191086530685425\n",
      "Epoch [127/500], Loss: 1.3191\n",
      "total loss: 13.191681981086731\n",
      "Epoch [128/500], Loss: 1.3192\n",
      "total loss: 13.188565254211426\n",
      "Epoch [129/500], Loss: 1.3189\n",
      "total loss: 13.193089723587036\n",
      "Epoch [130/500], Loss: 1.3193\n",
      "total loss: 13.185336709022522\n",
      "Epoch [131/500], Loss: 1.3185\n",
      "total loss: 13.188526391983032\n",
      "Epoch [132/500], Loss: 1.3189\n",
      "total loss: 13.189943432807922\n",
      "Epoch [133/500], Loss: 1.3190\n",
      "total loss: 13.190783500671387\n",
      "Epoch [134/500], Loss: 1.3191\n",
      "total loss: 13.187618613243103\n",
      "Epoch [135/500], Loss: 1.3188\n",
      "total loss: 13.18645441532135\n",
      "Epoch [136/500], Loss: 1.3186\n",
      "total loss: 13.186258554458618\n",
      "Epoch [137/500], Loss: 1.3186\n",
      "total loss: 13.185310244560242\n",
      "Epoch [138/500], Loss: 1.3185\n",
      "total loss: 13.187893509864807\n",
      "Epoch [139/500], Loss: 1.3188\n",
      "total loss: 13.192829847335815\n",
      "Epoch [140/500], Loss: 1.3193\n",
      "total loss: 13.185642719268799\n",
      "Epoch [141/500], Loss: 1.3186\n",
      "total loss: 13.184139490127563\n",
      "Epoch [142/500], Loss: 1.3184\n",
      "total loss: 13.182780981063843\n",
      "Epoch [143/500], Loss: 1.3183\n",
      "total loss: 13.182942748069763\n",
      "Epoch [144/500], Loss: 1.3183\n",
      "total loss: 13.182419419288635\n",
      "Epoch [145/500], Loss: 1.3182\n",
      "total loss: 13.181545972824097\n",
      "Epoch [146/500], Loss: 1.3182\n",
      "total loss: 13.18298351764679\n",
      "Epoch [147/500], Loss: 1.3183\n",
      "total loss: 13.18064832687378\n",
      "Epoch [148/500], Loss: 1.3181\n",
      "total loss: 13.18142056465149\n",
      "Epoch [149/500], Loss: 1.3181\n",
      "total loss: 13.181129574775696\n",
      "Epoch [150/500], Loss: 1.3181\n",
      "total loss: 13.181795239448547\n",
      "Epoch [151/500], Loss: 1.3182\n",
      "total loss: 13.180469155311584\n",
      "Epoch [152/500], Loss: 1.3180\n",
      "total loss: 13.181259393692017\n",
      "Epoch [153/500], Loss: 1.3181\n",
      "total loss: 13.183091163635254\n",
      "Epoch [154/500], Loss: 1.3183\n",
      "total loss: 13.177663087844849\n",
      "Epoch [155/500], Loss: 1.3178\n",
      "total loss: 13.182137846946716\n",
      "Epoch [156/500], Loss: 1.3182\n",
      "total loss: 13.185308694839478\n",
      "Epoch [157/500], Loss: 1.3185\n",
      "total loss: 13.178289771080017\n",
      "Epoch [158/500], Loss: 1.3178\n",
      "total loss: 13.179372549057007\n",
      "Epoch [159/500], Loss: 1.3179\n",
      "total loss: 13.17828619480133\n",
      "Epoch [160/500], Loss: 1.3178\n",
      "total loss: 13.178211569786072\n",
      "Epoch [161/500], Loss: 1.3178\n",
      "total loss: 13.178072571754456\n",
      "Epoch [162/500], Loss: 1.3178\n",
      "total loss: 13.179576516151428\n",
      "Epoch [163/500], Loss: 1.3180\n",
      "total loss: 13.17596971988678\n",
      "Epoch [164/500], Loss: 1.3176\n",
      "total loss: 13.176173686981201\n",
      "Epoch [165/500], Loss: 1.3176\n",
      "total loss: 13.177780151367188\n",
      "Epoch [166/500], Loss: 1.3178\n",
      "total loss: 13.177639484405518\n",
      "Epoch [167/500], Loss: 1.3178\n",
      "total loss: 13.178139209747314\n",
      "Epoch [168/500], Loss: 1.3178\n",
      "total loss: 13.179213643074036\n",
      "Epoch [169/500], Loss: 1.3179\n",
      "total loss: 13.177618741989136\n",
      "Epoch [170/500], Loss: 1.3178\n",
      "total loss: 13.17643690109253\n",
      "Epoch [171/500], Loss: 1.3176\n",
      "total loss: 13.175910234451294\n",
      "Epoch [172/500], Loss: 1.3176\n",
      "total loss: 13.176164031028748\n",
      "Epoch [173/500], Loss: 1.3176\n",
      "total loss: 13.177412271499634\n",
      "Epoch [174/500], Loss: 1.3177\n",
      "total loss: 13.175280928611755\n",
      "Epoch [175/500], Loss: 1.3175\n",
      "total loss: 13.174304485321045\n",
      "Epoch [176/500], Loss: 1.3174\n",
      "total loss: 13.174894452095032\n",
      "Epoch [177/500], Loss: 1.3175\n",
      "total loss: 13.174422144889832\n",
      "Epoch [178/500], Loss: 1.3174\n",
      "total loss: 13.17532217502594\n",
      "Epoch [179/500], Loss: 1.3175\n",
      "total loss: 13.172069311141968\n",
      "Epoch [180/500], Loss: 1.3172\n",
      "total loss: 13.172767519950867\n",
      "Epoch [181/500], Loss: 1.3173\n",
      "total loss: 13.172874808311462\n",
      "Epoch [182/500], Loss: 1.3173\n",
      "total loss: 13.174616694450378\n",
      "Epoch [183/500], Loss: 1.3175\n",
      "total loss: 13.172536134719849\n",
      "Epoch [184/500], Loss: 1.3173\n",
      "total loss: 13.173998832702637\n",
      "Epoch [185/500], Loss: 1.3174\n",
      "total loss: 13.172024726867676\n",
      "Epoch [186/500], Loss: 1.3172\n",
      "total loss: 13.174832582473755\n",
      "Epoch [187/500], Loss: 1.3175\n",
      "total loss: 13.179241061210632\n",
      "Epoch [188/500], Loss: 1.3179\n",
      "total loss: 13.17368757724762\n",
      "Epoch [189/500], Loss: 1.3174\n",
      "total loss: 13.173080563545227\n",
      "Epoch [190/500], Loss: 1.3173\n",
      "total loss: 13.17414128780365\n",
      "Epoch [191/500], Loss: 1.3174\n",
      "total loss: 13.17454993724823\n",
      "Epoch [192/500], Loss: 1.3175\n",
      "total loss: 13.174802899360657\n",
      "Epoch [193/500], Loss: 1.3175\n",
      "total loss: 13.173391699790955\n",
      "Epoch [194/500], Loss: 1.3173\n",
      "total loss: 13.171771883964539\n",
      "Epoch [195/500], Loss: 1.3172\n",
      "total loss: 13.174257397651672\n",
      "Epoch [196/500], Loss: 1.3174\n",
      "total loss: 13.172179818153381\n",
      "Epoch [197/500], Loss: 1.3172\n",
      "total loss: 13.171921133995056\n",
      "Epoch [198/500], Loss: 1.3172\n",
      "total loss: 13.16880476474762\n",
      "Epoch [199/500], Loss: 1.3169\n",
      "total loss: 13.171184182167053\n",
      "Epoch [200/500], Loss: 1.3171\n",
      "total loss: 13.170879483222961\n",
      "Epoch [201/500], Loss: 1.3171\n",
      "total loss: 13.169309735298157\n",
      "Epoch [202/500], Loss: 1.3169\n",
      "total loss: 13.169526934623718\n",
      "Epoch [203/500], Loss: 1.3170\n",
      "total loss: 13.170048236846924\n",
      "Epoch [204/500], Loss: 1.3170\n",
      "total loss: 13.168852686882019\n",
      "Epoch [205/500], Loss: 1.3169\n",
      "total loss: 13.174044609069824\n",
      "Epoch [206/500], Loss: 1.3174\n",
      "total loss: 13.169153928756714\n",
      "Epoch [207/500], Loss: 1.3169\n",
      "total loss: 13.1702378988266\n",
      "Epoch [208/500], Loss: 1.3170\n",
      "total loss: 13.167742133140564\n",
      "Epoch [209/500], Loss: 1.3168\n",
      "total loss: 13.167555093765259\n",
      "Epoch [210/500], Loss: 1.3168\n",
      "total loss: 13.16715931892395\n",
      "Epoch [211/500], Loss: 1.3167\n",
      "total loss: 13.16778826713562\n",
      "Epoch [212/500], Loss: 1.3168\n",
      "total loss: 13.167301177978516\n",
      "Epoch [213/500], Loss: 1.3167\n",
      "total loss: 13.166932106018066\n",
      "Epoch [214/500], Loss: 1.3167\n",
      "total loss: 13.167819261550903\n",
      "Epoch [215/500], Loss: 1.3168\n",
      "total loss: 13.166243433952332\n",
      "Epoch [216/500], Loss: 1.3166\n",
      "total loss: 13.166344165802002\n",
      "Epoch [217/500], Loss: 1.3166\n",
      "total loss: 13.166202425956726\n",
      "Epoch [218/500], Loss: 1.3166\n",
      "total loss: 13.166699528694153\n",
      "Epoch [219/500], Loss: 1.3167\n",
      "total loss: 13.16662049293518\n",
      "Epoch [220/500], Loss: 1.3167\n",
      "total loss: 13.16552472114563\n",
      "Epoch [221/500], Loss: 1.3166\n",
      "total loss: 13.167471766471863\n",
      "Epoch [222/500], Loss: 1.3167\n",
      "total loss: 13.166233539581299\n",
      "Epoch [223/500], Loss: 1.3166\n",
      "total loss: 13.166496992111206\n",
      "Epoch [224/500], Loss: 1.3166\n",
      "total loss: 13.165961503982544\n",
      "Epoch [225/500], Loss: 1.3166\n",
      "total loss: 13.16524338722229\n",
      "Epoch [226/500], Loss: 1.3165\n",
      "total loss: 13.164769291877747\n",
      "Epoch [227/500], Loss: 1.3165\n",
      "total loss: 13.16659927368164\n",
      "Epoch [228/500], Loss: 1.3167\n",
      "total loss: 13.164636611938477\n",
      "Epoch [229/500], Loss: 1.3165\n",
      "total loss: 13.165813326835632\n",
      "Epoch [230/500], Loss: 1.3166\n",
      "total loss: 13.166606545448303\n",
      "Epoch [231/500], Loss: 1.3167\n",
      "total loss: 13.164662718772888\n",
      "Epoch [232/500], Loss: 1.3165\n",
      "total loss: 13.165548920631409\n",
      "Epoch [233/500], Loss: 1.3166\n",
      "total loss: 13.162829756736755\n",
      "Epoch [234/500], Loss: 1.3163\n",
      "total loss: 13.163721442222595\n",
      "Epoch [235/500], Loss: 1.3164\n",
      "total loss: 13.167653322219849\n",
      "Epoch [236/500], Loss: 1.3168\n",
      "total loss: 13.164716720581055\n",
      "Epoch [237/500], Loss: 1.3165\n",
      "total loss: 13.164130330085754\n",
      "Epoch [238/500], Loss: 1.3164\n",
      "total loss: 13.162841200828552\n",
      "Epoch [239/500], Loss: 1.3163\n",
      "total loss: 13.162530303001404\n",
      "Epoch [240/500], Loss: 1.3163\n",
      "total loss: 13.164060711860657\n",
      "Epoch [241/500], Loss: 1.3164\n",
      "total loss: 13.162757277488708\n",
      "Epoch [242/500], Loss: 1.3163\n",
      "total loss: 13.163496494293213\n",
      "Epoch [243/500], Loss: 1.3163\n",
      "total loss: 13.161525130271912\n",
      "Epoch [244/500], Loss: 1.3162\n",
      "total loss: 13.161702394485474\n",
      "Epoch [245/500], Loss: 1.3162\n",
      "total loss: 13.161651372909546\n",
      "Epoch [246/500], Loss: 1.3162\n",
      "total loss: 13.161826133728027\n",
      "Epoch [247/500], Loss: 1.3162\n",
      "total loss: 13.16176438331604\n",
      "Epoch [248/500], Loss: 1.3162\n",
      "total loss: 13.161700963973999\n",
      "Epoch [249/500], Loss: 1.3162\n",
      "total loss: 13.164953351020813\n",
      "Epoch [250/500], Loss: 1.3165\n",
      "total loss: 13.161920428276062\n",
      "Epoch [251/500], Loss: 1.3162\n",
      "total loss: 13.164093255996704\n",
      "Epoch [252/500], Loss: 1.3164\n",
      "total loss: 13.162147641181946\n",
      "Epoch [253/500], Loss: 1.3162\n",
      "total loss: 13.160873532295227\n",
      "Epoch [254/500], Loss: 1.3161\n",
      "total loss: 13.161651968955994\n",
      "Epoch [255/500], Loss: 1.3162\n",
      "total loss: 13.162612438201904\n",
      "Epoch [256/500], Loss: 1.3163\n",
      "total loss: 13.160594701766968\n",
      "Epoch [257/500], Loss: 1.3161\n",
      "total loss: 13.1603684425354\n",
      "Epoch [258/500], Loss: 1.3160\n",
      "total loss: 13.16066861152649\n",
      "Epoch [259/500], Loss: 1.3161\n",
      "total loss: 13.16153609752655\n",
      "Epoch [260/500], Loss: 1.3162\n",
      "total loss: 13.158719301223755\n",
      "Epoch [261/500], Loss: 1.3159\n",
      "total loss: 13.160719513893127\n",
      "Epoch [262/500], Loss: 1.3161\n",
      "total loss: 13.159458637237549\n",
      "Epoch [263/500], Loss: 1.3159\n",
      "total loss: 13.163710236549377\n",
      "Epoch [264/500], Loss: 1.3164\n",
      "total loss: 13.158803343772888\n",
      "Epoch [265/500], Loss: 1.3159\n",
      "total loss: 13.158480167388916\n",
      "Epoch [266/500], Loss: 1.3158\n",
      "total loss: 13.15928041934967\n",
      "Epoch [267/500], Loss: 1.3159\n",
      "total loss: 13.158941507339478\n",
      "Epoch [268/500], Loss: 1.3159\n",
      "total loss: 13.159587621688843\n",
      "Epoch [269/500], Loss: 1.3160\n",
      "total loss: 13.160155653953552\n",
      "Epoch [270/500], Loss: 1.3160\n",
      "total loss: 13.159995317459106\n",
      "Epoch [271/500], Loss: 1.3160\n",
      "total loss: 13.15764856338501\n",
      "Epoch [272/500], Loss: 1.3158\n",
      "total loss: 13.15946888923645\n",
      "Epoch [273/500], Loss: 1.3159\n",
      "total loss: 13.158187508583069\n",
      "Epoch [274/500], Loss: 1.3158\n",
      "total loss: 13.158648371696472\n",
      "Epoch [275/500], Loss: 1.3159\n",
      "total loss: 13.158155679702759\n",
      "Epoch [276/500], Loss: 1.3158\n",
      "total loss: 13.158095359802246\n",
      "Epoch [277/500], Loss: 1.3158\n",
      "total loss: 13.157556056976318\n",
      "Epoch [278/500], Loss: 1.3158\n",
      "total loss: 13.158027052879333\n",
      "Epoch [279/500], Loss: 1.3158\n",
      "total loss: 13.158360600471497\n",
      "Epoch [280/500], Loss: 1.3158\n",
      "total loss: 13.157146215438843\n",
      "Epoch [281/500], Loss: 1.3157\n",
      "total loss: 13.159566044807434\n",
      "Epoch [282/500], Loss: 1.3160\n",
      "total loss: 13.159255266189575\n",
      "Epoch [283/500], Loss: 1.3159\n",
      "total loss: 13.157348275184631\n",
      "Epoch [284/500], Loss: 1.3157\n",
      "total loss: 13.158882975578308\n",
      "Epoch [285/500], Loss: 1.3159\n",
      "total loss: 13.156615257263184\n",
      "Epoch [286/500], Loss: 1.3157\n",
      "total loss: 13.158626794815063\n",
      "Epoch [287/500], Loss: 1.3159\n",
      "total loss: 13.157450437545776\n",
      "Epoch [288/500], Loss: 1.3157\n",
      "total loss: 13.155823469161987\n",
      "Epoch [289/500], Loss: 1.3156\n",
      "total loss: 13.157032132148743\n",
      "Epoch [290/500], Loss: 1.3157\n",
      "total loss: 13.156180381774902\n",
      "Epoch [291/500], Loss: 1.3156\n",
      "total loss: 13.156208276748657\n",
      "Epoch [292/500], Loss: 1.3156\n",
      "total loss: 13.155912041664124\n",
      "Epoch [293/500], Loss: 1.3156\n",
      "total loss: 13.157197713851929\n",
      "Epoch [294/500], Loss: 1.3157\n",
      "total loss: 13.156218886375427\n",
      "Epoch [295/500], Loss: 1.3156\n",
      "total loss: 13.155144572257996\n",
      "Epoch [296/500], Loss: 1.3155\n",
      "total loss: 13.155523896217346\n",
      "Epoch [297/500], Loss: 1.3156\n",
      "total loss: 13.155760288238525\n",
      "Epoch [298/500], Loss: 1.3156\n",
      "total loss: 13.157187342643738\n",
      "Epoch [299/500], Loss: 1.3157\n",
      "total loss: 13.157997369766235\n",
      "Epoch [300/500], Loss: 1.3158\n",
      "total loss: 13.155195713043213\n",
      "Epoch [301/500], Loss: 1.3155\n",
      "total loss: 13.157824039459229\n",
      "Epoch [302/500], Loss: 1.3158\n",
      "total loss: 13.154358983039856\n",
      "Epoch [303/500], Loss: 1.3154\n",
      "total loss: 13.156282544136047\n",
      "Epoch [304/500], Loss: 1.3156\n",
      "total loss: 13.155386567115784\n",
      "Epoch [305/500], Loss: 1.3155\n",
      "total loss: 13.154865860939026\n",
      "Epoch [306/500], Loss: 1.3155\n",
      "total loss: 13.155635952949524\n",
      "Epoch [307/500], Loss: 1.3156\n",
      "total loss: 13.15438687801361\n",
      "Epoch [308/500], Loss: 1.3154\n",
      "total loss: 13.154901504516602\n",
      "Epoch [309/500], Loss: 1.3155\n",
      "total loss: 13.153173208236694\n",
      "Epoch [310/500], Loss: 1.3153\n",
      "total loss: 13.1543710231781\n",
      "Epoch [311/500], Loss: 1.3154\n",
      "total loss: 13.153393149375916\n",
      "Epoch [312/500], Loss: 1.3153\n",
      "total loss: 13.153762698173523\n",
      "Epoch [313/500], Loss: 1.3154\n",
      "total loss: 13.15399694442749\n",
      "Epoch [314/500], Loss: 1.3154\n",
      "total loss: 13.15360951423645\n",
      "Epoch [315/500], Loss: 1.3154\n",
      "total loss: 13.15414834022522\n",
      "Epoch [316/500], Loss: 1.3154\n",
      "total loss: 13.153227806091309\n",
      "Epoch [317/500], Loss: 1.3153\n",
      "total loss: 13.153307676315308\n",
      "Epoch [318/500], Loss: 1.3153\n",
      "total loss: 13.153138279914856\n",
      "Epoch [319/500], Loss: 1.3153\n",
      "total loss: 13.1536945104599\n",
      "Epoch [320/500], Loss: 1.3154\n",
      "total loss: 13.153148412704468\n",
      "Epoch [321/500], Loss: 1.3153\n",
      "total loss: 13.153505206108093\n",
      "Epoch [322/500], Loss: 1.3154\n",
      "total loss: 13.15275776386261\n",
      "Epoch [323/500], Loss: 1.3153\n",
      "total loss: 13.152888536453247\n",
      "Epoch [324/500], Loss: 1.3153\n",
      "total loss: 13.153711199760437\n",
      "Epoch [325/500], Loss: 1.3154\n",
      "total loss: 13.153180122375488\n",
      "Epoch [326/500], Loss: 1.3153\n",
      "total loss: 13.152921795845032\n",
      "Epoch [327/500], Loss: 1.3153\n",
      "total loss: 13.154176354408264\n",
      "Epoch [328/500], Loss: 1.3154\n",
      "total loss: 13.152302145957947\n",
      "Epoch [329/500], Loss: 1.3152\n",
      "total loss: 13.15207827091217\n",
      "Epoch [330/500], Loss: 1.3152\n",
      "total loss: 13.152920365333557\n",
      "Epoch [331/500], Loss: 1.3153\n",
      "total loss: 13.151842951774597\n",
      "Epoch [332/500], Loss: 1.3152\n",
      "total loss: 13.153446793556213\n",
      "Epoch [333/500], Loss: 1.3153\n",
      "total loss: 13.15282690525055\n",
      "Epoch [334/500], Loss: 1.3153\n",
      "total loss: 13.152746081352234\n",
      "Epoch [335/500], Loss: 1.3153\n",
      "total loss: 13.152580499649048\n",
      "Epoch [336/500], Loss: 1.3153\n",
      "total loss: 13.151963591575623\n",
      "Epoch [337/500], Loss: 1.3152\n",
      "total loss: 13.151144862174988\n",
      "Epoch [338/500], Loss: 1.3151\n",
      "total loss: 13.152098059654236\n",
      "Epoch [339/500], Loss: 1.3152\n",
      "total loss: 13.15221083164215\n",
      "Epoch [340/500], Loss: 1.3152\n",
      "total loss: 13.150915741920471\n",
      "Epoch [341/500], Loss: 1.3151\n",
      "total loss: 13.151516556739807\n",
      "Epoch [342/500], Loss: 1.3152\n",
      "total loss: 13.151075959205627\n",
      "Epoch [343/500], Loss: 1.3151\n",
      "total loss: 13.15096402168274\n",
      "Epoch [344/500], Loss: 1.3151\n",
      "total loss: 13.150876998901367\n",
      "Epoch [345/500], Loss: 1.3151\n",
      "total loss: 13.151029467582703\n",
      "Epoch [346/500], Loss: 1.3151\n",
      "total loss: 13.1508229970932\n",
      "Epoch [347/500], Loss: 1.3151\n",
      "total loss: 13.151628375053406\n",
      "Epoch [348/500], Loss: 1.3152\n",
      "total loss: 13.151545643806458\n",
      "Epoch [349/500], Loss: 1.3152\n",
      "total loss: 13.150336623191833\n",
      "Epoch [350/500], Loss: 1.3150\n",
      "total loss: 13.150343656539917\n",
      "Epoch [351/500], Loss: 1.3150\n",
      "total loss: 13.150604248046875\n",
      "Epoch [352/500], Loss: 1.3151\n",
      "total loss: 13.150198459625244\n",
      "Epoch [353/500], Loss: 1.3150\n",
      "total loss: 13.15095341205597\n",
      "Epoch [354/500], Loss: 1.3151\n",
      "total loss: 13.150933384895325\n",
      "Epoch [355/500], Loss: 1.3151\n",
      "total loss: 13.150152206420898\n",
      "Epoch [356/500], Loss: 1.3150\n",
      "total loss: 13.150072574615479\n",
      "Epoch [357/500], Loss: 1.3150\n",
      "total loss: 13.150300860404968\n",
      "Epoch [358/500], Loss: 1.3150\n",
      "total loss: 13.150698065757751\n",
      "Epoch [359/500], Loss: 1.3151\n",
      "total loss: 13.149522423744202\n",
      "Epoch [360/500], Loss: 1.3150\n",
      "total loss: 13.148882627487183\n",
      "Epoch [361/500], Loss: 1.3149\n",
      "total loss: 13.15078616142273\n",
      "Epoch [362/500], Loss: 1.3151\n",
      "total loss: 13.149556756019592\n",
      "Epoch [363/500], Loss: 1.3150\n",
      "total loss: 13.149465918540955\n",
      "Epoch [364/500], Loss: 1.3149\n",
      "total loss: 13.148638844490051\n",
      "Epoch [365/500], Loss: 1.3149\n",
      "total loss: 13.149563193321228\n",
      "Epoch [366/500], Loss: 1.3150\n",
      "total loss: 13.149864435195923\n",
      "Epoch [367/500], Loss: 1.3150\n",
      "total loss: 13.148595094680786\n",
      "Epoch [368/500], Loss: 1.3149\n",
      "total loss: 13.148453712463379\n",
      "Epoch [369/500], Loss: 1.3148\n",
      "total loss: 13.148931741714478\n",
      "Epoch [370/500], Loss: 1.3149\n",
      "total loss: 13.148577451705933\n",
      "Epoch [371/500], Loss: 1.3149\n",
      "total loss: 13.149588346481323\n",
      "Epoch [372/500], Loss: 1.3150\n",
      "total loss: 13.148474097251892\n",
      "Epoch [373/500], Loss: 1.3148\n",
      "total loss: 13.148436069488525\n",
      "Epoch [374/500], Loss: 1.3148\n",
      "total loss: 13.14905333518982\n",
      "Epoch [375/500], Loss: 1.3149\n",
      "total loss: 13.149107933044434\n",
      "Epoch [376/500], Loss: 1.3149\n",
      "total loss: 13.149528503417969\n",
      "Epoch [377/500], Loss: 1.3150\n",
      "total loss: 13.149954199790955\n",
      "Epoch [378/500], Loss: 1.3150\n",
      "total loss: 13.147851467132568\n",
      "Epoch [379/500], Loss: 1.3148\n",
      "total loss: 13.147741436958313\n",
      "Epoch [380/500], Loss: 1.3148\n",
      "total loss: 13.147668957710266\n",
      "Epoch [381/500], Loss: 1.3148\n",
      "total loss: 13.147752285003662\n",
      "Epoch [382/500], Loss: 1.3148\n",
      "total loss: 13.14832091331482\n",
      "Epoch [383/500], Loss: 1.3148\n",
      "total loss: 13.147975087165833\n",
      "Epoch [384/500], Loss: 1.3148\n",
      "total loss: 13.148134589195251\n",
      "Epoch [385/500], Loss: 1.3148\n",
      "total loss: 13.147103071212769\n",
      "Epoch [386/500], Loss: 1.3147\n",
      "total loss: 13.148240208625793\n",
      "Epoch [387/500], Loss: 1.3148\n",
      "total loss: 13.147077798843384\n",
      "Epoch [388/500], Loss: 1.3147\n",
      "total loss: 13.147591233253479\n",
      "Epoch [389/500], Loss: 1.3148\n",
      "total loss: 13.147296786308289\n",
      "Epoch [390/500], Loss: 1.3147\n",
      "total loss: 13.147568702697754\n",
      "Epoch [391/500], Loss: 1.3148\n",
      "total loss: 13.147498965263367\n",
      "Epoch [392/500], Loss: 1.3147\n",
      "total loss: 13.147225379943848\n",
      "Epoch [393/500], Loss: 1.3147\n",
      "total loss: 13.148008584976196\n",
      "Epoch [394/500], Loss: 1.3148\n",
      "total loss: 13.147169947624207\n",
      "Epoch [395/500], Loss: 1.3147\n",
      "total loss: 13.147223114967346\n",
      "Epoch [396/500], Loss: 1.3147\n",
      "total loss: 13.147895693778992\n",
      "Epoch [397/500], Loss: 1.3148\n",
      "total loss: 13.14654803276062\n",
      "Epoch [398/500], Loss: 1.3147\n",
      "total loss: 13.146549224853516\n",
      "Epoch [399/500], Loss: 1.3147\n",
      "total loss: 13.146348357200623\n",
      "Epoch [400/500], Loss: 1.3146\n",
      "total loss: 13.146495461463928\n",
      "Epoch [401/500], Loss: 1.3146\n",
      "total loss: 13.146565675735474\n",
      "Epoch [402/500], Loss: 1.3147\n",
      "total loss: 13.14594566822052\n",
      "Epoch [403/500], Loss: 1.3146\n",
      "total loss: 13.146361470222473\n",
      "Epoch [404/500], Loss: 1.3146\n",
      "total loss: 13.146255731582642\n",
      "Epoch [405/500], Loss: 1.3146\n",
      "total loss: 13.146065354347229\n",
      "Epoch [406/500], Loss: 1.3146\n",
      "total loss: 13.146442770957947\n",
      "Epoch [407/500], Loss: 1.3146\n",
      "total loss: 13.146332144737244\n",
      "Epoch [408/500], Loss: 1.3146\n",
      "total loss: 13.146305680274963\n",
      "Epoch [409/500], Loss: 1.3146\n",
      "total loss: 13.14568042755127\n",
      "Epoch [410/500], Loss: 1.3146\n",
      "total loss: 13.147376537322998\n",
      "Epoch [411/500], Loss: 1.3147\n",
      "total loss: 13.14602792263031\n",
      "Epoch [412/500], Loss: 1.3146\n",
      "total loss: 13.145835280418396\n",
      "Epoch [413/500], Loss: 1.3146\n",
      "total loss: 13.146057963371277\n",
      "Epoch [414/500], Loss: 1.3146\n",
      "total loss: 13.145545363426208\n",
      "Epoch [415/500], Loss: 1.3146\n",
      "total loss: 13.145894050598145\n",
      "Epoch [416/500], Loss: 1.3146\n",
      "total loss: 13.145991921424866\n",
      "Epoch [417/500], Loss: 1.3146\n",
      "total loss: 13.145586609840393\n",
      "Epoch [418/500], Loss: 1.3146\n",
      "total loss: 13.145417928695679\n",
      "Epoch [419/500], Loss: 1.3145\n",
      "total loss: 13.145256638526917\n",
      "Epoch [420/500], Loss: 1.3145\n",
      "total loss: 13.145231008529663\n",
      "Epoch [421/500], Loss: 1.3145\n",
      "total loss: 13.145696997642517\n",
      "Epoch [422/500], Loss: 1.3146\n",
      "total loss: 13.145390510559082\n",
      "Epoch [423/500], Loss: 1.3145\n",
      "total loss: 13.145104169845581\n",
      "Epoch [424/500], Loss: 1.3145\n",
      "total loss: 13.144921064376831\n",
      "Epoch [425/500], Loss: 1.3145\n",
      "total loss: 13.145463705062866\n",
      "Epoch [426/500], Loss: 1.3145\n",
      "total loss: 13.144980907440186\n",
      "Epoch [427/500], Loss: 1.3145\n",
      "total loss: 13.145153403282166\n",
      "Epoch [428/500], Loss: 1.3145\n",
      "total loss: 13.145195484161377\n",
      "Epoch [429/500], Loss: 1.3145\n",
      "total loss: 13.145036339759827\n",
      "Epoch [430/500], Loss: 1.3145\n",
      "total loss: 13.14487898349762\n",
      "Epoch [431/500], Loss: 1.3145\n",
      "total loss: 13.145107865333557\n",
      "Epoch [432/500], Loss: 1.3145\n",
      "total loss: 13.144753575325012\n",
      "Epoch [433/500], Loss: 1.3145\n",
      "total loss: 13.144835233688354\n",
      "Epoch [434/500], Loss: 1.3145\n",
      "total loss: 13.144668221473694\n",
      "Epoch [435/500], Loss: 1.3145\n",
      "total loss: 13.144840002059937\n",
      "Epoch [436/500], Loss: 1.3145\n",
      "total loss: 13.14436662197113\n",
      "Epoch [437/500], Loss: 1.3144\n",
      "total loss: 13.144439458847046\n",
      "Epoch [438/500], Loss: 1.3144\n",
      "total loss: 13.144835829734802\n",
      "Epoch [439/500], Loss: 1.3145\n",
      "total loss: 13.144293189048767\n",
      "Epoch [440/500], Loss: 1.3144\n",
      "total loss: 13.144041657447815\n",
      "Epoch [441/500], Loss: 1.3144\n",
      "total loss: 13.144266247749329\n",
      "Epoch [442/500], Loss: 1.3144\n",
      "total loss: 13.144834399223328\n",
      "Epoch [443/500], Loss: 1.3145\n",
      "total loss: 13.144200801849365\n",
      "Epoch [444/500], Loss: 1.3144\n",
      "total loss: 13.144268989562988\n",
      "Epoch [445/500], Loss: 1.3144\n",
      "total loss: 13.144458770751953\n",
      "Epoch [446/500], Loss: 1.3144\n",
      "total loss: 13.143889904022217\n",
      "Epoch [447/500], Loss: 1.3144\n",
      "total loss: 13.144385695457458\n",
      "Epoch [448/500], Loss: 1.3144\n",
      "total loss: 13.143905282020569\n",
      "Epoch [449/500], Loss: 1.3144\n",
      "total loss: 13.144017457962036\n",
      "Epoch [450/500], Loss: 1.3144\n",
      "total loss: 13.1440988779068\n",
      "Epoch [451/500], Loss: 1.3144\n",
      "total loss: 13.144383788108826\n",
      "Epoch [452/500], Loss: 1.3144\n",
      "total loss: 13.14345407485962\n",
      "Epoch [453/500], Loss: 1.3143\n",
      "total loss: 13.14363181591034\n",
      "Epoch [454/500], Loss: 1.3144\n",
      "total loss: 13.143473863601685\n",
      "Epoch [455/500], Loss: 1.3143\n",
      "total loss: 13.143343091011047\n",
      "Epoch [456/500], Loss: 1.3143\n",
      "total loss: 13.143667817115784\n",
      "Epoch [457/500], Loss: 1.3144\n",
      "total loss: 13.143218278884888\n",
      "Epoch [458/500], Loss: 1.3143\n",
      "total loss: 13.143523097038269\n",
      "Epoch [459/500], Loss: 1.3144\n",
      "total loss: 13.143583416938782\n",
      "Epoch [460/500], Loss: 1.3144\n",
      "total loss: 13.143412828445435\n",
      "Epoch [461/500], Loss: 1.3143\n",
      "total loss: 13.143033266067505\n",
      "Epoch [462/500], Loss: 1.3143\n",
      "total loss: 13.14322316646576\n",
      "Epoch [463/500], Loss: 1.3143\n",
      "total loss: 13.144430875778198\n",
      "Epoch [464/500], Loss: 1.3144\n",
      "total loss: 13.142996549606323\n",
      "Epoch [465/500], Loss: 1.3143\n",
      "total loss: 13.143118739128113\n",
      "Epoch [466/500], Loss: 1.3143\n",
      "total loss: 13.142722129821777\n",
      "Epoch [467/500], Loss: 1.3143\n",
      "total loss: 13.143641591072083\n",
      "Epoch [468/500], Loss: 1.3144\n",
      "total loss: 13.143213033676147\n",
      "Epoch [469/500], Loss: 1.3143\n",
      "total loss: 13.143710494041443\n",
      "Epoch [470/500], Loss: 1.3144\n",
      "total loss: 13.143145322799683\n",
      "Epoch [471/500], Loss: 1.3143\n",
      "total loss: 13.14307713508606\n",
      "Epoch [472/500], Loss: 1.3143\n",
      "total loss: 13.143138527870178\n",
      "Epoch [473/500], Loss: 1.3143\n",
      "total loss: 13.142589569091797\n",
      "Epoch [474/500], Loss: 1.3143\n",
      "total loss: 13.143172979354858\n",
      "Epoch [475/500], Loss: 1.3143\n",
      "total loss: 13.14336359500885\n",
      "Epoch [476/500], Loss: 1.3143\n",
      "total loss: 13.143027186393738\n",
      "Epoch [477/500], Loss: 1.3143\n",
      "total loss: 13.14329743385315\n",
      "Epoch [478/500], Loss: 1.3143\n",
      "total loss: 13.14302384853363\n",
      "Epoch [479/500], Loss: 1.3143\n",
      "total loss: 13.142730593681335\n",
      "Epoch [480/500], Loss: 1.3143\n",
      "total loss: 13.142920970916748\n",
      "Epoch [481/500], Loss: 1.3143\n",
      "total loss: 13.142718434333801\n",
      "Epoch [482/500], Loss: 1.3143\n",
      "total loss: 13.142600178718567\n",
      "Epoch [483/500], Loss: 1.3143\n",
      "total loss: 13.14274549484253\n",
      "Epoch [484/500], Loss: 1.3143\n",
      "total loss: 13.142774820327759\n",
      "Epoch [485/500], Loss: 1.3143\n",
      "total loss: 13.14220118522644\n",
      "Epoch [486/500], Loss: 1.3142\n",
      "total loss: 13.141988039016724\n",
      "Epoch [487/500], Loss: 1.3142\n",
      "total loss: 13.142163157463074\n",
      "Epoch [488/500], Loss: 1.3142\n",
      "total loss: 13.142452478408813\n",
      "Epoch [489/500], Loss: 1.3142\n",
      "total loss: 13.141918420791626\n",
      "Epoch [490/500], Loss: 1.3142\n",
      "total loss: 13.141942977905273\n",
      "Epoch [491/500], Loss: 1.3142\n",
      "total loss: 13.141921401023865\n",
      "Epoch [492/500], Loss: 1.3142\n",
      "total loss: 13.141712546348572\n",
      "Epoch [493/500], Loss: 1.3142\n",
      "total loss: 13.141966342926025\n",
      "Epoch [494/500], Loss: 1.3142\n",
      "total loss: 13.142077445983887\n",
      "Epoch [495/500], Loss: 1.3142\n",
      "total loss: 13.141634345054626\n",
      "Epoch [496/500], Loss: 1.3142\n",
      "total loss: 13.142061233520508\n",
      "Epoch [497/500], Loss: 1.3142\n",
      "total loss: 13.14178991317749\n",
      "Epoch [498/500], Loss: 1.3142\n",
      "total loss: 13.141963243484497\n",
      "Epoch [499/500], Loss: 1.3142\n",
      "total loss: 13.141766548156738\n",
      "Epoch [500/500], Loss: 1.3142\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model\n",
    "trained_model = train_model(input_data, desired_output, 500, learning_rate=0.0005, viz_accuracy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(trained_model, new_data, expected_result):\n",
    "    test_input = torch.tensor(new_data, dtype=torch.float32)\n",
    "\n",
    "    test_output = trained_model(test_input)\n",
    "\n",
    "    predicted_classes = torch.argmax(test_output, dim=1)\n",
    "\n",
    "    test_output_compare = torch.tensor(expected_result, dtype=torch.int)\n",
    "\n",
    "    return float(sum(test_output_compare == predicted_classes)/len(test_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"../Data_Generation/Data_files/heisenberg_data_test.csv\")\n",
    "test_df.columns = ['val1', 'val2', 'val3', \"last_matrix\"]\n",
    "just_input_test = test_df.drop('last_matrix', axis=1)\n",
    "input_data_test = np.array(just_input_test.values.tolist())\n",
    "test_df['last_matrix'] = test_df['last_matrix'] - 1\n",
    "desired_output = torch.tensor(test_df['last_matrix'].tolist(), dtype=torch.float32).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16089\\AppData\\Local\\Temp\\ipykernel_12212\\875939923.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_output_compare = torch.tensor(expected_result, dtype=torch.int)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.36433643102645874"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can get about 36% accuracy\n",
    "test_model(trained_model, input_data_test, desired_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a vizualization of what we would predict for a bunch of ordered pairs\n",
    "\n",
    "# TODO figure out how to visualize this when we have 3 points\n",
    "# confidence_df = pd.DataFrame({\n",
    "#     'x': [],\n",
    "#     'y': [],\n",
    "#     'confidence': []\n",
    "# })\n",
    "\n",
    "# for i in range(-5000000000000000000, 5000000000000000001, 200000000000000000):\n",
    "#     for j in range(-5000000000000000000, 5000000000000000001, 200000000000000000):\n",
    "#         demo_test_type = torch.tensor([[i, j]], dtype=torch.float32)\n",
    "#         confidence = int(torch.argmax(trained_model(demo_test_type)))\n",
    "#         df2 = pd.DataFrame([[i, j, confidence]], columns=['x', 'y', 'confidence'])\n",
    "#         confidence_df = pd.concat([confidence_df, df2])\n",
    "# plt.scatter(confidence_df['x'], confidence_df['y'], c=confidence_df['confidence'])\n",
    "# plt.xlabel(\"point1\")\n",
    "# plt.ylabel(\"point2\")\n",
    "# plt.title(\"Decision boundary between predicting last matrix applied to the vector\")\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
