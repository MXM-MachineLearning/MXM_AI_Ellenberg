{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code adapted from ChatGPT\n",
    "\n",
    "# Define your neural network model\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "\n",
    "        self.step1 = nn.Linear(3, 128, bias=True)\n",
    "        self.step2 = nn.ReLU()\n",
    "        self.step3 = nn.Linear(128, 64, bias=True)\n",
    "        self.step4 = nn.ReLU()\n",
    "        self.step5 = nn.Linear(64, 16, bias=True)\n",
    "        self.step6 = nn.ReLU()\n",
    "        self.step7 = nn.Linear(16, 4, bias=True)\n",
    "\n",
    "        # multi-class classification adapted from ChatGPT\n",
    "        self.step8 = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # RUN IT ON A GPU if it exists\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.to(\"cuda\")\n",
    "\n",
    "        x = self.step1(x)\n",
    "        x = self.step2(x)\n",
    "        x = self.step3(x)\n",
    "        x = self.step4(x)\n",
    "        x = self.step5(x)\n",
    "        x = self.step6(x)\n",
    "        x = self.step7(x)\n",
    "        x = self.step8(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def train_model(inputs, desired_outputs, num_epochs=100, learning_rate=0.01, viz_accuracy=False):\n",
    "    # Convert inputs and desired_outputs to PyTorch tensors\n",
    "    inputs = torch.tensor(inputs, dtype=torch.float32)\n",
    "    desired_outputs = torch.tensor(desired_outputs, dtype=torch.float32)\n",
    "    \n",
    "    # Create a DataLoader to handle batching (if needed)\n",
    "    dataset = TensorDataset(inputs, desired_outputs)\n",
    "    dataloader = DataLoader(dataset, batch_size=1000, shuffle=True)  # Adjust batch_size as needed\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = SimpleModel()\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()  # Mean Squared Error loss\n",
    "    # optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "    # criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.995)\n",
    "    losses = []\n",
    "\n",
    "    inputs = inputs.float()\n",
    "    desired_outputs = desired_outputs.long()\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for batch_inputs, batch_desired_outputs in dataloader:\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            outputs = model(batch_inputs)  # Forward pass\n",
    "\n",
    "            batch_desired_outputs = batch_desired_outputs.long()\n",
    "\n",
    "            loss = criterion(outputs, batch_desired_outputs)  # Compute the loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update the model's parameters\n",
    "            cur_item = loss.item()\n",
    "            total_loss += cur_item\n",
    "        \n",
    "        # Print the average loss for this epoch\n",
    "        scheduler.step()\n",
    "        print(f\"total loss: {total_loss}\")\n",
    "        average_loss = total_loss / len(dataloader)\n",
    "        if viz_accuracy:\n",
    "            losses.append(average_loss)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.4f}')\n",
    "    \n",
    "    if viz_accuracy:\n",
    "        plt.scatter(x=range(1, len(losses)+1), y=losses)\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Average loss\")\n",
    "        plt.show()\n",
    "\n",
    "    # Return the trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"learned_Q_moves.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "just_input = df.drop('last_matrix', axis=1)\n",
    "just_input = just_input.drop('num_moves_Q_learning_needs', axis=1)\n",
    "\n",
    "# adapted from https://stackoverflow.com/questions/43898035/pandas-combine-column-values-into-a-list-in-a-new-column\n",
    "input_data = np.array(just_input.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_output = torch.tensor(df['num_moves_Q_learning_needs'].tolist(), dtype=torch.float32).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16089\\AppData\\Local\\Temp\\ipykernel_17528\\2668634726.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  desired_outputs = torch.tensor(desired_outputs, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total loss: 4.169216752052307\n",
      "Epoch [1/500], Loss: 1.3897\n",
      "total loss: 4.1396753787994385\n",
      "Epoch [2/500], Loss: 1.3799\n",
      "total loss: 4.114672064781189\n",
      "Epoch [3/500], Loss: 1.3716\n",
      "total loss: 4.092397451400757\n",
      "Epoch [4/500], Loss: 1.3641\n",
      "total loss: 4.070271611213684\n",
      "Epoch [5/500], Loss: 1.3568\n",
      "total loss: 4.047674298286438\n",
      "Epoch [6/500], Loss: 1.3492\n",
      "total loss: 4.025381565093994\n",
      "Epoch [7/500], Loss: 1.3418\n",
      "total loss: 4.000218987464905\n",
      "Epoch [8/500], Loss: 1.3334\n",
      "total loss: 3.9739266633987427\n",
      "Epoch [9/500], Loss: 1.3246\n",
      "total loss: 3.944557547569275\n",
      "Epoch [10/500], Loss: 1.3149\n",
      "total loss: 3.913728952407837\n",
      "Epoch [11/500], Loss: 1.3046\n",
      "total loss: 3.88240647315979\n",
      "Epoch [12/500], Loss: 1.2941\n",
      "total loss: 3.852086663246155\n",
      "Epoch [13/500], Loss: 1.2840\n",
      "total loss: 3.8219372034072876\n",
      "Epoch [14/500], Loss: 1.2740\n",
      "total loss: 3.7925093173980713\n",
      "Epoch [15/500], Loss: 1.2642\n",
      "total loss: 3.7646394968032837\n",
      "Epoch [16/500], Loss: 1.2549\n",
      "total loss: 3.738114595413208\n",
      "Epoch [17/500], Loss: 1.2460\n",
      "total loss: 3.7136930227279663\n",
      "Epoch [18/500], Loss: 1.2379\n",
      "total loss: 3.690522074699402\n",
      "Epoch [19/500], Loss: 1.2302\n",
      "total loss: 3.6713911294937134\n",
      "Epoch [20/500], Loss: 1.2238\n",
      "total loss: 3.650312662124634\n",
      "Epoch [21/500], Loss: 1.2168\n",
      "total loss: 3.632861852645874\n",
      "Epoch [22/500], Loss: 1.2110\n",
      "total loss: 3.6176012754440308\n",
      "Epoch [23/500], Loss: 1.2059\n",
      "total loss: 3.6027101278305054\n",
      "Epoch [24/500], Loss: 1.2009\n",
      "total loss: 3.590669870376587\n",
      "Epoch [25/500], Loss: 1.1969\n",
      "total loss: 3.5785449743270874\n",
      "Epoch [26/500], Loss: 1.1928\n",
      "total loss: 3.568209409713745\n",
      "Epoch [27/500], Loss: 1.1894\n",
      "total loss: 3.5577460527420044\n",
      "Epoch [28/500], Loss: 1.1859\n",
      "total loss: 3.5501983165740967\n",
      "Epoch [29/500], Loss: 1.1834\n",
      "total loss: 3.5404127836227417\n",
      "Epoch [30/500], Loss: 1.1801\n",
      "total loss: 3.5340898036956787\n",
      "Epoch [31/500], Loss: 1.1780\n",
      "total loss: 3.527220368385315\n",
      "Epoch [32/500], Loss: 1.1757\n",
      "total loss: 3.5202614068984985\n",
      "Epoch [33/500], Loss: 1.1734\n",
      "total loss: 3.5136629343032837\n",
      "Epoch [34/500], Loss: 1.1712\n",
      "total loss: 3.5085980892181396\n",
      "Epoch [35/500], Loss: 1.1695\n",
      "total loss: 3.5028187036514282\n",
      "Epoch [36/500], Loss: 1.1676\n",
      "total loss: 3.4981212615966797\n",
      "Epoch [37/500], Loss: 1.1660\n",
      "total loss: 3.494119882583618\n",
      "Epoch [38/500], Loss: 1.1647\n",
      "total loss: 3.489930510520935\n",
      "Epoch [39/500], Loss: 1.1633\n",
      "total loss: 3.485118269920349\n",
      "Epoch [40/500], Loss: 1.1617\n",
      "total loss: 3.480699896812439\n",
      "Epoch [41/500], Loss: 1.1602\n",
      "total loss: 3.476511836051941\n",
      "Epoch [42/500], Loss: 1.1588\n",
      "total loss: 3.472712278366089\n",
      "Epoch [43/500], Loss: 1.1576\n",
      "total loss: 3.471186876296997\n",
      "Epoch [44/500], Loss: 1.1571\n",
      "total loss: 3.4659528732299805\n",
      "Epoch [45/500], Loss: 1.1553\n",
      "total loss: 3.4635965824127197\n",
      "Epoch [46/500], Loss: 1.1545\n",
      "total loss: 3.460072636604309\n",
      "Epoch [47/500], Loss: 1.1534\n",
      "total loss: 3.457295060157776\n",
      "Epoch [48/500], Loss: 1.1524\n",
      "total loss: 3.4549275636672974\n",
      "Epoch [49/500], Loss: 1.1516\n",
      "total loss: 3.4523123502731323\n",
      "Epoch [50/500], Loss: 1.1508\n",
      "total loss: 3.4494779109954834\n",
      "Epoch [51/500], Loss: 1.1498\n",
      "total loss: 3.4470185041427612\n",
      "Epoch [52/500], Loss: 1.1490\n",
      "total loss: 3.445610523223877\n",
      "Epoch [53/500], Loss: 1.1485\n",
      "total loss: 3.44286572933197\n",
      "Epoch [54/500], Loss: 1.1476\n",
      "total loss: 3.440101385116577\n",
      "Epoch [55/500], Loss: 1.1467\n",
      "total loss: 3.438528895378113\n",
      "Epoch [56/500], Loss: 1.1462\n",
      "total loss: 3.4357420206069946\n",
      "Epoch [57/500], Loss: 1.1452\n",
      "total loss: 3.4332797527313232\n",
      "Epoch [58/500], Loss: 1.1444\n",
      "total loss: 3.431922197341919\n",
      "Epoch [59/500], Loss: 1.1440\n",
      "total loss: 3.4292361736297607\n",
      "Epoch [60/500], Loss: 1.1431\n",
      "total loss: 3.4272403717041016\n",
      "Epoch [61/500], Loss: 1.1424\n",
      "total loss: 3.4263453483581543\n",
      "Epoch [62/500], Loss: 1.1421\n",
      "total loss: 3.4236785173416138\n",
      "Epoch [63/500], Loss: 1.1412\n",
      "total loss: 3.423117518424988\n",
      "Epoch [64/500], Loss: 1.1410\n",
      "total loss: 3.4210973978042603\n",
      "Epoch [65/500], Loss: 1.1404\n",
      "total loss: 3.4191927909851074\n",
      "Epoch [66/500], Loss: 1.1397\n",
      "total loss: 3.4167966842651367\n",
      "Epoch [67/500], Loss: 1.1389\n",
      "total loss: 3.4158239364624023\n",
      "Epoch [68/500], Loss: 1.1386\n",
      "total loss: 3.4159196615219116\n",
      "Epoch [69/500], Loss: 1.1386\n",
      "total loss: 3.4143755435943604\n",
      "Epoch [70/500], Loss: 1.1381\n",
      "total loss: 3.4105530977249146\n",
      "Epoch [71/500], Loss: 1.1369\n",
      "total loss: 3.409675359725952\n",
      "Epoch [72/500], Loss: 1.1366\n",
      "total loss: 3.4090147018432617\n",
      "Epoch [73/500], Loss: 1.1363\n",
      "total loss: 3.406108021736145\n",
      "Epoch [74/500], Loss: 1.1354\n",
      "total loss: 3.406679630279541\n",
      "Epoch [75/500], Loss: 1.1356\n",
      "total loss: 3.404040575027466\n",
      "Epoch [76/500], Loss: 1.1347\n",
      "total loss: 3.4041807651519775\n",
      "Epoch [77/500], Loss: 1.1347\n",
      "total loss: 3.4031271934509277\n",
      "Epoch [78/500], Loss: 1.1344\n",
      "total loss: 3.401700019836426\n",
      "Epoch [79/500], Loss: 1.1339\n",
      "total loss: 3.399337649345398\n",
      "Epoch [80/500], Loss: 1.1331\n",
      "total loss: 3.3972634077072144\n",
      "Epoch [81/500], Loss: 1.1324\n",
      "total loss: 3.3957642316818237\n",
      "Epoch [82/500], Loss: 1.1319\n",
      "total loss: 3.394834518432617\n",
      "Epoch [83/500], Loss: 1.1316\n",
      "total loss: 3.394103169441223\n",
      "Epoch [84/500], Loss: 1.1314\n",
      "total loss: 3.3933039903640747\n",
      "Epoch [85/500], Loss: 1.1311\n",
      "total loss: 3.393644332885742\n",
      "Epoch [86/500], Loss: 1.1312\n",
      "total loss: 3.3909075260162354\n",
      "Epoch [87/500], Loss: 1.1303\n",
      "total loss: 3.390655040740967\n",
      "Epoch [88/500], Loss: 1.1302\n",
      "total loss: 3.389512300491333\n",
      "Epoch [89/500], Loss: 1.1298\n",
      "total loss: 3.3881499767303467\n",
      "Epoch [90/500], Loss: 1.1294\n",
      "total loss: 3.385287642478943\n",
      "Epoch [91/500], Loss: 1.1284\n",
      "total loss: 3.384535312652588\n",
      "Epoch [92/500], Loss: 1.1282\n",
      "total loss: 3.3851702213287354\n",
      "Epoch [93/500], Loss: 1.1284\n",
      "total loss: 3.383229374885559\n",
      "Epoch [94/500], Loss: 1.1277\n",
      "total loss: 3.382620334625244\n",
      "Epoch [95/500], Loss: 1.1275\n",
      "total loss: 3.381617307662964\n",
      "Epoch [96/500], Loss: 1.1272\n",
      "total loss: 3.3815929889678955\n",
      "Epoch [97/500], Loss: 1.1272\n",
      "total loss: 3.379806160926819\n",
      "Epoch [98/500], Loss: 1.1266\n",
      "total loss: 3.3793805837631226\n",
      "Epoch [99/500], Loss: 1.1265\n",
      "total loss: 3.3784223794937134\n",
      "Epoch [100/500], Loss: 1.1261\n",
      "total loss: 3.377104640007019\n",
      "Epoch [101/500], Loss: 1.1257\n",
      "total loss: 3.3756203651428223\n",
      "Epoch [102/500], Loss: 1.1252\n",
      "total loss: 3.3759883642196655\n",
      "Epoch [103/500], Loss: 1.1253\n",
      "total loss: 3.374234914779663\n",
      "Epoch [104/500], Loss: 1.1247\n",
      "total loss: 3.373009443283081\n",
      "Epoch [105/500], Loss: 1.1243\n",
      "total loss: 3.372281312942505\n",
      "Epoch [106/500], Loss: 1.1241\n",
      "total loss: 3.371621012687683\n",
      "Epoch [107/500], Loss: 1.1239\n",
      "total loss: 3.3697752952575684\n",
      "Epoch [108/500], Loss: 1.1233\n",
      "total loss: 3.369620203971863\n",
      "Epoch [109/500], Loss: 1.1232\n",
      "total loss: 3.369791030883789\n",
      "Epoch [110/500], Loss: 1.1233\n",
      "total loss: 3.3685861825942993\n",
      "Epoch [111/500], Loss: 1.1229\n",
      "total loss: 3.3684521913528442\n",
      "Epoch [112/500], Loss: 1.1228\n",
      "total loss: 3.3679975271224976\n",
      "Epoch [113/500], Loss: 1.1227\n",
      "total loss: 3.365309000015259\n",
      "Epoch [114/500], Loss: 1.1218\n",
      "total loss: 3.3660476207733154\n",
      "Epoch [115/500], Loss: 1.1220\n",
      "total loss: 3.36519992351532\n",
      "Epoch [116/500], Loss: 1.1217\n",
      "total loss: 3.364100456237793\n",
      "Epoch [117/500], Loss: 1.1214\n",
      "total loss: 3.362795829772949\n",
      "Epoch [118/500], Loss: 1.1209\n",
      "total loss: 3.362631320953369\n",
      "Epoch [119/500], Loss: 1.1209\n",
      "total loss: 3.3608978986740112\n",
      "Epoch [120/500], Loss: 1.1203\n",
      "total loss: 3.361034870147705\n",
      "Epoch [121/500], Loss: 1.1203\n",
      "total loss: 3.3599960803985596\n",
      "Epoch [122/500], Loss: 1.1200\n",
      "total loss: 3.35844349861145\n",
      "Epoch [123/500], Loss: 1.1195\n",
      "total loss: 3.359516143798828\n",
      "Epoch [124/500], Loss: 1.1198\n",
      "total loss: 3.3571419715881348\n",
      "Epoch [125/500], Loss: 1.1190\n",
      "total loss: 3.3581602573394775\n",
      "Epoch [126/500], Loss: 1.1194\n",
      "total loss: 3.3557028770446777\n",
      "Epoch [127/500], Loss: 1.1186\n",
      "total loss: 3.354971170425415\n",
      "Epoch [128/500], Loss: 1.1183\n",
      "total loss: 3.3551363945007324\n",
      "Epoch [129/500], Loss: 1.1184\n",
      "total loss: 3.3545870780944824\n",
      "Epoch [130/500], Loss: 1.1182\n",
      "total loss: 3.3529105186462402\n",
      "Epoch [131/500], Loss: 1.1176\n",
      "total loss: 3.3537650108337402\n",
      "Epoch [132/500], Loss: 1.1179\n",
      "total loss: 3.3513882160186768\n",
      "Epoch [133/500], Loss: 1.1171\n",
      "total loss: 3.3531997203826904\n",
      "Epoch [134/500], Loss: 1.1177\n",
      "total loss: 3.35194730758667\n",
      "Epoch [135/500], Loss: 1.1173\n",
      "total loss: 3.350082278251648\n",
      "Epoch [136/500], Loss: 1.1167\n",
      "total loss: 3.3506745100021362\n",
      "Epoch [137/500], Loss: 1.1169\n",
      "total loss: 3.3495490550994873\n",
      "Epoch [138/500], Loss: 1.1165\n",
      "total loss: 3.3487967252731323\n",
      "Epoch [139/500], Loss: 1.1163\n",
      "total loss: 3.348645806312561\n",
      "Epoch [140/500], Loss: 1.1162\n",
      "total loss: 3.3482550382614136\n",
      "Epoch [141/500], Loss: 1.1161\n",
      "total loss: 3.3462413549423218\n",
      "Epoch [142/500], Loss: 1.1154\n",
      "total loss: 3.3471614122390747\n",
      "Epoch [143/500], Loss: 1.1157\n",
      "total loss: 3.3459689617156982\n",
      "Epoch [144/500], Loss: 1.1153\n",
      "total loss: 3.3460159301757812\n",
      "Epoch [145/500], Loss: 1.1153\n",
      "total loss: 3.344547152519226\n",
      "Epoch [146/500], Loss: 1.1148\n",
      "total loss: 3.3443832397460938\n",
      "Epoch [147/500], Loss: 1.1148\n",
      "total loss: 3.342315673828125\n",
      "Epoch [148/500], Loss: 1.1141\n",
      "total loss: 3.3438339233398438\n",
      "Epoch [149/500], Loss: 1.1146\n",
      "total loss: 3.3427772521972656\n",
      "Epoch [150/500], Loss: 1.1143\n",
      "total loss: 3.3434810638427734\n",
      "Epoch [151/500], Loss: 1.1145\n",
      "total loss: 3.3417341709136963\n",
      "Epoch [152/500], Loss: 1.1139\n",
      "total loss: 3.3407689332962036\n",
      "Epoch [153/500], Loss: 1.1136\n",
      "total loss: 3.3413394689559937\n",
      "Epoch [154/500], Loss: 1.1138\n",
      "total loss: 3.340347647666931\n",
      "Epoch [155/500], Loss: 1.1134\n",
      "total loss: 3.3406060934066772\n",
      "Epoch [156/500], Loss: 1.1135\n",
      "total loss: 3.3396315574645996\n",
      "Epoch [157/500], Loss: 1.1132\n",
      "total loss: 3.339251756668091\n",
      "Epoch [158/500], Loss: 1.1131\n",
      "total loss: 3.3382740020751953\n",
      "Epoch [159/500], Loss: 1.1128\n",
      "total loss: 3.337677001953125\n",
      "Epoch [160/500], Loss: 1.1126\n",
      "total loss: 3.3366247415542603\n",
      "Epoch [161/500], Loss: 1.1122\n",
      "total loss: 3.3370734453201294\n",
      "Epoch [162/500], Loss: 1.1124\n",
      "total loss: 3.3378883600234985\n",
      "Epoch [163/500], Loss: 1.1126\n",
      "total loss: 3.3358306884765625\n",
      "Epoch [164/500], Loss: 1.1119\n",
      "total loss: 3.3362081050872803\n",
      "Epoch [165/500], Loss: 1.1121\n",
      "total loss: 3.3355414867401123\n",
      "Epoch [166/500], Loss: 1.1118\n",
      "total loss: 3.334219455718994\n",
      "Epoch [167/500], Loss: 1.1114\n",
      "total loss: 3.334764838218689\n",
      "Epoch [168/500], Loss: 1.1116\n",
      "total loss: 3.332926392555237\n",
      "Epoch [169/500], Loss: 1.1110\n",
      "total loss: 3.333440899848938\n",
      "Epoch [170/500], Loss: 1.1111\n",
      "total loss: 3.332492709159851\n",
      "Epoch [171/500], Loss: 1.1108\n",
      "total loss: 3.3333940505981445\n",
      "Epoch [172/500], Loss: 1.1111\n",
      "total loss: 3.3331574201583862\n",
      "Epoch [173/500], Loss: 1.1111\n",
      "total loss: 3.3325780630111694\n",
      "Epoch [174/500], Loss: 1.1109\n",
      "total loss: 3.332339286804199\n",
      "Epoch [175/500], Loss: 1.1108\n",
      "total loss: 3.3308509588241577\n",
      "Epoch [176/500], Loss: 1.1103\n",
      "total loss: 3.3297979831695557\n",
      "Epoch [177/500], Loss: 1.1099\n",
      "total loss: 3.330385684967041\n",
      "Epoch [178/500], Loss: 1.1101\n",
      "total loss: 3.3302881717681885\n",
      "Epoch [179/500], Loss: 1.1101\n",
      "total loss: 3.3300944566726685\n",
      "Epoch [180/500], Loss: 1.1100\n",
      "total loss: 3.328817844390869\n",
      "Epoch [181/500], Loss: 1.1096\n",
      "total loss: 3.329131841659546\n",
      "Epoch [182/500], Loss: 1.1097\n",
      "total loss: 3.329300045967102\n",
      "Epoch [183/500], Loss: 1.1098\n",
      "total loss: 3.3280930519104004\n",
      "Epoch [184/500], Loss: 1.1094\n",
      "total loss: 3.328270435333252\n",
      "Epoch [185/500], Loss: 1.1094\n",
      "total loss: 3.3280903100967407\n",
      "Epoch [186/500], Loss: 1.1094\n",
      "total loss: 3.3278403282165527\n",
      "Epoch [187/500], Loss: 1.1093\n",
      "total loss: 3.326153874397278\n",
      "Epoch [188/500], Loss: 1.1087\n",
      "total loss: 3.3266243934631348\n",
      "Epoch [189/500], Loss: 1.1089\n",
      "total loss: 3.3262741565704346\n",
      "Epoch [190/500], Loss: 1.1088\n",
      "total loss: 3.324652314186096\n",
      "Epoch [191/500], Loss: 1.1082\n",
      "total loss: 3.3259068727493286\n",
      "Epoch [192/500], Loss: 1.1086\n",
      "total loss: 3.325134038925171\n",
      "Epoch [193/500], Loss: 1.1084\n",
      "total loss: 3.3242567777633667\n",
      "Epoch [194/500], Loss: 1.1081\n",
      "total loss: 3.324597716331482\n",
      "Epoch [195/500], Loss: 1.1082\n",
      "total loss: 3.3243374824523926\n",
      "Epoch [196/500], Loss: 1.1081\n",
      "total loss: 3.3236780166625977\n",
      "Epoch [197/500], Loss: 1.1079\n",
      "total loss: 3.3237640857696533\n",
      "Epoch [198/500], Loss: 1.1079\n",
      "total loss: 3.3228100538253784\n",
      "Epoch [199/500], Loss: 1.1076\n",
      "total loss: 3.3211530447006226\n",
      "Epoch [200/500], Loss: 1.1071\n",
      "total loss: 3.32207715511322\n",
      "Epoch [201/500], Loss: 1.1074\n",
      "total loss: 3.321690082550049\n",
      "Epoch [202/500], Loss: 1.1072\n",
      "total loss: 3.3223356008529663\n",
      "Epoch [203/500], Loss: 1.1074\n",
      "total loss: 3.321417808532715\n",
      "Epoch [204/500], Loss: 1.1071\n",
      "total loss: 3.3215627670288086\n",
      "Epoch [205/500], Loss: 1.1072\n",
      "total loss: 3.3213318586349487\n",
      "Epoch [206/500], Loss: 1.1071\n",
      "total loss: 3.3202983140945435\n",
      "Epoch [207/500], Loss: 1.1068\n",
      "total loss: 3.3203160762786865\n",
      "Epoch [208/500], Loss: 1.1068\n",
      "total loss: 3.319949507713318\n",
      "Epoch [209/500], Loss: 1.1066\n",
      "total loss: 3.3183226585388184\n",
      "Epoch [210/500], Loss: 1.1061\n",
      "total loss: 3.319087266921997\n",
      "Epoch [211/500], Loss: 1.1064\n",
      "total loss: 3.318631410598755\n",
      "Epoch [212/500], Loss: 1.1062\n",
      "total loss: 3.318284511566162\n",
      "Epoch [213/500], Loss: 1.1061\n",
      "total loss: 3.3181958198547363\n",
      "Epoch [214/500], Loss: 1.1061\n",
      "total loss: 3.318171739578247\n",
      "Epoch [215/500], Loss: 1.1061\n",
      "total loss: 3.3175946474075317\n",
      "Epoch [216/500], Loss: 1.1059\n",
      "total loss: 3.318503737449646\n",
      "Epoch [217/500], Loss: 1.1062\n",
      "total loss: 3.3168379068374634\n",
      "Epoch [218/500], Loss: 1.1056\n",
      "total loss: 3.317750096321106\n",
      "Epoch [219/500], Loss: 1.1059\n",
      "total loss: 3.3168870210647583\n",
      "Epoch [220/500], Loss: 1.1056\n",
      "total loss: 3.316351890563965\n",
      "Epoch [221/500], Loss: 1.1055\n",
      "total loss: 3.3169416189193726\n",
      "Epoch [222/500], Loss: 1.1056\n",
      "total loss: 3.316450834274292\n",
      "Epoch [223/500], Loss: 1.1055\n",
      "total loss: 3.3153765201568604\n",
      "Epoch [224/500], Loss: 1.1051\n",
      "total loss: 3.3159029483795166\n",
      "Epoch [225/500], Loss: 1.1053\n",
      "total loss: 3.314894914627075\n",
      "Epoch [226/500], Loss: 1.1050\n",
      "total loss: 3.3143686056137085\n",
      "Epoch [227/500], Loss: 1.1048\n",
      "total loss: 3.314910054206848\n",
      "Epoch [228/500], Loss: 1.1050\n",
      "total loss: 3.31399142742157\n",
      "Epoch [229/500], Loss: 1.1047\n",
      "total loss: 3.3138551712036133\n",
      "Epoch [230/500], Loss: 1.1046\n",
      "total loss: 3.313297748565674\n",
      "Epoch [231/500], Loss: 1.1044\n",
      "total loss: 3.313592553138733\n",
      "Epoch [232/500], Loss: 1.1045\n",
      "total loss: 3.3137251138687134\n",
      "Epoch [233/500], Loss: 1.1046\n",
      "total loss: 3.3131661415100098\n",
      "Epoch [234/500], Loss: 1.1044\n",
      "total loss: 3.314757466316223\n",
      "Epoch [235/500], Loss: 1.1049\n",
      "total loss: 3.3119451999664307\n",
      "Epoch [236/500], Loss: 1.1040\n",
      "total loss: 3.3116878271102905\n",
      "Epoch [237/500], Loss: 1.1039\n",
      "total loss: 3.3127673864364624\n",
      "Epoch [238/500], Loss: 1.1043\n",
      "total loss: 3.3121864795684814\n",
      "Epoch [239/500], Loss: 1.1041\n",
      "total loss: 3.311048984527588\n",
      "Epoch [240/500], Loss: 1.1037\n",
      "total loss: 3.3123342990875244\n",
      "Epoch [241/500], Loss: 1.1041\n",
      "total loss: 3.3116451501846313\n",
      "Epoch [242/500], Loss: 1.1039\n",
      "total loss: 3.310367703437805\n",
      "Epoch [243/500], Loss: 1.1035\n",
      "total loss: 3.3109068870544434\n",
      "Epoch [244/500], Loss: 1.1036\n",
      "total loss: 3.310685873031616\n",
      "Epoch [245/500], Loss: 1.1036\n",
      "total loss: 3.311332583427429\n",
      "Epoch [246/500], Loss: 1.1038\n",
      "total loss: 3.311132311820984\n",
      "Epoch [247/500], Loss: 1.1037\n",
      "total loss: 3.3098586797714233\n",
      "Epoch [248/500], Loss: 1.1033\n",
      "total loss: 3.307310104370117\n",
      "Epoch [249/500], Loss: 1.1024\n",
      "total loss: 3.3095728158950806\n",
      "Epoch [250/500], Loss: 1.1032\n",
      "total loss: 3.3086676597595215\n",
      "Epoch [251/500], Loss: 1.1029\n",
      "total loss: 3.310012698173523\n",
      "Epoch [252/500], Loss: 1.1033\n",
      "total loss: 3.3078083992004395\n",
      "Epoch [253/500], Loss: 1.1026\n",
      "total loss: 3.307893395423889\n",
      "Epoch [254/500], Loss: 1.1026\n",
      "total loss: 3.3082722425460815\n",
      "Epoch [255/500], Loss: 1.1028\n",
      "total loss: 3.3084932565689087\n",
      "Epoch [256/500], Loss: 1.1028\n",
      "total loss: 3.30777370929718\n",
      "Epoch [257/500], Loss: 1.1026\n",
      "total loss: 3.30811083316803\n",
      "Epoch [258/500], Loss: 1.1027\n",
      "total loss: 3.307677745819092\n",
      "Epoch [259/500], Loss: 1.1026\n",
      "total loss: 3.3078302145004272\n",
      "Epoch [260/500], Loss: 1.1026\n",
      "total loss: 3.3073854446411133\n",
      "Epoch [261/500], Loss: 1.1025\n",
      "total loss: 3.306494116783142\n",
      "Epoch [262/500], Loss: 1.1022\n",
      "total loss: 3.30629301071167\n",
      "Epoch [263/500], Loss: 1.1021\n",
      "total loss: 3.307325601577759\n",
      "Epoch [264/500], Loss: 1.1024\n",
      "total loss: 3.307758092880249\n",
      "Epoch [265/500], Loss: 1.1026\n",
      "total loss: 3.30592942237854\n",
      "Epoch [266/500], Loss: 1.1020\n",
      "total loss: 3.3060922622680664\n",
      "Epoch [267/500], Loss: 1.1020\n",
      "total loss: 3.304769277572632\n",
      "Epoch [268/500], Loss: 1.1016\n",
      "total loss: 3.3055152893066406\n",
      "Epoch [269/500], Loss: 1.1018\n",
      "total loss: 3.3051440715789795\n",
      "Epoch [270/500], Loss: 1.1017\n",
      "total loss: 3.3053470849990845\n",
      "Epoch [271/500], Loss: 1.1018\n",
      "total loss: 3.3044530153274536\n",
      "Epoch [272/500], Loss: 1.1015\n",
      "total loss: 3.3048819303512573\n",
      "Epoch [273/500], Loss: 1.1016\n",
      "total loss: 3.303475856781006\n",
      "Epoch [274/500], Loss: 1.1012\n",
      "total loss: 3.303562045097351\n",
      "Epoch [275/500], Loss: 1.1012\n",
      "total loss: 3.303892970085144\n",
      "Epoch [276/500], Loss: 1.1013\n",
      "total loss: 3.3038541078567505\n",
      "Epoch [277/500], Loss: 1.1013\n",
      "total loss: 3.3039110898971558\n",
      "Epoch [278/500], Loss: 1.1013\n",
      "total loss: 3.3033740520477295\n",
      "Epoch [279/500], Loss: 1.1011\n",
      "total loss: 3.303457260131836\n",
      "Epoch [280/500], Loss: 1.1012\n",
      "total loss: 3.3036115169525146\n",
      "Epoch [281/500], Loss: 1.1012\n",
      "total loss: 3.303259253501892\n",
      "Epoch [282/500], Loss: 1.1011\n",
      "total loss: 3.3024810552597046\n",
      "Epoch [283/500], Loss: 1.1008\n",
      "total loss: 3.3042306900024414\n",
      "Epoch [284/500], Loss: 1.1014\n",
      "total loss: 3.3022292852401733\n",
      "Epoch [285/500], Loss: 1.1007\n",
      "total loss: 3.303993344306946\n",
      "Epoch [286/500], Loss: 1.1013\n",
      "total loss: 3.302137851715088\n",
      "Epoch [287/500], Loss: 1.1007\n",
      "total loss: 3.3024216890335083\n",
      "Epoch [288/500], Loss: 1.1008\n",
      "total loss: 3.3001816272735596\n",
      "Epoch [289/500], Loss: 1.1001\n",
      "total loss: 3.301810145378113\n",
      "Epoch [290/500], Loss: 1.1006\n",
      "total loss: 3.3000823259353638\n",
      "Epoch [291/500], Loss: 1.1000\n",
      "total loss: 3.301016688346863\n",
      "Epoch [292/500], Loss: 1.1003\n",
      "total loss: 3.3010364770889282\n",
      "Epoch [293/500], Loss: 1.1003\n",
      "total loss: 3.3019334077835083\n",
      "Epoch [294/500], Loss: 1.1006\n",
      "total loss: 3.300773501396179\n",
      "Epoch [295/500], Loss: 1.1003\n",
      "total loss: 3.2998838424682617\n",
      "Epoch [296/500], Loss: 1.1000\n",
      "total loss: 3.3004298210144043\n",
      "Epoch [297/500], Loss: 1.1001\n",
      "total loss: 3.299039363861084\n",
      "Epoch [298/500], Loss: 1.0997\n",
      "total loss: 3.3012415170669556\n",
      "Epoch [299/500], Loss: 1.1004\n",
      "total loss: 3.3017581701278687\n",
      "Epoch [300/500], Loss: 1.1006\n",
      "total loss: 3.2998011112213135\n",
      "Epoch [301/500], Loss: 1.0999\n",
      "total loss: 3.2996455430984497\n",
      "Epoch [302/500], Loss: 1.0999\n",
      "total loss: 3.3009800910949707\n",
      "Epoch [303/500], Loss: 1.1003\n",
      "total loss: 3.29965603351593\n",
      "Epoch [304/500], Loss: 1.0999\n",
      "total loss: 3.299607038497925\n",
      "Epoch [305/500], Loss: 1.0999\n",
      "total loss: 3.298896074295044\n",
      "Epoch [306/500], Loss: 1.0996\n",
      "total loss: 3.297598123550415\n",
      "Epoch [307/500], Loss: 1.0992\n",
      "total loss: 3.2999236583709717\n",
      "Epoch [308/500], Loss: 1.1000\n",
      "total loss: 3.299019932746887\n",
      "Epoch [309/500], Loss: 1.0997\n",
      "total loss: 3.298615336418152\n",
      "Epoch [310/500], Loss: 1.0995\n",
      "total loss: 3.2982985973358154\n",
      "Epoch [311/500], Loss: 1.0994\n",
      "total loss: 3.2988141775131226\n",
      "Epoch [312/500], Loss: 1.0996\n",
      "total loss: 3.296985626220703\n",
      "Epoch [313/500], Loss: 1.0990\n",
      "total loss: 3.2967945337295532\n",
      "Epoch [314/500], Loss: 1.0989\n",
      "total loss: 3.2981306314468384\n",
      "Epoch [315/500], Loss: 1.0994\n",
      "total loss: 3.29914391040802\n",
      "Epoch [316/500], Loss: 1.0997\n",
      "total loss: 3.2978378534317017\n",
      "Epoch [317/500], Loss: 1.0993\n",
      "total loss: 3.2971251010894775\n",
      "Epoch [318/500], Loss: 1.0990\n",
      "total loss: 3.2970772981643677\n",
      "Epoch [319/500], Loss: 1.0990\n",
      "total loss: 3.298049569129944\n",
      "Epoch [320/500], Loss: 1.0993\n",
      "total loss: 3.296988844871521\n",
      "Epoch [321/500], Loss: 1.0990\n",
      "total loss: 3.2976319789886475\n",
      "Epoch [322/500], Loss: 1.0992\n",
      "total loss: 3.2963491678237915\n",
      "Epoch [323/500], Loss: 1.0988\n",
      "total loss: 3.297004818916321\n",
      "Epoch [324/500], Loss: 1.0990\n",
      "total loss: 3.296059012413025\n",
      "Epoch [325/500], Loss: 1.0987\n",
      "total loss: 3.2951749563217163\n",
      "Epoch [326/500], Loss: 1.0984\n",
      "total loss: 3.2971123456954956\n",
      "Epoch [327/500], Loss: 1.0990\n",
      "total loss: 3.2956342697143555\n",
      "Epoch [328/500], Loss: 1.0985\n",
      "total loss: 3.2959203720092773\n",
      "Epoch [329/500], Loss: 1.0986\n",
      "total loss: 3.2952874898910522\n",
      "Epoch [330/500], Loss: 1.0984\n",
      "total loss: 3.2962836027145386\n",
      "Epoch [331/500], Loss: 1.0988\n",
      "total loss: 3.2949689626693726\n",
      "Epoch [332/500], Loss: 1.0983\n",
      "total loss: 3.2964279651641846\n",
      "Epoch [333/500], Loss: 1.0988\n",
      "total loss: 3.2954044342041016\n",
      "Epoch [334/500], Loss: 1.0985\n",
      "total loss: 3.2946332693099976\n",
      "Epoch [335/500], Loss: 1.0982\n",
      "total loss: 3.2943735122680664\n",
      "Epoch [336/500], Loss: 1.0981\n",
      "total loss: 3.294051766395569\n",
      "Epoch [337/500], Loss: 1.0980\n",
      "total loss: 3.2952020168304443\n",
      "Epoch [338/500], Loss: 1.0984\n",
      "total loss: 3.295250177383423\n",
      "Epoch [339/500], Loss: 1.0984\n",
      "total loss: 3.295050859451294\n",
      "Epoch [340/500], Loss: 1.0984\n",
      "total loss: 3.294564962387085\n",
      "Epoch [341/500], Loss: 1.0982\n",
      "total loss: 3.2950353622436523\n",
      "Epoch [342/500], Loss: 1.0983\n",
      "total loss: 3.2936118841171265\n",
      "Epoch [343/500], Loss: 1.0979\n",
      "total loss: 3.295229434967041\n",
      "Epoch [344/500], Loss: 1.0984\n",
      "total loss: 3.294034481048584\n",
      "Epoch [345/500], Loss: 1.0980\n",
      "total loss: 3.293260335922241\n",
      "Epoch [346/500], Loss: 1.0978\n",
      "total loss: 3.293075442314148\n",
      "Epoch [347/500], Loss: 1.0977\n",
      "total loss: 3.2933977842330933\n",
      "Epoch [348/500], Loss: 1.0978\n",
      "total loss: 3.294425129890442\n",
      "Epoch [349/500], Loss: 1.0981\n",
      "total loss: 3.2930988073349\n",
      "Epoch [350/500], Loss: 1.0977\n",
      "total loss: 3.2939757108688354\n",
      "Epoch [351/500], Loss: 1.0980\n",
      "total loss: 3.2933406829833984\n",
      "Epoch [352/500], Loss: 1.0978\n",
      "total loss: 3.2926641702651978\n",
      "Epoch [353/500], Loss: 1.0976\n",
      "total loss: 3.291911244392395\n",
      "Epoch [354/500], Loss: 1.0973\n",
      "total loss: 3.2917641401290894\n",
      "Epoch [355/500], Loss: 1.0973\n",
      "total loss: 3.292993664741516\n",
      "Epoch [356/500], Loss: 1.0977\n",
      "total loss: 3.2913206815719604\n",
      "Epoch [357/500], Loss: 1.0971\n",
      "total loss: 3.292271852493286\n",
      "Epoch [358/500], Loss: 1.0974\n",
      "total loss: 3.290501356124878\n",
      "Epoch [359/500], Loss: 1.0968\n",
      "total loss: 3.2924762964248657\n",
      "Epoch [360/500], Loss: 1.0975\n",
      "total loss: 3.2924352884292603\n",
      "Epoch [361/500], Loss: 1.0975\n",
      "total loss: 3.291991353034973\n",
      "Epoch [362/500], Loss: 1.0973\n",
      "total loss: 3.2919180393218994\n",
      "Epoch [363/500], Loss: 1.0973\n",
      "total loss: 3.290907382965088\n",
      "Epoch [364/500], Loss: 1.0970\n",
      "total loss: 3.2913252115249634\n",
      "Epoch [365/500], Loss: 1.0971\n",
      "total loss: 3.2916715145111084\n",
      "Epoch [366/500], Loss: 1.0972\n",
      "total loss: 3.2915672063827515\n",
      "Epoch [367/500], Loss: 1.0972\n",
      "total loss: 3.2912354469299316\n",
      "Epoch [368/500], Loss: 1.0971\n",
      "total loss: 3.2902183532714844\n",
      "Epoch [369/500], Loss: 1.0967\n",
      "total loss: 3.2924952507019043\n",
      "Epoch [370/500], Loss: 1.0975\n",
      "total loss: 3.290283441543579\n",
      "Epoch [371/500], Loss: 1.0968\n",
      "total loss: 3.2901819944381714\n",
      "Epoch [372/500], Loss: 1.0967\n",
      "total loss: 3.2903538942337036\n",
      "Epoch [373/500], Loss: 1.0968\n",
      "total loss: 3.290251851081848\n",
      "Epoch [374/500], Loss: 1.0968\n",
      "total loss: 3.289876103401184\n",
      "Epoch [375/500], Loss: 1.0966\n",
      "total loss: 3.2900633811950684\n",
      "Epoch [376/500], Loss: 1.0967\n",
      "total loss: 3.290239691734314\n",
      "Epoch [377/500], Loss: 1.0967\n",
      "total loss: 3.290695071220398\n",
      "Epoch [378/500], Loss: 1.0969\n",
      "total loss: 3.290416121482849\n",
      "Epoch [379/500], Loss: 1.0968\n",
      "total loss: 3.2892976999282837\n",
      "Epoch [380/500], Loss: 1.0964\n",
      "total loss: 3.2901641130447388\n",
      "Epoch [381/500], Loss: 1.0967\n",
      "total loss: 3.2899210453033447\n",
      "Epoch [382/500], Loss: 1.0966\n",
      "total loss: 3.2895129919052124\n",
      "Epoch [383/500], Loss: 1.0965\n",
      "total loss: 3.289036989212036\n",
      "Epoch [384/500], Loss: 1.0963\n",
      "total loss: 3.2899365425109863\n",
      "Epoch [385/500], Loss: 1.0966\n",
      "total loss: 3.2890387773513794\n",
      "Epoch [386/500], Loss: 1.0963\n",
      "total loss: 3.2899376153945923\n",
      "Epoch [387/500], Loss: 1.0966\n",
      "total loss: 3.289276361465454\n",
      "Epoch [388/500], Loss: 1.0964\n",
      "total loss: 3.2893465757369995\n",
      "Epoch [389/500], Loss: 1.0964\n",
      "total loss: 3.2893035411834717\n",
      "Epoch [390/500], Loss: 1.0964\n",
      "total loss: 3.2887285947799683\n",
      "Epoch [391/500], Loss: 1.0962\n",
      "total loss: 3.2881715297698975\n",
      "Epoch [392/500], Loss: 1.0961\n",
      "total loss: 3.289016842842102\n",
      "Epoch [393/500], Loss: 1.0963\n",
      "total loss: 3.28781259059906\n",
      "Epoch [394/500], Loss: 1.0959\n",
      "total loss: 3.2887203693389893\n",
      "Epoch [395/500], Loss: 1.0962\n",
      "total loss: 3.2887074947357178\n",
      "Epoch [396/500], Loss: 1.0962\n",
      "total loss: 3.2886983156204224\n",
      "Epoch [397/500], Loss: 1.0962\n",
      "total loss: 3.287697911262512\n",
      "Epoch [398/500], Loss: 1.0959\n",
      "total loss: 3.287610650062561\n",
      "Epoch [399/500], Loss: 1.0959\n",
      "total loss: 3.2875843048095703\n",
      "Epoch [400/500], Loss: 1.0959\n",
      "total loss: 3.286772131919861\n",
      "Epoch [401/500], Loss: 1.0956\n",
      "total loss: 3.2875752449035645\n",
      "Epoch [402/500], Loss: 1.0959\n",
      "total loss: 3.2878416776657104\n",
      "Epoch [403/500], Loss: 1.0959\n",
      "total loss: 3.2885076999664307\n",
      "Epoch [404/500], Loss: 1.0962\n",
      "total loss: 3.2858675718307495\n",
      "Epoch [405/500], Loss: 1.0953\n",
      "total loss: 3.2871086597442627\n",
      "Epoch [406/500], Loss: 1.0957\n",
      "total loss: 3.287377715110779\n",
      "Epoch [407/500], Loss: 1.0958\n",
      "total loss: 3.287453532218933\n",
      "Epoch [408/500], Loss: 1.0958\n",
      "total loss: 3.286266565322876\n",
      "Epoch [409/500], Loss: 1.0954\n",
      "total loss: 3.286653161048889\n",
      "Epoch [410/500], Loss: 1.0956\n",
      "total loss: 3.2865428924560547\n",
      "Epoch [411/500], Loss: 1.0955\n",
      "total loss: 3.2876415252685547\n",
      "Epoch [412/500], Loss: 1.0959\n",
      "total loss: 3.286657691001892\n",
      "Epoch [413/500], Loss: 1.0956\n",
      "total loss: 3.287121295928955\n",
      "Epoch [414/500], Loss: 1.0957\n",
      "total loss: 3.2869322299957275\n",
      "Epoch [415/500], Loss: 1.0956\n",
      "total loss: 3.2858686447143555\n",
      "Epoch [416/500], Loss: 1.0953\n",
      "total loss: 3.2854987382888794\n",
      "Epoch [417/500], Loss: 1.0952\n",
      "total loss: 3.286822557449341\n",
      "Epoch [418/500], Loss: 1.0956\n",
      "total loss: 3.2863985300064087\n",
      "Epoch [419/500], Loss: 1.0955\n",
      "total loss: 3.2859716415405273\n",
      "Epoch [420/500], Loss: 1.0953\n",
      "total loss: 3.2866461277008057\n",
      "Epoch [421/500], Loss: 1.0955\n",
      "total loss: 3.2855037450790405\n",
      "Epoch [422/500], Loss: 1.0952\n",
      "total loss: 3.286917567253113\n",
      "Epoch [423/500], Loss: 1.0956\n",
      "total loss: 3.285249948501587\n",
      "Epoch [424/500], Loss: 1.0951\n",
      "total loss: 3.2840888500213623\n",
      "Epoch [425/500], Loss: 1.0947\n",
      "total loss: 3.2852882146835327\n",
      "Epoch [426/500], Loss: 1.0951\n",
      "total loss: 3.2857203483581543\n",
      "Epoch [427/500], Loss: 1.0952\n",
      "total loss: 3.286580801010132\n",
      "Epoch [428/500], Loss: 1.0955\n",
      "total loss: 3.2856982946395874\n",
      "Epoch [429/500], Loss: 1.0952\n",
      "total loss: 3.2861248254776\n",
      "Epoch [430/500], Loss: 1.0954\n",
      "total loss: 3.2847245931625366\n",
      "Epoch [431/500], Loss: 1.0949\n",
      "total loss: 3.285332441329956\n",
      "Epoch [432/500], Loss: 1.0951\n",
      "total loss: 3.28450345993042\n",
      "Epoch [433/500], Loss: 1.0948\n",
      "total loss: 3.285715341567993\n",
      "Epoch [434/500], Loss: 1.0952\n",
      "total loss: 3.284004330635071\n",
      "Epoch [435/500], Loss: 1.0947\n",
      "total loss: 3.2846181392669678\n",
      "Epoch [436/500], Loss: 1.0949\n",
      "total loss: 3.283760190010071\n",
      "Epoch [437/500], Loss: 1.0946\n",
      "total loss: 3.2850894927978516\n",
      "Epoch [438/500], Loss: 1.0950\n",
      "total loss: 3.284497022628784\n",
      "Epoch [439/500], Loss: 1.0948\n",
      "total loss: 3.2837647199630737\n",
      "Epoch [440/500], Loss: 1.0946\n",
      "total loss: 3.2841804027557373\n",
      "Epoch [441/500], Loss: 1.0947\n",
      "total loss: 3.284763813018799\n",
      "Epoch [442/500], Loss: 1.0949\n",
      "total loss: 3.2845648527145386\n",
      "Epoch [443/500], Loss: 1.0949\n",
      "total loss: 3.283492922782898\n",
      "Epoch [444/500], Loss: 1.0945\n",
      "total loss: 3.2845709323883057\n",
      "Epoch [445/500], Loss: 1.0949\n",
      "total loss: 3.284231185913086\n",
      "Epoch [446/500], Loss: 1.0947\n",
      "total loss: 3.2834837436676025\n",
      "Epoch [447/500], Loss: 1.0945\n",
      "total loss: 3.2844749689102173\n",
      "Epoch [448/500], Loss: 1.0948\n",
      "total loss: 3.2863707542419434\n",
      "Epoch [449/500], Loss: 1.0955\n",
      "total loss: 3.282903552055359\n",
      "Epoch [450/500], Loss: 1.0943\n",
      "total loss: 3.2849338054656982\n",
      "Epoch [451/500], Loss: 1.0950\n",
      "total loss: 3.283288598060608\n",
      "Epoch [452/500], Loss: 1.0944\n",
      "total loss: 3.283337712287903\n",
      "Epoch [453/500], Loss: 1.0944\n",
      "total loss: 3.2822089195251465\n",
      "Epoch [454/500], Loss: 1.0941\n",
      "total loss: 3.283915400505066\n",
      "Epoch [455/500], Loss: 1.0946\n",
      "total loss: 3.284488320350647\n",
      "Epoch [456/500], Loss: 1.0948\n",
      "total loss: 3.283138155937195\n",
      "Epoch [457/500], Loss: 1.0944\n",
      "total loss: 3.283042550086975\n",
      "Epoch [458/500], Loss: 1.0943\n",
      "total loss: 3.282995104789734\n",
      "Epoch [459/500], Loss: 1.0943\n",
      "total loss: 3.283754348754883\n",
      "Epoch [460/500], Loss: 1.0946\n",
      "total loss: 3.2834746837615967\n",
      "Epoch [461/500], Loss: 1.0945\n",
      "total loss: 3.2831461429595947\n",
      "Epoch [462/500], Loss: 1.0944\n",
      "total loss: 3.2829941511154175\n",
      "Epoch [463/500], Loss: 1.0943\n",
      "total loss: 3.2824496030807495\n",
      "Epoch [464/500], Loss: 1.0941\n",
      "total loss: 3.282314896583557\n",
      "Epoch [465/500], Loss: 1.0941\n",
      "total loss: 3.2823420763015747\n",
      "Epoch [466/500], Loss: 1.0941\n",
      "total loss: 3.2823972702026367\n",
      "Epoch [467/500], Loss: 1.0941\n",
      "total loss: 3.283537268638611\n",
      "Epoch [468/500], Loss: 1.0945\n",
      "total loss: 3.2825496196746826\n",
      "Epoch [469/500], Loss: 1.0942\n",
      "total loss: 3.2832367420196533\n",
      "Epoch [470/500], Loss: 1.0944\n",
      "total loss: 3.282106637954712\n",
      "Epoch [471/500], Loss: 1.0940\n",
      "total loss: 3.2822338342666626\n",
      "Epoch [472/500], Loss: 1.0941\n",
      "total loss: 3.2820894718170166\n",
      "Epoch [473/500], Loss: 1.0940\n",
      "total loss: 3.2821871042251587\n",
      "Epoch [474/500], Loss: 1.0941\n",
      "total loss: 3.2819515466690063\n",
      "Epoch [475/500], Loss: 1.0940\n",
      "total loss: 3.2819236516952515\n",
      "Epoch [476/500], Loss: 1.0940\n",
      "total loss: 3.28296160697937\n",
      "Epoch [477/500], Loss: 1.0943\n",
      "total loss: 3.281239628791809\n",
      "Epoch [478/500], Loss: 1.0937\n",
      "total loss: 3.2819775342941284\n",
      "Epoch [479/500], Loss: 1.0940\n",
      "total loss: 3.2816861867904663\n",
      "Epoch [480/500], Loss: 1.0939\n",
      "total loss: 3.28118097782135\n",
      "Epoch [481/500], Loss: 1.0937\n",
      "total loss: 3.2819021940231323\n",
      "Epoch [482/500], Loss: 1.0940\n",
      "total loss: 3.281768321990967\n",
      "Epoch [483/500], Loss: 1.0939\n",
      "total loss: 3.280882954597473\n",
      "Epoch [484/500], Loss: 1.0936\n",
      "total loss: 3.2812347412109375\n",
      "Epoch [485/500], Loss: 1.0937\n",
      "total loss: 3.2812230587005615\n",
      "Epoch [486/500], Loss: 1.0937\n",
      "total loss: 3.2817022800445557\n",
      "Epoch [487/500], Loss: 1.0939\n",
      "total loss: 3.2811529636383057\n",
      "Epoch [488/500], Loss: 1.0937\n",
      "total loss: 3.2815372943878174\n",
      "Epoch [489/500], Loss: 1.0938\n",
      "total loss: 3.2820833921432495\n",
      "Epoch [490/500], Loss: 1.0940\n",
      "total loss: 3.2794605493545532\n",
      "Epoch [491/500], Loss: 1.0932\n",
      "total loss: 3.2804843187332153\n",
      "Epoch [492/500], Loss: 1.0935\n",
      "total loss: 3.2811330556869507\n",
      "Epoch [493/500], Loss: 1.0937\n",
      "total loss: 3.2804077863693237\n",
      "Epoch [494/500], Loss: 1.0935\n",
      "total loss: 3.2797553539276123\n",
      "Epoch [495/500], Loss: 1.0933\n",
      "total loss: 3.2805463075637817\n",
      "Epoch [496/500], Loss: 1.0935\n",
      "total loss: 3.2812552452087402\n",
      "Epoch [497/500], Loss: 1.0938\n",
      "total loss: 3.2793437242507935\n",
      "Epoch [498/500], Loss: 1.0931\n",
      "total loss: 3.281525135040283\n",
      "Epoch [499/500], Loss: 1.0938\n",
      "total loss: 3.280759334564209\n",
      "Epoch [500/500], Loss: 1.0936\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model\n",
    "trained_model = train_model(input_data, desired_output, 500, learning_rate=0.0005, viz_accuracy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(trained_model, new_data, expected_result):\n",
    "    test_input = torch.tensor(new_data, dtype=torch.float32)\n",
    "\n",
    "    test_output = trained_model(test_input)\n",
    "\n",
    "    predicted_classes = torch.argmax(test_output, dim=1)\n",
    "\n",
    "    test_output_compare = torch.tensor(expected_result, dtype=torch.int)\n",
    "\n",
    "    return float(sum(test_output_compare == predicted_classes)/len(test_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"learned_Q_moves_test.csv\")\n",
    "just_input_test = test_df.drop('num_moves_Q_learning_needs', axis=1)\n",
    "just_input_test = just_input_test.drop('last_matrix', axis=1)\n",
    "\n",
    "input_data_test = np.array(just_input_test.values.tolist())\n",
    "desired_output = torch.tensor(test_df['num_moves_Q_learning_needs'].tolist(), dtype=torch.float32).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16089\\AppData\\Local\\Temp\\ipykernel_17528\\875939923.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_output_compare = torch.tensor(expected_result, dtype=torch.int)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6199694275856018"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can get about 62% accuracy\n",
    "test_model(trained_model, input_data_test, desired_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a vizualization of what we would predict for a bunch of ordered pairs\n",
    "\n",
    "# TODO figure out how to visualize this when we have 3 points\n",
    "# confidence_df = pd.DataFrame({\n",
    "#     'x': [],\n",
    "#     'y': [],\n",
    "#     'confidence': []\n",
    "# })\n",
    "\n",
    "# for i in range(-5000000000000000000, 5000000000000000001, 200000000000000000):\n",
    "#     for j in range(-5000000000000000000, 5000000000000000001, 200000000000000000):\n",
    "#         demo_test_type = torch.tensor([[i, j]], dtype=torch.float32)\n",
    "#         confidence = int(torch.argmax(trained_model(demo_test_type)))\n",
    "#         df2 = pd.DataFrame([[i, j, confidence]], columns=['x', 'y', 'confidence'])\n",
    "#         confidence_df = pd.concat([confidence_df, df2])\n",
    "# plt.scatter(confidence_df['x'], confidence_df['y'], c=confidence_df['confidence'])\n",
    "# plt.xlabel(\"point1\")\n",
    "# plt.ylabel(\"point2\")\n",
    "# plt.title(\"Decision boundary between predicting last matrix applied to the vector\")\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
