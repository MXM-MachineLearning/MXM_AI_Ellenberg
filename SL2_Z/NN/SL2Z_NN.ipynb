{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code adapted from ChatGPT\n",
    "\n",
    "# Define your neural network model\n",
    "class SL2Z_NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SL2Z_NN, self).__init__()\n",
    "\n",
    "        self.step1 = nn.Linear(4, 128, bias=True)\n",
    "        self.step2 = nn.ReLU()\n",
    "        self.step3 = nn.Linear(128, 64, bias=True)\n",
    "        self.step4 = nn.ReLU()\n",
    "        self.step5 = nn.Linear(64, 16, bias=True)\n",
    "        self.step6 = nn.ReLU()\n",
    "        self.step7 = nn.Linear(16, 4, bias=True)\n",
    "\n",
    "        # multi-class classification adapted from ChatGPT\n",
    "        self.step8 = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # RUN IT ON A GPU if it exists\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.to(\"cuda\")\n",
    "\n",
    "        x = self.step1(x)\n",
    "        x = self.step2(x)\n",
    "        x = self.step3(x)\n",
    "        x = self.step4(x)\n",
    "        x = self.step5(x)\n",
    "        x = self.step6(x)\n",
    "        x = self.step7(x)\n",
    "        x = self.step8(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def train_model(inputs, desired_outputs, num_epochs=100, learning_rate=0.01, viz_accuracy=False):\n",
    "    # Convert inputs and desired_outputs to PyTorch tensors\n",
    "    inputs = torch.tensor(inputs, dtype=torch.float32)\n",
    "    desired_outputs = torch.tensor(desired_outputs, dtype=torch.float32)\n",
    "    \n",
    "    # Create a DataLoader to handle batching (if needed)\n",
    "    dataset = TensorDataset(inputs, desired_outputs)\n",
    "    dataloader = DataLoader(dataset, batch_size=1000, shuffle=True)  # Adjust batch_size as needed\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = SL2Z_NN()\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()  # Mean Squared Error loss\n",
    "    # optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "    # criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.995)\n",
    "    losses = []\n",
    "\n",
    "    inputs = inputs.float()\n",
    "    desired_outputs = desired_outputs.long()\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for batch_inputs, batch_desired_outputs in dataloader:\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            outputs = model(batch_inputs)  # Forward pass\n",
    "\n",
    "            batch_desired_outputs = batch_desired_outputs.long()\n",
    "\n",
    "            loss = criterion(outputs, batch_desired_outputs)  # Compute the loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update the model's parameters\n",
    "            cur_item = loss.item()\n",
    "            total_loss += cur_item\n",
    "        \n",
    "        # Print the average loss for this epoch\n",
    "        scheduler.step()\n",
    "        print(f\"total loss: {total_loss}\")\n",
    "        average_loss = total_loss / len(dataloader)\n",
    "        if viz_accuracy:\n",
    "            losses.append(average_loss)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.4f}')\n",
    "    \n",
    "    if viz_accuracy:\n",
    "        plt.scatter(x=range(1, len(losses)+1), y=losses)\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Average loss\")\n",
    "        plt.show()\n",
    "\n",
    "    # Return the trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Data_Generation/Data_files/train_rows_SL2Z_Q_learn.csv\")\n",
    "test_df = pd.read_csv(\"../Data_Generation/Data_files/test_rows_SL2Z_Q_learn.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Data_Generation/Data_files/train_rows_SL2Z_Q_learn.csv\")\n",
    "test_df = pd.read_csv(\"../Data_Generation/Data_files/test_rows_SL2Z_Q_learn.csv\")\n",
    "\n",
    "\n",
    "train = df.drop(\"num_moves_Q_learning_needs\", axis=1)\n",
    "# random.seed(42)\n",
    "# np.random.seed(42)\n",
    "# reordered = df.sample(frac=1).reset_index(drop=True)\n",
    "# bound = int(reordered.shape[0] * 0.6)\n",
    "# plus_one = bound+1\n",
    "\n",
    "# train = reordered.iloc[1:bound]\n",
    "# test_df = reordered.iloc[plus_one:reordered.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "just_input = train.drop('first_move_by_Q_learning', axis=1)\n",
    "\n",
    "# adapted from https://stackoverflow.com/questions/43898035/pandas-combine-column-values-into-a-list-in-a-new-column\n",
    "input_data = np.array(just_input.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_output = torch.tensor(train['first_move_by_Q_learning'].tolist(), dtype=torch.float32).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16089\\AppData\\Local\\Temp\\ipykernel_29400\\3137828461.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  desired_outputs = torch.tensor(desired_outputs, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total loss: 1.3838326930999756\n",
      "Epoch [1/500], Loss: 1.3838\n",
      "total loss: 1.3754180669784546\n",
      "Epoch [2/500], Loss: 1.3754\n",
      "total loss: 1.366914987564087\n",
      "Epoch [3/500], Loss: 1.3669\n",
      "total loss: 1.3587239980697632\n",
      "Epoch [4/500], Loss: 1.3587\n",
      "total loss: 1.3510946035385132\n",
      "Epoch [5/500], Loss: 1.3511\n",
      "total loss: 1.3439035415649414\n",
      "Epoch [6/500], Loss: 1.3439\n",
      "total loss: 1.3370788097381592\n",
      "Epoch [7/500], Loss: 1.3371\n",
      "total loss: 1.3307058811187744\n",
      "Epoch [8/500], Loss: 1.3307\n",
      "total loss: 1.3249000310897827\n",
      "Epoch [9/500], Loss: 1.3249\n",
      "total loss: 1.3193894624710083\n",
      "Epoch [10/500], Loss: 1.3194\n",
      "total loss: 1.3140640258789062\n",
      "Epoch [11/500], Loss: 1.3141\n",
      "total loss: 1.3089449405670166\n",
      "Epoch [12/500], Loss: 1.3089\n",
      "total loss: 1.304017424583435\n",
      "Epoch [13/500], Loss: 1.3040\n",
      "total loss: 1.2991807460784912\n",
      "Epoch [14/500], Loss: 1.2992\n",
      "total loss: 1.2943940162658691\n",
      "Epoch [15/500], Loss: 1.2944\n",
      "total loss: 1.2895092964172363\n",
      "Epoch [16/500], Loss: 1.2895\n",
      "total loss: 1.284536361694336\n",
      "Epoch [17/500], Loss: 1.2845\n",
      "total loss: 1.2794874906539917\n",
      "Epoch [18/500], Loss: 1.2795\n",
      "total loss: 1.2744271755218506\n",
      "Epoch [19/500], Loss: 1.2744\n",
      "total loss: 1.2694276571273804\n",
      "Epoch [20/500], Loss: 1.2694\n",
      "total loss: 1.2644842863082886\n",
      "Epoch [21/500], Loss: 1.2645\n",
      "total loss: 1.2596067190170288\n",
      "Epoch [22/500], Loss: 1.2596\n",
      "total loss: 1.2547581195831299\n",
      "Epoch [23/500], Loss: 1.2548\n",
      "total loss: 1.250028371810913\n",
      "Epoch [24/500], Loss: 1.2500\n",
      "total loss: 1.2453927993774414\n",
      "Epoch [25/500], Loss: 1.2454\n",
      "total loss: 1.2408801317214966\n",
      "Epoch [26/500], Loss: 1.2409\n",
      "total loss: 1.236487865447998\n",
      "Epoch [27/500], Loss: 1.2365\n",
      "total loss: 1.2322278022766113\n",
      "Epoch [28/500], Loss: 1.2322\n",
      "total loss: 1.2280174493789673\n",
      "Epoch [29/500], Loss: 1.2280\n",
      "total loss: 1.223748803138733\n",
      "Epoch [30/500], Loss: 1.2237\n",
      "total loss: 1.2194550037384033\n",
      "Epoch [31/500], Loss: 1.2195\n",
      "total loss: 1.2151850461959839\n",
      "Epoch [32/500], Loss: 1.2152\n",
      "total loss: 1.2108979225158691\n",
      "Epoch [33/500], Loss: 1.2109\n",
      "total loss: 1.206616759300232\n",
      "Epoch [34/500], Loss: 1.2066\n",
      "total loss: 1.2023457288742065\n",
      "Epoch [35/500], Loss: 1.2023\n",
      "total loss: 1.198121190071106\n",
      "Epoch [36/500], Loss: 1.1981\n",
      "total loss: 1.193907380104065\n",
      "Epoch [37/500], Loss: 1.1939\n",
      "total loss: 1.1896523237228394\n",
      "Epoch [38/500], Loss: 1.1897\n",
      "total loss: 1.185473918914795\n",
      "Epoch [39/500], Loss: 1.1855\n",
      "total loss: 1.1813526153564453\n",
      "Epoch [40/500], Loss: 1.1814\n",
      "total loss: 1.1772592067718506\n",
      "Epoch [41/500], Loss: 1.1773\n",
      "total loss: 1.1731815338134766\n",
      "Epoch [42/500], Loss: 1.1732\n",
      "total loss: 1.1691120862960815\n",
      "Epoch [43/500], Loss: 1.1691\n",
      "total loss: 1.1650526523590088\n",
      "Epoch [44/500], Loss: 1.1651\n",
      "total loss: 1.1610167026519775\n",
      "Epoch [45/500], Loss: 1.1610\n",
      "total loss: 1.1569737195968628\n",
      "Epoch [46/500], Loss: 1.1570\n",
      "total loss: 1.1529477834701538\n",
      "Epoch [47/500], Loss: 1.1529\n",
      "total loss: 1.1489325761795044\n",
      "Epoch [48/500], Loss: 1.1489\n",
      "total loss: 1.1449337005615234\n",
      "Epoch [49/500], Loss: 1.1449\n",
      "total loss: 1.1409809589385986\n",
      "Epoch [50/500], Loss: 1.1410\n",
      "total loss: 1.1370450258255005\n",
      "Epoch [51/500], Loss: 1.1370\n",
      "total loss: 1.1331181526184082\n",
      "Epoch [52/500], Loss: 1.1331\n",
      "total loss: 1.129197359085083\n",
      "Epoch [53/500], Loss: 1.1292\n",
      "total loss: 1.1253242492675781\n",
      "Epoch [54/500], Loss: 1.1253\n",
      "total loss: 1.121490716934204\n",
      "Epoch [55/500], Loss: 1.1215\n",
      "total loss: 1.1176501512527466\n",
      "Epoch [56/500], Loss: 1.1177\n",
      "total loss: 1.113816499710083\n",
      "Epoch [57/500], Loss: 1.1138\n",
      "total loss: 1.1099692583084106\n",
      "Epoch [58/500], Loss: 1.1100\n",
      "total loss: 1.1061198711395264\n",
      "Epoch [59/500], Loss: 1.1061\n",
      "total loss: 1.1023236513137817\n",
      "Epoch [60/500], Loss: 1.1023\n",
      "total loss: 1.098570466041565\n",
      "Epoch [61/500], Loss: 1.0986\n",
      "total loss: 1.0948487520217896\n",
      "Epoch [62/500], Loss: 1.0948\n",
      "total loss: 1.0911242961883545\n",
      "Epoch [63/500], Loss: 1.0911\n",
      "total loss: 1.0874004364013672\n",
      "Epoch [64/500], Loss: 1.0874\n",
      "total loss: 1.083712100982666\n",
      "Epoch [65/500], Loss: 1.0837\n",
      "total loss: 1.0800453424453735\n",
      "Epoch [66/500], Loss: 1.0800\n",
      "total loss: 1.076397180557251\n",
      "Epoch [67/500], Loss: 1.0764\n",
      "total loss: 1.0727736949920654\n",
      "Epoch [68/500], Loss: 1.0728\n",
      "total loss: 1.069201946258545\n",
      "Epoch [69/500], Loss: 1.0692\n",
      "total loss: 1.0656702518463135\n",
      "Epoch [70/500], Loss: 1.0657\n",
      "total loss: 1.0621687173843384\n",
      "Epoch [71/500], Loss: 1.0622\n",
      "total loss: 1.0586978197097778\n",
      "Epoch [72/500], Loss: 1.0587\n",
      "total loss: 1.0552562475204468\n",
      "Epoch [73/500], Loss: 1.0553\n",
      "total loss: 1.051819086074829\n",
      "Epoch [74/500], Loss: 1.0518\n",
      "total loss: 1.0483524799346924\n",
      "Epoch [75/500], Loss: 1.0484\n",
      "total loss: 1.0449098348617554\n",
      "Epoch [76/500], Loss: 1.0449\n",
      "total loss: 1.041532278060913\n",
      "Epoch [77/500], Loss: 1.0415\n",
      "total loss: 1.0382243394851685\n",
      "Epoch [78/500], Loss: 1.0382\n",
      "total loss: 1.0349807739257812\n",
      "Epoch [79/500], Loss: 1.0350\n",
      "total loss: 1.031775951385498\n",
      "Epoch [80/500], Loss: 1.0318\n",
      "total loss: 1.0286349058151245\n",
      "Epoch [81/500], Loss: 1.0286\n",
      "total loss: 1.0255426168441772\n",
      "Epoch [82/500], Loss: 1.0255\n",
      "total loss: 1.0225129127502441\n",
      "Epoch [83/500], Loss: 1.0225\n",
      "total loss: 1.0195419788360596\n",
      "Epoch [84/500], Loss: 1.0195\n",
      "total loss: 1.0166239738464355\n",
      "Epoch [85/500], Loss: 1.0166\n",
      "total loss: 1.0137474536895752\n",
      "Epoch [86/500], Loss: 1.0137\n",
      "total loss: 1.0109241008758545\n",
      "Epoch [87/500], Loss: 1.0109\n",
      "total loss: 1.0081257820129395\n",
      "Epoch [88/500], Loss: 1.0081\n",
      "total loss: 1.0053887367248535\n",
      "Epoch [89/500], Loss: 1.0054\n",
      "total loss: 1.002691626548767\n",
      "Epoch [90/500], Loss: 1.0027\n",
      "total loss: 1.0000337362289429\n",
      "Epoch [91/500], Loss: 1.0000\n",
      "total loss: 0.9974256157875061\n",
      "Epoch [92/500], Loss: 0.9974\n",
      "total loss: 0.9948606491088867\n",
      "Epoch [93/500], Loss: 0.9949\n",
      "total loss: 0.9923491477966309\n",
      "Epoch [94/500], Loss: 0.9923\n",
      "total loss: 0.9898945689201355\n",
      "Epoch [95/500], Loss: 0.9899\n",
      "total loss: 0.9874817132949829\n",
      "Epoch [96/500], Loss: 0.9875\n",
      "total loss: 0.9851123690605164\n",
      "Epoch [97/500], Loss: 0.9851\n",
      "total loss: 0.9827762246131897\n",
      "Epoch [98/500], Loss: 0.9828\n",
      "total loss: 0.980476438999176\n",
      "Epoch [99/500], Loss: 0.9805\n",
      "total loss: 0.9782218933105469\n",
      "Epoch [100/500], Loss: 0.9782\n",
      "total loss: 0.9760190844535828\n",
      "Epoch [101/500], Loss: 0.9760\n",
      "total loss: 0.973861575126648\n",
      "Epoch [102/500], Loss: 0.9739\n",
      "total loss: 0.9717353582382202\n",
      "Epoch [103/500], Loss: 0.9717\n",
      "total loss: 0.9696413278579712\n",
      "Epoch [104/500], Loss: 0.9696\n",
      "total loss: 0.9675925970077515\n",
      "Epoch [105/500], Loss: 0.9676\n",
      "total loss: 0.9655698537826538\n",
      "Epoch [106/500], Loss: 0.9656\n",
      "total loss: 0.9635798931121826\n",
      "Epoch [107/500], Loss: 0.9636\n",
      "total loss: 0.9616147875785828\n",
      "Epoch [108/500], Loss: 0.9616\n",
      "total loss: 0.959657609462738\n",
      "Epoch [109/500], Loss: 0.9597\n",
      "total loss: 0.9577060341835022\n",
      "Epoch [110/500], Loss: 0.9577\n",
      "total loss: 0.9557651877403259\n",
      "Epoch [111/500], Loss: 0.9558\n",
      "total loss: 0.9538042545318604\n",
      "Epoch [112/500], Loss: 0.9538\n",
      "total loss: 0.9518142938613892\n",
      "Epoch [113/500], Loss: 0.9518\n",
      "total loss: 0.9497997164726257\n",
      "Epoch [114/500], Loss: 0.9498\n",
      "total loss: 0.9477863311767578\n",
      "Epoch [115/500], Loss: 0.9478\n",
      "total loss: 0.9458110332489014\n",
      "Epoch [116/500], Loss: 0.9458\n",
      "total loss: 0.9439169764518738\n",
      "Epoch [117/500], Loss: 0.9439\n",
      "total loss: 0.9421058297157288\n",
      "Epoch [118/500], Loss: 0.9421\n",
      "total loss: 0.9403610825538635\n",
      "Epoch [119/500], Loss: 0.9404\n",
      "total loss: 0.9386617541313171\n",
      "Epoch [120/500], Loss: 0.9387\n",
      "total loss: 0.9369802474975586\n",
      "Epoch [121/500], Loss: 0.9370\n",
      "total loss: 0.9353023171424866\n",
      "Epoch [122/500], Loss: 0.9353\n",
      "total loss: 0.9335906505584717\n",
      "Epoch [123/500], Loss: 0.9336\n",
      "total loss: 0.9317977428436279\n",
      "Epoch [124/500], Loss: 0.9318\n",
      "total loss: 0.9298615455627441\n",
      "Epoch [125/500], Loss: 0.9299\n",
      "total loss: 0.9278416037559509\n",
      "Epoch [126/500], Loss: 0.9278\n",
      "total loss: 0.92600017786026\n",
      "Epoch [127/500], Loss: 0.9260\n",
      "total loss: 0.9244164824485779\n",
      "Epoch [128/500], Loss: 0.9244\n",
      "total loss: 0.9229621291160583\n",
      "Epoch [129/500], Loss: 0.9230\n",
      "total loss: 0.9215283393859863\n",
      "Epoch [130/500], Loss: 0.9215\n",
      "total loss: 0.9200631976127625\n",
      "Epoch [131/500], Loss: 0.9201\n",
      "total loss: 0.9185810685157776\n",
      "Epoch [132/500], Loss: 0.9186\n",
      "total loss: 0.9171093702316284\n",
      "Epoch [133/500], Loss: 0.9171\n",
      "total loss: 0.915652334690094\n",
      "Epoch [134/500], Loss: 0.9157\n",
      "total loss: 0.9142121076583862\n",
      "Epoch [135/500], Loss: 0.9142\n",
      "total loss: 0.9127854704856873\n",
      "Epoch [136/500], Loss: 0.9128\n",
      "total loss: 0.9113620519638062\n",
      "Epoch [137/500], Loss: 0.9114\n",
      "total loss: 0.909944474697113\n",
      "Epoch [138/500], Loss: 0.9099\n",
      "total loss: 0.9085399508476257\n",
      "Epoch [139/500], Loss: 0.9085\n",
      "total loss: 0.9071592092514038\n",
      "Epoch [140/500], Loss: 0.9072\n",
      "total loss: 0.9057909250259399\n",
      "Epoch [141/500], Loss: 0.9058\n",
      "total loss: 0.9044415354728699\n",
      "Epoch [142/500], Loss: 0.9044\n",
      "total loss: 0.9031199216842651\n",
      "Epoch [143/500], Loss: 0.9031\n",
      "total loss: 0.9018160104751587\n",
      "Epoch [144/500], Loss: 0.9018\n",
      "total loss: 0.9005353450775146\n",
      "Epoch [145/500], Loss: 0.9005\n",
      "total loss: 0.8992660045623779\n",
      "Epoch [146/500], Loss: 0.8993\n",
      "total loss: 0.898012638092041\n",
      "Epoch [147/500], Loss: 0.8980\n",
      "total loss: 0.8967788815498352\n",
      "Epoch [148/500], Loss: 0.8968\n",
      "total loss: 0.8955492377281189\n",
      "Epoch [149/500], Loss: 0.8955\n",
      "total loss: 0.8942949175834656\n",
      "Epoch [150/500], Loss: 0.8943\n",
      "total loss: 0.8930521011352539\n",
      "Epoch [151/500], Loss: 0.8931\n",
      "total loss: 0.8918023705482483\n",
      "Epoch [152/500], Loss: 0.8918\n",
      "total loss: 0.8905465602874756\n",
      "Epoch [153/500], Loss: 0.8905\n",
      "total loss: 0.8892813324928284\n",
      "Epoch [154/500], Loss: 0.8893\n",
      "total loss: 0.8880029320716858\n",
      "Epoch [155/500], Loss: 0.8880\n",
      "total loss: 0.8867180943489075\n",
      "Epoch [156/500], Loss: 0.8867\n",
      "total loss: 0.8854324221611023\n",
      "Epoch [157/500], Loss: 0.8854\n",
      "total loss: 0.8841401934623718\n",
      "Epoch [158/500], Loss: 0.8841\n",
      "total loss: 0.8828460574150085\n",
      "Epoch [159/500], Loss: 0.8828\n",
      "total loss: 0.881577730178833\n",
      "Epoch [160/500], Loss: 0.8816\n",
      "total loss: 0.8803172707557678\n",
      "Epoch [161/500], Loss: 0.8803\n",
      "total loss: 0.8790524005889893\n",
      "Epoch [162/500], Loss: 0.8791\n",
      "total loss: 0.8777962923049927\n",
      "Epoch [163/500], Loss: 0.8778\n",
      "total loss: 0.8765614032745361\n",
      "Epoch [164/500], Loss: 0.8766\n",
      "total loss: 0.8753232359886169\n",
      "Epoch [165/500], Loss: 0.8753\n",
      "total loss: 0.8740979433059692\n",
      "Epoch [166/500], Loss: 0.8741\n",
      "total loss: 0.872871458530426\n",
      "Epoch [167/500], Loss: 0.8729\n",
      "total loss: 0.8716406226158142\n",
      "Epoch [168/500], Loss: 0.8716\n",
      "total loss: 0.8704163432121277\n",
      "Epoch [169/500], Loss: 0.8704\n",
      "total loss: 0.8692029714584351\n",
      "Epoch [170/500], Loss: 0.8692\n",
      "total loss: 0.8680139183998108\n",
      "Epoch [171/500], Loss: 0.8680\n",
      "total loss: 0.8668681383132935\n",
      "Epoch [172/500], Loss: 0.8669\n",
      "total loss: 0.8657718896865845\n",
      "Epoch [173/500], Loss: 0.8658\n",
      "total loss: 0.8647078275680542\n",
      "Epoch [174/500], Loss: 0.8647\n",
      "total loss: 0.8636813759803772\n",
      "Epoch [175/500], Loss: 0.8637\n",
      "total loss: 0.8626844882965088\n",
      "Epoch [176/500], Loss: 0.8627\n",
      "total loss: 0.8617000579833984\n",
      "Epoch [177/500], Loss: 0.8617\n",
      "total loss: 0.8607301115989685\n",
      "Epoch [178/500], Loss: 0.8607\n",
      "total loss: 0.8597739934921265\n",
      "Epoch [179/500], Loss: 0.8598\n",
      "total loss: 0.8588261008262634\n",
      "Epoch [180/500], Loss: 0.8588\n",
      "total loss: 0.8578873872756958\n",
      "Epoch [181/500], Loss: 0.8579\n",
      "total loss: 0.856963574886322\n",
      "Epoch [182/500], Loss: 0.8570\n",
      "total loss: 0.8560516238212585\n",
      "Epoch [183/500], Loss: 0.8561\n",
      "total loss: 0.8551517128944397\n",
      "Epoch [184/500], Loss: 0.8552\n",
      "total loss: 0.8542667627334595\n",
      "Epoch [185/500], Loss: 0.8543\n",
      "total loss: 0.8533889651298523\n",
      "Epoch [186/500], Loss: 0.8534\n",
      "total loss: 0.8525230288505554\n",
      "Epoch [187/500], Loss: 0.8525\n",
      "total loss: 0.8516696691513062\n",
      "Epoch [188/500], Loss: 0.8517\n",
      "total loss: 0.850827157497406\n",
      "Epoch [189/500], Loss: 0.8508\n",
      "total loss: 0.8500140905380249\n",
      "Epoch [190/500], Loss: 0.8500\n",
      "total loss: 0.8492234349250793\n",
      "Epoch [191/500], Loss: 0.8492\n",
      "total loss: 0.848441481590271\n",
      "Epoch [192/500], Loss: 0.8484\n",
      "total loss: 0.847667396068573\n",
      "Epoch [193/500], Loss: 0.8477\n",
      "total loss: 0.8469047546386719\n",
      "Epoch [194/500], Loss: 0.8469\n",
      "total loss: 0.8461535573005676\n",
      "Epoch [195/500], Loss: 0.8462\n",
      "total loss: 0.8454044461250305\n",
      "Epoch [196/500], Loss: 0.8454\n",
      "total loss: 0.8446576595306396\n",
      "Epoch [197/500], Loss: 0.8447\n",
      "total loss: 0.8439186811447144\n",
      "Epoch [198/500], Loss: 0.8439\n",
      "total loss: 0.843183159828186\n",
      "Epoch [199/500], Loss: 0.8432\n",
      "total loss: 0.8424555063247681\n",
      "Epoch [200/500], Loss: 0.8425\n",
      "total loss: 0.8417294025421143\n",
      "Epoch [201/500], Loss: 0.8417\n",
      "total loss: 0.8410053253173828\n",
      "Epoch [202/500], Loss: 0.8410\n",
      "total loss: 0.8402806520462036\n",
      "Epoch [203/500], Loss: 0.8403\n",
      "total loss: 0.8395641446113586\n",
      "Epoch [204/500], Loss: 0.8396\n",
      "total loss: 0.8388540148735046\n",
      "Epoch [205/500], Loss: 0.8389\n",
      "total loss: 0.8381446003913879\n",
      "Epoch [206/500], Loss: 0.8381\n",
      "total loss: 0.8374386429786682\n",
      "Epoch [207/500], Loss: 0.8374\n",
      "total loss: 0.8367392420768738\n",
      "Epoch [208/500], Loss: 0.8367\n",
      "total loss: 0.8360474109649658\n",
      "Epoch [209/500], Loss: 0.8360\n",
      "total loss: 0.8353634476661682\n",
      "Epoch [210/500], Loss: 0.8354\n",
      "total loss: 0.8346849679946899\n",
      "Epoch [211/500], Loss: 0.8347\n",
      "total loss: 0.8340216279029846\n",
      "Epoch [212/500], Loss: 0.8340\n",
      "total loss: 0.8333689570426941\n",
      "Epoch [213/500], Loss: 0.8334\n",
      "total loss: 0.8327307105064392\n",
      "Epoch [214/500], Loss: 0.8327\n",
      "total loss: 0.8320991396903992\n",
      "Epoch [215/500], Loss: 0.8321\n",
      "total loss: 0.831482470035553\n",
      "Epoch [216/500], Loss: 0.8315\n",
      "total loss: 0.8308815360069275\n",
      "Epoch [217/500], Loss: 0.8309\n",
      "total loss: 0.8302931189537048\n",
      "Epoch [218/500], Loss: 0.8303\n",
      "total loss: 0.8297165036201477\n",
      "Epoch [219/500], Loss: 0.8297\n",
      "total loss: 0.8291499018669128\n",
      "Epoch [220/500], Loss: 0.8291\n",
      "total loss: 0.8285935521125793\n",
      "Epoch [221/500], Loss: 0.8286\n",
      "total loss: 0.8280508518218994\n",
      "Epoch [222/500], Loss: 0.8281\n",
      "total loss: 0.8275198936462402\n",
      "Epoch [223/500], Loss: 0.8275\n",
      "total loss: 0.8269953727722168\n",
      "Epoch [224/500], Loss: 0.8270\n",
      "total loss: 0.826480507850647\n",
      "Epoch [225/500], Loss: 0.8265\n",
      "total loss: 0.8259761333465576\n",
      "Epoch [226/500], Loss: 0.8260\n",
      "total loss: 0.8254812955856323\n",
      "Epoch [227/500], Loss: 0.8255\n",
      "total loss: 0.8249955177307129\n",
      "Epoch [228/500], Loss: 0.8250\n",
      "total loss: 0.824515700340271\n",
      "Epoch [229/500], Loss: 0.8245\n",
      "total loss: 0.8240392208099365\n",
      "Epoch [230/500], Loss: 0.8240\n",
      "total loss: 0.8235710263252258\n",
      "Epoch [231/500], Loss: 0.8236\n",
      "total loss: 0.8231074810028076\n",
      "Epoch [232/500], Loss: 0.8231\n",
      "total loss: 0.822651207447052\n",
      "Epoch [233/500], Loss: 0.8227\n",
      "total loss: 0.8222022652626038\n",
      "Epoch [234/500], Loss: 0.8222\n",
      "total loss: 0.8217595219612122\n",
      "Epoch [235/500], Loss: 0.8218\n",
      "total loss: 0.8213223814964294\n",
      "Epoch [236/500], Loss: 0.8213\n",
      "total loss: 0.820891261100769\n",
      "Epoch [237/500], Loss: 0.8209\n",
      "total loss: 0.820462167263031\n",
      "Epoch [238/500], Loss: 0.8205\n",
      "total loss: 0.8200357556343079\n",
      "Epoch [239/500], Loss: 0.8200\n",
      "total loss: 0.8196148872375488\n",
      "Epoch [240/500], Loss: 0.8196\n",
      "total loss: 0.8191985487937927\n",
      "Epoch [241/500], Loss: 0.8192\n",
      "total loss: 0.8187826871871948\n",
      "Epoch [242/500], Loss: 0.8188\n",
      "total loss: 0.818371057510376\n",
      "Epoch [243/500], Loss: 0.8184\n",
      "total loss: 0.8179593682289124\n",
      "Epoch [244/500], Loss: 0.8180\n",
      "total loss: 0.8175463080406189\n",
      "Epoch [245/500], Loss: 0.8175\n",
      "total loss: 0.8171285390853882\n",
      "Epoch [246/500], Loss: 0.8171\n",
      "total loss: 0.8167030811309814\n",
      "Epoch [247/500], Loss: 0.8167\n",
      "total loss: 0.8162633776664734\n",
      "Epoch [248/500], Loss: 0.8163\n",
      "total loss: 0.8158031702041626\n",
      "Epoch [249/500], Loss: 0.8158\n",
      "total loss: 0.8153114914894104\n",
      "Epoch [250/500], Loss: 0.8153\n",
      "total loss: 0.814764142036438\n",
      "Epoch [251/500], Loss: 0.8148\n",
      "total loss: 0.8141381144523621\n",
      "Epoch [252/500], Loss: 0.8141\n",
      "total loss: 0.8134191632270813\n",
      "Epoch [253/500], Loss: 0.8134\n",
      "total loss: 0.8126019239425659\n",
      "Epoch [254/500], Loss: 0.8126\n",
      "total loss: 0.811704158782959\n",
      "Epoch [255/500], Loss: 0.8117\n",
      "total loss: 0.810724139213562\n",
      "Epoch [256/500], Loss: 0.8107\n",
      "total loss: 0.8095970749855042\n",
      "Epoch [257/500], Loss: 0.8096\n",
      "total loss: 0.8082793354988098\n",
      "Epoch [258/500], Loss: 0.8083\n",
      "total loss: 0.8069482445716858\n",
      "Epoch [259/500], Loss: 0.8069\n",
      "total loss: 0.805858314037323\n",
      "Epoch [260/500], Loss: 0.8059\n",
      "total loss: 0.8050858378410339\n",
      "Epoch [261/500], Loss: 0.8051\n",
      "total loss: 0.8045144081115723\n",
      "Epoch [262/500], Loss: 0.8045\n",
      "total loss: 0.8040282726287842\n",
      "Epoch [263/500], Loss: 0.8040\n",
      "total loss: 0.8035768866539001\n",
      "Epoch [264/500], Loss: 0.8036\n",
      "total loss: 0.8031401634216309\n",
      "Epoch [265/500], Loss: 0.8031\n",
      "total loss: 0.8027064204216003\n",
      "Epoch [266/500], Loss: 0.8027\n",
      "total loss: 0.8022802472114563\n",
      "Epoch [267/500], Loss: 0.8023\n",
      "total loss: 0.8018608093261719\n",
      "Epoch [268/500], Loss: 0.8019\n",
      "total loss: 0.8014483451843262\n",
      "Epoch [269/500], Loss: 0.8014\n",
      "total loss: 0.8010422587394714\n",
      "Epoch [270/500], Loss: 0.8010\n",
      "total loss: 0.8006430268287659\n",
      "Epoch [271/500], Loss: 0.8006\n",
      "total loss: 0.8002519607543945\n",
      "Epoch [272/500], Loss: 0.8003\n",
      "total loss: 0.7998684644699097\n",
      "Epoch [273/500], Loss: 0.7999\n",
      "total loss: 0.7994887828826904\n",
      "Epoch [274/500], Loss: 0.7995\n",
      "total loss: 0.7991124391555786\n",
      "Epoch [275/500], Loss: 0.7991\n",
      "total loss: 0.7987425923347473\n",
      "Epoch [276/500], Loss: 0.7987\n",
      "total loss: 0.7983794212341309\n",
      "Epoch [277/500], Loss: 0.7984\n",
      "total loss: 0.7980228066444397\n",
      "Epoch [278/500], Loss: 0.7980\n",
      "total loss: 0.7976759672164917\n",
      "Epoch [279/500], Loss: 0.7977\n",
      "total loss: 0.797336220741272\n",
      "Epoch [280/500], Loss: 0.7973\n",
      "total loss: 0.7970058917999268\n",
      "Epoch [281/500], Loss: 0.7970\n",
      "total loss: 0.7966843247413635\n",
      "Epoch [282/500], Loss: 0.7967\n",
      "total loss: 0.7963704466819763\n",
      "Epoch [283/500], Loss: 0.7964\n",
      "total loss: 0.7960647940635681\n",
      "Epoch [284/500], Loss: 0.7961\n",
      "total loss: 0.7957680225372314\n",
      "Epoch [285/500], Loss: 0.7958\n",
      "total loss: 0.7954760193824768\n",
      "Epoch [286/500], Loss: 0.7955\n",
      "total loss: 0.7951881885528564\n",
      "Epoch [287/500], Loss: 0.7952\n",
      "total loss: 0.7949060797691345\n",
      "Epoch [288/500], Loss: 0.7949\n",
      "total loss: 0.794628381729126\n",
      "Epoch [289/500], Loss: 0.7946\n",
      "total loss: 0.7943552732467651\n",
      "Epoch [290/500], Loss: 0.7944\n",
      "total loss: 0.7940859198570251\n",
      "Epoch [291/500], Loss: 0.7941\n",
      "total loss: 0.7938187718391418\n",
      "Epoch [292/500], Loss: 0.7938\n",
      "total loss: 0.793555498123169\n",
      "Epoch [293/500], Loss: 0.7936\n",
      "total loss: 0.7932955622673035\n",
      "Epoch [294/500], Loss: 0.7933\n",
      "total loss: 0.793039083480835\n",
      "Epoch [295/500], Loss: 0.7930\n",
      "total loss: 0.7927855849266052\n",
      "Epoch [296/500], Loss: 0.7928\n",
      "total loss: 0.7925350069999695\n",
      "Epoch [297/500], Loss: 0.7925\n",
      "total loss: 0.7922894358634949\n",
      "Epoch [298/500], Loss: 0.7923\n",
      "total loss: 0.7920467257499695\n",
      "Epoch [299/500], Loss: 0.7920\n",
      "total loss: 0.7918077111244202\n",
      "Epoch [300/500], Loss: 0.7918\n",
      "total loss: 0.7915740013122559\n",
      "Epoch [301/500], Loss: 0.7916\n",
      "total loss: 0.7913429141044617\n",
      "Epoch [302/500], Loss: 0.7913\n",
      "total loss: 0.7911136150360107\n",
      "Epoch [303/500], Loss: 0.7911\n",
      "total loss: 0.7908874750137329\n",
      "Epoch [304/500], Loss: 0.7909\n",
      "total loss: 0.7906641960144043\n",
      "Epoch [305/500], Loss: 0.7907\n",
      "total loss: 0.7904461622238159\n",
      "Epoch [306/500], Loss: 0.7904\n",
      "total loss: 0.7902300357818604\n",
      "Epoch [307/500], Loss: 0.7902\n",
      "total loss: 0.7900192737579346\n",
      "Epoch [308/500], Loss: 0.7900\n",
      "total loss: 0.7898101210594177\n",
      "Epoch [309/500], Loss: 0.7898\n",
      "total loss: 0.7896018624305725\n",
      "Epoch [310/500], Loss: 0.7896\n",
      "total loss: 0.789394736289978\n",
      "Epoch [311/500], Loss: 0.7894\n",
      "total loss: 0.7891913056373596\n",
      "Epoch [312/500], Loss: 0.7892\n",
      "total loss: 0.7889906167984009\n",
      "Epoch [313/500], Loss: 0.7890\n",
      "total loss: 0.7887945175170898\n",
      "Epoch [314/500], Loss: 0.7888\n",
      "total loss: 0.7885992527008057\n",
      "Epoch [315/500], Loss: 0.7886\n",
      "total loss: 0.7884060740470886\n",
      "Epoch [316/500], Loss: 0.7884\n",
      "total loss: 0.7882152795791626\n",
      "Epoch [317/500], Loss: 0.7882\n",
      "total loss: 0.7880277633666992\n",
      "Epoch [318/500], Loss: 0.7880\n",
      "total loss: 0.7878426313400269\n",
      "Epoch [319/500], Loss: 0.7878\n",
      "total loss: 0.7876585125923157\n",
      "Epoch [320/500], Loss: 0.7877\n",
      "total loss: 0.7874771356582642\n",
      "Epoch [321/500], Loss: 0.7875\n",
      "total loss: 0.7872985005378723\n",
      "Epoch [322/500], Loss: 0.7873\n",
      "total loss: 0.7871208786964417\n",
      "Epoch [323/500], Loss: 0.7871\n",
      "total loss: 0.7869448661804199\n",
      "Epoch [324/500], Loss: 0.7869\n",
      "total loss: 0.7867711782455444\n",
      "Epoch [325/500], Loss: 0.7868\n",
      "total loss: 0.7866002321243286\n",
      "Epoch [326/500], Loss: 0.7866\n",
      "total loss: 0.7864320278167725\n",
      "Epoch [327/500], Loss: 0.7864\n",
      "total loss: 0.7862651348114014\n",
      "Epoch [328/500], Loss: 0.7863\n",
      "total loss: 0.7861003875732422\n",
      "Epoch [329/500], Loss: 0.7861\n",
      "total loss: 0.7859379649162292\n",
      "Epoch [330/500], Loss: 0.7859\n",
      "total loss: 0.7857778072357178\n",
      "Epoch [331/500], Loss: 0.7858\n",
      "total loss: 0.7856189608573914\n",
      "Epoch [332/500], Loss: 0.7856\n",
      "total loss: 0.7854612469673157\n",
      "Epoch [333/500], Loss: 0.7855\n",
      "total loss: 0.7853058576583862\n",
      "Epoch [334/500], Loss: 0.7853\n",
      "total loss: 0.7851537466049194\n",
      "Epoch [335/500], Loss: 0.7852\n",
      "total loss: 0.7850034832954407\n",
      "Epoch [336/500], Loss: 0.7850\n",
      "total loss: 0.7848538756370544\n",
      "Epoch [337/500], Loss: 0.7849\n",
      "total loss: 0.7847049832344055\n",
      "Epoch [338/500], Loss: 0.7847\n",
      "total loss: 0.7845581769943237\n",
      "Epoch [339/500], Loss: 0.7846\n",
      "total loss: 0.7844128608703613\n",
      "Epoch [340/500], Loss: 0.7844\n",
      "total loss: 0.7842682600021362\n",
      "Epoch [341/500], Loss: 0.7843\n",
      "total loss: 0.7841253876686096\n",
      "Epoch [342/500], Loss: 0.7841\n",
      "total loss: 0.7839856743812561\n",
      "Epoch [343/500], Loss: 0.7840\n",
      "total loss: 0.7838464379310608\n",
      "Epoch [344/500], Loss: 0.7838\n",
      "total loss: 0.7837083339691162\n",
      "Epoch [345/500], Loss: 0.7837\n",
      "total loss: 0.7835717797279358\n",
      "Epoch [346/500], Loss: 0.7836\n",
      "total loss: 0.7834362983703613\n",
      "Epoch [347/500], Loss: 0.7834\n",
      "total loss: 0.7833030223846436\n",
      "Epoch [348/500], Loss: 0.7833\n",
      "total loss: 0.7831707000732422\n",
      "Epoch [349/500], Loss: 0.7832\n",
      "total loss: 0.7830386161804199\n",
      "Epoch [350/500], Loss: 0.7830\n",
      "total loss: 0.7829092144966125\n",
      "Epoch [351/500], Loss: 0.7829\n",
      "total loss: 0.7827814817428589\n",
      "Epoch [352/500], Loss: 0.7828\n",
      "total loss: 0.7826546430587769\n",
      "Epoch [353/500], Loss: 0.7827\n",
      "total loss: 0.7825286984443665\n",
      "Epoch [354/500], Loss: 0.7825\n",
      "total loss: 0.7824039459228516\n",
      "Epoch [355/500], Loss: 0.7824\n",
      "total loss: 0.7822811603546143\n",
      "Epoch [356/500], Loss: 0.7823\n",
      "total loss: 0.7821587920188904\n",
      "Epoch [357/500], Loss: 0.7822\n",
      "total loss: 0.7820379734039307\n",
      "Epoch [358/500], Loss: 0.7820\n",
      "total loss: 0.7819180488586426\n",
      "Epoch [359/500], Loss: 0.7819\n",
      "total loss: 0.7817992568016052\n",
      "Epoch [360/500], Loss: 0.7818\n",
      "total loss: 0.7816827297210693\n",
      "Epoch [361/500], Loss: 0.7817\n",
      "total loss: 0.7815664410591125\n",
      "Epoch [362/500], Loss: 0.7816\n",
      "total loss: 0.7814509272575378\n",
      "Epoch [363/500], Loss: 0.7815\n",
      "total loss: 0.7813372611999512\n",
      "Epoch [364/500], Loss: 0.7813\n",
      "total loss: 0.7812248468399048\n",
      "Epoch [365/500], Loss: 0.7812\n",
      "total loss: 0.78111332654953\n",
      "Epoch [366/500], Loss: 0.7811\n",
      "total loss: 0.7810027599334717\n",
      "Epoch [367/500], Loss: 0.7810\n",
      "total loss: 0.7808924913406372\n",
      "Epoch [368/500], Loss: 0.7809\n",
      "total loss: 0.7807849049568176\n",
      "Epoch [369/500], Loss: 0.7808\n",
      "total loss: 0.7806785702705383\n",
      "Epoch [370/500], Loss: 0.7807\n",
      "total loss: 0.7805713415145874\n",
      "Epoch [371/500], Loss: 0.7806\n",
      "total loss: 0.7804660201072693\n",
      "Epoch [372/500], Loss: 0.7805\n",
      "total loss: 0.7803617119789124\n",
      "Epoch [373/500], Loss: 0.7804\n",
      "total loss: 0.7802578210830688\n",
      "Epoch [374/500], Loss: 0.7803\n",
      "total loss: 0.7801552414894104\n",
      "Epoch [375/500], Loss: 0.7802\n",
      "total loss: 0.7800542712211609\n",
      "Epoch [376/500], Loss: 0.7801\n",
      "total loss: 0.779954195022583\n",
      "Epoch [377/500], Loss: 0.7800\n",
      "total loss: 0.7798545360565186\n",
      "Epoch [378/500], Loss: 0.7799\n",
      "total loss: 0.7797560095787048\n",
      "Epoch [379/500], Loss: 0.7798\n",
      "total loss: 0.7796580791473389\n",
      "Epoch [380/500], Loss: 0.7797\n",
      "total loss: 0.7795606851577759\n",
      "Epoch [381/500], Loss: 0.7796\n",
      "total loss: 0.7794644236564636\n",
      "Epoch [382/500], Loss: 0.7795\n",
      "total loss: 0.7793693542480469\n",
      "Epoch [383/500], Loss: 0.7794\n",
      "total loss: 0.7792752385139465\n",
      "Epoch [384/500], Loss: 0.7793\n",
      "total loss: 0.7791818380355835\n",
      "Epoch [385/500], Loss: 0.7792\n",
      "total loss: 0.7790887355804443\n",
      "Epoch [386/500], Loss: 0.7791\n",
      "total loss: 0.7789969444274902\n",
      "Epoch [387/500], Loss: 0.7790\n",
      "total loss: 0.7789057493209839\n",
      "Epoch [388/500], Loss: 0.7789\n",
      "total loss: 0.7788153290748596\n",
      "Epoch [389/500], Loss: 0.7788\n",
      "total loss: 0.7787262201309204\n",
      "Epoch [390/500], Loss: 0.7787\n",
      "total loss: 0.7786383032798767\n",
      "Epoch [391/500], Loss: 0.7786\n",
      "total loss: 0.7785503268241882\n",
      "Epoch [392/500], Loss: 0.7786\n",
      "total loss: 0.7784627676010132\n",
      "Epoch [393/500], Loss: 0.7785\n",
      "total loss: 0.7783763408660889\n",
      "Epoch [394/500], Loss: 0.7784\n",
      "total loss: 0.7782907485961914\n",
      "Epoch [395/500], Loss: 0.7783\n",
      "total loss: 0.7782061696052551\n",
      "Epoch [396/500], Loss: 0.7782\n",
      "total loss: 0.7781222462654114\n",
      "Epoch [397/500], Loss: 0.7781\n",
      "total loss: 0.7780389785766602\n",
      "Epoch [398/500], Loss: 0.7780\n",
      "total loss: 0.7779564261436462\n",
      "Epoch [399/500], Loss: 0.7780\n",
      "total loss: 0.7778742909431458\n",
      "Epoch [400/500], Loss: 0.7779\n",
      "total loss: 0.7777931690216064\n",
      "Epoch [401/500], Loss: 0.7778\n",
      "total loss: 0.7777125239372253\n",
      "Epoch [402/500], Loss: 0.7777\n",
      "total loss: 0.7776325941085815\n",
      "Epoch [403/500], Loss: 0.7776\n",
      "total loss: 0.7775533199310303\n",
      "Epoch [404/500], Loss: 0.7776\n",
      "total loss: 0.777475118637085\n",
      "Epoch [405/500], Loss: 0.7775\n",
      "total loss: 0.7773968577384949\n",
      "Epoch [406/500], Loss: 0.7774\n",
      "total loss: 0.7773197293281555\n",
      "Epoch [407/500], Loss: 0.7773\n",
      "total loss: 0.7772433161735535\n",
      "Epoch [408/500], Loss: 0.7772\n",
      "total loss: 0.7771669030189514\n",
      "Epoch [409/500], Loss: 0.7772\n",
      "total loss: 0.7770922780036926\n",
      "Epoch [410/500], Loss: 0.7771\n",
      "total loss: 0.7770177125930786\n",
      "Epoch [411/500], Loss: 0.7770\n",
      "total loss: 0.776943564414978\n",
      "Epoch [412/500], Loss: 0.7769\n",
      "total loss: 0.7768703103065491\n",
      "Epoch [413/500], Loss: 0.7769\n",
      "total loss: 0.7767974734306335\n",
      "Epoch [414/500], Loss: 0.7768\n",
      "total loss: 0.776725172996521\n",
      "Epoch [415/500], Loss: 0.7767\n",
      "total loss: 0.7766536474227905\n",
      "Epoch [416/500], Loss: 0.7767\n",
      "total loss: 0.7765827178955078\n",
      "Epoch [417/500], Loss: 0.7766\n",
      "total loss: 0.776512086391449\n",
      "Epoch [418/500], Loss: 0.7765\n",
      "total loss: 0.7764423489570618\n",
      "Epoch [419/500], Loss: 0.7764\n",
      "total loss: 0.776373028755188\n",
      "Epoch [420/500], Loss: 0.7764\n",
      "total loss: 0.7763040661811829\n",
      "Epoch [421/500], Loss: 0.7763\n",
      "total loss: 0.776235818862915\n",
      "Epoch [422/500], Loss: 0.7762\n",
      "total loss: 0.7761679887771606\n",
      "Epoch [423/500], Loss: 0.7762\n",
      "total loss: 0.7761006951332092\n",
      "Epoch [424/500], Loss: 0.7761\n",
      "total loss: 0.7760339975357056\n",
      "Epoch [425/500], Loss: 0.7760\n",
      "total loss: 0.7759681940078735\n",
      "Epoch [426/500], Loss: 0.7760\n",
      "total loss: 0.7759027481079102\n",
      "Epoch [427/500], Loss: 0.7759\n",
      "total loss: 0.7758380174636841\n",
      "Epoch [428/500], Loss: 0.7758\n",
      "total loss: 0.775773286819458\n",
      "Epoch [429/500], Loss: 0.7758\n",
      "total loss: 0.7757092714309692\n",
      "Epoch [430/500], Loss: 0.7757\n",
      "total loss: 0.7756459712982178\n",
      "Epoch [431/500], Loss: 0.7756\n",
      "total loss: 0.7755831480026245\n",
      "Epoch [432/500], Loss: 0.7756\n",
      "total loss: 0.7755205631256104\n",
      "Epoch [433/500], Loss: 0.7755\n",
      "total loss: 0.775458574295044\n",
      "Epoch [434/500], Loss: 0.7755\n",
      "total loss: 0.7753969430923462\n",
      "Epoch [435/500], Loss: 0.7754\n",
      "total loss: 0.7753360867500305\n",
      "Epoch [436/500], Loss: 0.7753\n",
      "total loss: 0.775275468826294\n",
      "Epoch [437/500], Loss: 0.7753\n",
      "total loss: 0.7752158641815186\n",
      "Epoch [438/500], Loss: 0.7752\n",
      "total loss: 0.7751563191413879\n",
      "Epoch [439/500], Loss: 0.7752\n",
      "total loss: 0.7750971913337708\n",
      "Epoch [440/500], Loss: 0.7751\n",
      "total loss: 0.7750387191772461\n",
      "Epoch [441/500], Loss: 0.7750\n",
      "total loss: 0.7749802470207214\n",
      "Epoch [442/500], Loss: 0.7750\n",
      "total loss: 0.7749228477478027\n",
      "Epoch [443/500], Loss: 0.7749\n",
      "total loss: 0.7748656272888184\n",
      "Epoch [444/500], Loss: 0.7749\n",
      "total loss: 0.7748088836669922\n",
      "Epoch [445/500], Loss: 0.7748\n",
      "total loss: 0.7747522592544556\n",
      "Epoch [446/500], Loss: 0.7748\n",
      "total loss: 0.7746965885162354\n",
      "Epoch [447/500], Loss: 0.7747\n",
      "total loss: 0.7746412754058838\n",
      "Epoch [448/500], Loss: 0.7746\n",
      "total loss: 0.7745858430862427\n",
      "Epoch [449/500], Loss: 0.7746\n",
      "total loss: 0.774531364440918\n",
      "Epoch [450/500], Loss: 0.7745\n",
      "total loss: 0.7744771838188171\n",
      "Epoch [451/500], Loss: 0.7745\n",
      "total loss: 0.7744235396385193\n",
      "Epoch [452/500], Loss: 0.7744\n",
      "total loss: 0.7743700742721558\n",
      "Epoch [453/500], Loss: 0.7744\n",
      "total loss: 0.7743167877197266\n",
      "Epoch [454/500], Loss: 0.7743\n",
      "total loss: 0.774264395236969\n",
      "Epoch [455/500], Loss: 0.7743\n",
      "total loss: 0.7742127180099487\n",
      "Epoch [456/500], Loss: 0.7742\n",
      "total loss: 0.7741607427597046\n",
      "Epoch [457/500], Loss: 0.7742\n",
      "total loss: 0.7741093635559082\n",
      "Epoch [458/500], Loss: 0.7741\n",
      "total loss: 0.7740578651428223\n",
      "Epoch [459/500], Loss: 0.7741\n",
      "total loss: 0.7740075588226318\n",
      "Epoch [460/500], Loss: 0.7740\n",
      "total loss: 0.7739577293395996\n",
      "Epoch [461/500], Loss: 0.7740\n",
      "total loss: 0.7739076614379883\n",
      "Epoch [462/500], Loss: 0.7739\n",
      "total loss: 0.7738584280014038\n",
      "Epoch [463/500], Loss: 0.7739\n",
      "total loss: 0.7738090753555298\n",
      "Epoch [464/500], Loss: 0.7738\n",
      "total loss: 0.7737603783607483\n",
      "Epoch [465/500], Loss: 0.7738\n",
      "total loss: 0.7737120389938354\n",
      "Epoch [466/500], Loss: 0.7737\n",
      "total loss: 0.7736640572547913\n",
      "Epoch [467/500], Loss: 0.7737\n",
      "total loss: 0.7736161947250366\n",
      "Epoch [468/500], Loss: 0.7736\n",
      "total loss: 0.7735688090324402\n",
      "Epoch [469/500], Loss: 0.7736\n",
      "total loss: 0.7735220193862915\n",
      "Epoch [470/500], Loss: 0.7735\n",
      "total loss: 0.7734754085540771\n",
      "Epoch [471/500], Loss: 0.7735\n",
      "total loss: 0.7734293341636658\n",
      "Epoch [472/500], Loss: 0.7734\n",
      "total loss: 0.7733832597732544\n",
      "Epoch [473/500], Loss: 0.7734\n",
      "total loss: 0.7733380794525146\n",
      "Epoch [474/500], Loss: 0.7733\n",
      "total loss: 0.7732927203178406\n",
      "Epoch [475/500], Loss: 0.7733\n",
      "total loss: 0.7732473611831665\n",
      "Epoch [476/500], Loss: 0.7732\n",
      "total loss: 0.7732031345367432\n",
      "Epoch [477/500], Loss: 0.7732\n",
      "total loss: 0.7731590867042542\n",
      "Epoch [478/500], Loss: 0.7732\n",
      "total loss: 0.7731146812438965\n",
      "Epoch [479/500], Loss: 0.7731\n",
      "total loss: 0.7730711698532104\n",
      "Epoch [480/500], Loss: 0.7731\n",
      "total loss: 0.7730280160903931\n",
      "Epoch [481/500], Loss: 0.7730\n",
      "total loss: 0.7729848623275757\n",
      "Epoch [482/500], Loss: 0.7730\n",
      "total loss: 0.7729418873786926\n",
      "Epoch [483/500], Loss: 0.7729\n",
      "total loss: 0.7728992104530334\n",
      "Epoch [484/500], Loss: 0.7729\n",
      "total loss: 0.7728573679924011\n",
      "Epoch [485/500], Loss: 0.7729\n",
      "total loss: 0.7728153467178345\n",
      "Epoch [486/500], Loss: 0.7728\n",
      "total loss: 0.7727735042572021\n",
      "Epoch [487/500], Loss: 0.7728\n",
      "total loss: 0.7727317214012146\n",
      "Epoch [488/500], Loss: 0.7727\n",
      "total loss: 0.772691011428833\n",
      "Epoch [489/500], Loss: 0.7727\n",
      "total loss: 0.7726502418518066\n",
      "Epoch [490/500], Loss: 0.7727\n",
      "total loss: 0.7726092338562012\n",
      "Epoch [491/500], Loss: 0.7726\n",
      "total loss: 0.7725690603256226\n",
      "Epoch [492/500], Loss: 0.7726\n",
      "total loss: 0.7725294232368469\n",
      "Epoch [493/500], Loss: 0.7725\n",
      "total loss: 0.7724893093109131\n",
      "Epoch [494/500], Loss: 0.7725\n",
      "total loss: 0.7724494934082031\n",
      "Epoch [495/500], Loss: 0.7724\n",
      "total loss: 0.7724103927612305\n",
      "Epoch [496/500], Loss: 0.7724\n",
      "total loss: 0.7723714113235474\n",
      "Epoch [497/500], Loss: 0.7724\n",
      "total loss: 0.772332489490509\n",
      "Epoch [498/500], Loss: 0.7723\n",
      "total loss: 0.7722939252853394\n",
      "Epoch [499/500], Loss: 0.7723\n",
      "total loss: 0.7722557783126831\n",
      "Epoch [500/500], Loss: 0.7723\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGwCAYAAABVdURTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA8uklEQVR4nO3dd3RUdf7/8dfMBBICJBAgDYKEIghIaCZGVl0gioAo6q4FXVDXgoJfFP3uMUsTXUW/KqKCoK5ld1UQGxawICBNFIFEQDoEEiWhiKlAIJn7+4NfZomEZO5k+jwf58w5cudzJ++54ua1n2oxDMMQAABAkLD6ugAAAAB3ItwAAICgQrgBAABBhXADAACCCuEGAAAEFcINAAAIKoQbAAAQVMJ8XYC32e127d+/X02bNpXFYvF1OQAAwAmGYaikpESJiYmyWmvvmwm5cLN//34lJSX5ugwAAOCCvLw8tWnTptY2IRdumjZtKunUw4mKivJxNQAAwBnFxcVKSkpy/B6vTciFm6qhqKioKMINAAABxpkpJUwoBgAAQcWn4WbFihUaNmyYEhMTZbFYtGDBAqfvXb16tcLCwtSzZ0+P1QcAAAKPT8NNWVmZUlJSNGvWLFP3FRYWauTIkRo4cKCHKgMAAIHKp3NuBg8erMGDB5u+b/To0RoxYoRsNpup3h4AABD8Am7OzRtvvKE9e/ZoypQpTrUvLy9XcXFxtRcAAAheARVudu7cqYcfflhvvfWWwsKc63SaNm2aoqOjHS/2uAEAILgFTLiprKzUiBEjNHXqVJ177rlO35eZmamioiLHKy8vz4NVAgAAXwuYfW5KSkq0bt06ZWVlaezYsZJOHaVgGIbCwsL01VdfacCAAWfcFx4ervDwcG+XCwAAfCRgwk1UVJQ2bdpU7dpLL72kpUuX6v3331dycrKPKgMAAP7Ep+GmtLRUu3btcvw5JydH2dnZiomJUdu2bZWZmalffvlF//73v2W1WtW9e/dq98fGxioiIuKM675QaTe0NueIDpYcV2zTCKUmx8hm5WBOAAC8zafhZt26derfv7/jz+PHj5ckjRo1Sm+++aby8/OVm5vrq/Kc9sXmfE39dIvyi447riVER2jKsK66onuCDysDACD0WAzDMHxdhDcVFxcrOjpaRUVFbjlb6ovN+brnrQ36/UOs6rOZfUtvAg4AAPVk5vd3wKyW8keVdkNTP91yRrCR5Lg29dMtqrSHVH4EAMCnCDf1sDbnSLWhqN8zJOUXHdfanCPeKwoAgBBHuKmHgyVnDzautAMAAPVHuKmH2KYRbm0HAADqj3BTD6nJMUqIrju4/FZ2wgvVAAAAiXBTLzarRZOGnldnu8cWMqkYAABvIdzUU/PGdR/twKRiAAC8h3BTT0wqBgDAvxBu6snZycJ7Dx/1cCUAAEAi3NRbanKM4qPqHpqa90Mu824AAPACwk092awW3ZTats52zLsBAMA7CDdu0K5lY6faMe8GAADPI9y4AZv5AQDgPwg3bsBmfgAA+A/CjRuwmR8AAP6DcOMmbOYHAIB/INy4CZv5AQDgHwg3bsJmfgAA+AfCjZuwmR8AAP6BcOMmbOYHAIB/INy4EZv5AQDge4QbN2IzPwAAfI9w40Z9zmkuq6X2NlbLqXYAAMAzCDdutH7fb6prrrDdONUOAAB4BuHGjdjrBgAA3yPcuBF73QAA4HuEGzdirxsAAHyPcONG7HUDAIDvEW7czNm9bhZvKfBwJQAAhCbCjZs5O+/m4+z9DE0BAOABhBs3S02OUUzjBnW2+7XsBENTAAB4AOHGzWxWi67p2dqptgxNAQDgfoQbD8joGu9UO4amAABwP8KNBzA0BQCA7xBuPIChKQAAfIdw4yEMTQEA4BuEGw9haAoAAN8g3HgIQ1MAAPgG4caDGJoCAMD7CDcexNAUAADeR7jxIIamAADwPsKNhzE0BQCAdxFuPIyhKQAAvItw42EMTQEA4F0+DTcrVqzQsGHDlJiYKIvFogULFtTaftWqVerXr59atGihRo0aqUuXLnruuee8U2w9ODs0NX/dzwxNAQBQTz4NN2VlZUpJSdGsWbOcat+4cWONHTtWK1as0NatWzVx4kRNnDhRr7zyiocrrR9nh6ZKyys0c+kuL1QEAEDwshiG4RddBRaLRR999JGGDx9u6r5rr71WjRs31n/+8x+n2hcXFys6OlpFRUWKiopyoVLXPPbpT3pt9d462zWLbKD1Ey+TzWrxfFEAAAQIM7+/A3rOTVZWlr799ltdeumlZ21TXl6u4uLiai9fcHZoqvDoSSYWAwBQDwEZbtq0aaPw8HD17dtXY8aM0R133HHWttOmTVN0dLTjlZSU5MVK/ys1OUbNGtU9NCUxsRgAgPoIyHCzcuVKrVu3TnPmzNGMGTM0d+7cs7bNzMxUUVGR45WXl+fFSv/LZrXotn7tnGrLnjcAALguzNcFuCI5OVmSdP755+vAgQN65JFHdNNNN9XYNjw8XOHh4d4s76zGDuikV1buUVl5Za3tqva8Se/QwkuVAQAQPAKy5+Z0drtd5eXlvi7DKTarRTf2dW5YjKEpAABc49NwU1paquzsbGVnZ0uScnJylJ2drdzcXEmnhpRGjhzpaD9r1ix9+umn2rlzp3bu3KnXXntNzzzzjG655RZflO8SjmMAAMCzfDostW7dOvXv39/x5/Hjx0uSRo0apTfffFP5+fmOoCOd6qXJzMxUTk6OwsLC1KFDBz311FO6++67vV67q6r2vDlSdrLWdgxNAQDgGr/Z58ZbfLXPzemmfrJZb3y7r852z12fomt6t/FCRQAA+LeQ2ecmULVpHulUuyNlJzxcCQAAwYdw4wMxTZxbvfVz4TEPVwIAQPAh3PhAfFSEU+0+YVIxAACmEW58wNmDNKsmFQMAAOcRbnzAZrXomp6tnWrLfjcAAJhDuPER9rsBAMAzCDc+wtAUAACeQbjxEYamAADwDMKNDzk7NDV/3c8MTQEA4CTCjQ85OzRVWl6hmUt3eaEiAAACH+HGh8wMTb3xbQ69NwAAOIFw42PODk0VHj3JxGIAAJxAuPGx1OQYNWtU99CUxMRiAACcQbjxMZvVotv6tXOqLXveAABQN8KNHxg7oJMah9vqbMeeNwAA1I1w4wdsVotu7JvkVFuGpgAAqB3hxk+w5w0AAO5BuPET7HkDAIB7EG78BHveAADgHoQbP8KeNwAA1B/hxo+w5w0AAPVHuPEjZva8YWIxAAA1I9z4GWf3vGFiMQAANSPc+Bkze94wsRgAgDMRbvwQE4sBAHAd4cYPmZlYfLDkuIerAQAgsBBu/JDNatGoi85xqm3LxuEergYAgMBCuPFTqcktnGr3w16GpQAAOB3hxk8dLi13qt2ba/YyqRgAgNMQbvxUbNMIp9oxqRgAgOoIN36K3YoBAHAN4cZPsVsxAACuIdz4MXYrBgDAPMKNHzOzW/HLK3bTewMAgAg3fs/Z3YqPnqik9wYAABFu/J6ZicWcNQUAAOHG75mZWMyycAAACDcBYeyATopsWPfEYoll4QAAEG4CgM1q0d2XtHeqLcvCAQChjnATIFgWDgCAcwg3AYJl4QAAOIdwE0BYFg4AQN0INwGEZeEAANSNcBNAWBYOAEDdfBpuVqxYoWHDhikxMVEWi0ULFiyotf2HH36oyy67TK1atVJUVJTS09P15ZdfeqdYP8GycAAAaufTcFNWVqaUlBTNmjXLqfYrVqzQZZddpkWLFmn9+vXq37+/hg0bpqysLA9X6j9YFg4AQO0shmH4xW8/i8Wijz76SMOHDzd1X7du3XTDDTdo8uTJTrUvLi5WdHS0ioqKFBUV5UKlvldpN9Rj6pcqK6+ss+0DGedqXEYnL1QFAIDnmPn9HdBzbux2u0pKShQTE3PWNuXl5SouLq72CnRmloUzsRgAEGoCOtw888wzKi0t1fXXX3/WNtOmTVN0dLTjlZTkXCjwd84uC2diMQAg1ARsuHnnnXc0depUzZ8/X7GxsWdtl5mZqaKiIscrLy/Pi1V6TmpyjKIjwpxqW1B0zMPVAADgPwIy3MybN0933HGH5s+fr4yMjFrbhoeHKyoqqtorGNisFl3WNc6ptkfKTni4GgAA/EfAhZu5c+fqtttu09y5czV06FBfl+NT/Tq1cqrdz4X03AAAQodPw01paamys7OVnZ0tScrJyVF2drZyc3MlnRpSGjlypKP9O++8o5EjR+rZZ59VWlqaCgoKVFBQoKKiIl+U73PxURFOtXuPJeEAgBDi03Czbt069erVS7169ZIkjR8/Xr169XIs687Pz3cEHUl65ZVXVFFRoTFjxighIcHxGjdunE/q97XU5BjFNK77OAZOCgcAhBK/2efGW4Jhn5vTPfbpT3pt9d4620U2tGnTI4Nks1o8XxQAAG4WMvvcgJPCAQD4PcJNgDNzUvjLK3Yz9wYAEPQINwHOzEnh9N4AAEIB4SYImDkpnOMYAADBjnATBMycFM5xDACAYEe4CRJmem8WbynwcDUAAPgO4SZImOm9mc+mfgCAIEa4CSJjB3RS4/C6e2/Y1A8AEMwIN0HEZrXoxr5JTrVlWTgAIFgRboIMm/oBAEId4SbIsKkfACDUEW6CDJv6AQBCHeEmCLGpHwAglBFughCb+gEAQhnhJkixqR8AIFQRboIUm/oBAEIV4SaIsakfACAUEW6CmJlN/ZhYDAAIFqbDTV5enn7++WfHn9euXav7779fr7zyilsLg3s4u6kfE4sBAMHCdLgZMWKEli1bJkkqKCjQZZddprVr12rChAl69NFH3V4g6ic1OUbREWFOtS0oOubhagAA8DzT4Wbz5s1KTU2VJM2fP1/du3fXt99+q7fffltvvvmmu+tDPdmsFl3WNc6ptqt3HfZwNQAAeJ7pcHPy5EmFh4dLkr7++mtdddVVkqQuXbooPz/fvdXBLfp1auVUu0WbC5h3AwAIeKbDTbdu3TRnzhytXLlSixcv1hVXXCFJ2r9/v1q0aOH2AlF/8VERTrXjOAYAQDAwHW6eeuopvfzyy/rjH/+om266SSkpKZKkTz75xDFcBf9i5jBNVk0BAAKdczNNT/PHP/5Rhw8fVnFxsZo3b+64ftdddykyMtKtxcE9qg7TfO7rnXW2rVo1ld6BXjgAQGAy3XNz7NgxlZeXO4LNvn37NGPGDG3fvl2xsbFuLxDuwXEMAIBQYTrcXH311fr3v/8tSSosLFRaWpqeffZZDR8+XLNnz3Z7gXAPjmMAAIQK0+Fmw4YNuvjiiyVJ77//vuLi4rRv3z79+9//1gsvvOD2AuE+HMcAAAgFpsPN0aNH1bRpU0nSV199pWuvvVZWq1UXXnih9u3b5/YC4T5mjmN4ecVuem8AAAHJdLjp2LGjFixYoLy8PH355Ze6/PLLJUkHDx5UVFSU2wuEezl7HAPLwgEAgcp0uJk8ebIeeughtWvXTqmpqUpPT5d0qhenV69ebi8Q7mVmWTi9NwCAQGQ63PzpT39Sbm6u1q1bpy+//NJxfeDAgXruuefcWhzcr2pZuDPovQEABCKLYRgu/1/zqtPB27Rp47aCPK24uFjR0dEqKioK2WG0Sruh8x/5UkdPVNbZNrKhTZseGSSb1eKFygAAqJmZ39+me27sdrseffRRRUdH65xzztE555yjZs2a6bHHHpPdbne5aHiPmWXh9N4AAAKN6XAzYcIEzZw5U08++aSysrKUlZWlJ554Qi+++KImTZrkiRrhAWY29WPuDQAgkJgelkpMTNScOXMcp4FX+fjjj3Xvvffql19+cWuB7saw1H89//UOp45kkKQHMs7VuIxOHq4IAICaeXRY6siRI+rSpcsZ17t06aIjR46Y/Tj4EL03AIBgZDrcpKSkaObMmWdcnzlzpuOEcAQG5t4AAIKR6WGp5cuXa+jQoWrbtq1jj5s1a9YoLy9PixYtchzN4K8YlqrOzMqpZpENtH7iZaycAgB4nUeHpS699FLt2LFD11xzjQoLC1VYWKhrr71W27dv9/tggzOZ6b0pPHpSa3MYegQA+LcwV25KTEzU448/7u5a4CNjB3TSnOW7dexk3Uv5C4qOeaEiAABc51S42bhxo9Mf2KNHD5eLgW/YrBYNPT9B72+oe6Xb6l2HdU3vwNm0EQAQepwKNz179pTFYlFd03MsFosqK+ueuwH/069TK6fCzddbD6rSbjDvBgDgt5wKNzk5OZ6uAz4WHxXhVLvCY6fm3aR3aOHhigAAcI1TE4qrjllw5mXGihUrNGzYMCUmJspisWjBggW1ts/Pz9eIESN07rnnymq16v777zf183B2Zk4LX7ylwMPVAADgOtOrpdyprKxMKSkpmjVrllPty8vL1apVK02cOJE9ddzMzGnh89f9zIZ+AAC/5dJqKXcZPHiwBg8e7HT7du3a6fnnn5ckvf76654qK2SNHdBJr6zco7Ly2udNlZZXaObSXRzHAADwSz7tufGG8vJyFRcXV3uhZjarRTf2TXKqLccxAAD8VdCHm2nTpik6OtrxSkpy7pd3qMroGu9UO45jAAD4K5fCTWFhof75z38qMzPTcVjmhg0b/PJE8MzMTBUVFTleeXl5vi7Jr5mZWEzvDQDAH5kONxs3btS5556rp556Ss8884wKCwslSR9++KEyMzPdXV+9hYeHKyoqqtoLZ2dmYjG9NwAAf2Q63IwfP1633nqrdu7cqYiI/+6NMmTIEK1YscKtxcE3xg7opMiGNqfa0nsDAPA3psPNDz/8oLvvvvuM661bt1ZBgbn9T0pLS5Wdna3s7GxJpzYLzM7OVm5urqRTQ0ojR46sdk9V+9LSUh06dEjZ2dnasmWL2a+BWpg5TJPeGwCAvzEdbsLDw2tccbRjxw61atXK1GetW7dOvXr1Uq9evSSd6hXq1auXJk+eLOnUpn1VQadKVfv169frnXfeUa9evTRkyBCzXwN1oPcGABCoLEZdB0b9zh133KFff/1V8+fPV0xMjDZu3Cibzabhw4frkksu0YwZMzxUqnsUFxcrOjpaRUVFzL+pw/Nf79BzX+90qu0DGeey7w0AwGPM/P423XPz7LPPqrS0VLGxsTp27JguvfRSdezYUU2bNtXjjz/uctHwP/TeAAACkekdiqOjo7V48WKtWrVKGzduVGlpqXr37q2MjAxP1Acfqpp740zvTdXcG3pvAAC+ZnpYKtAxLGVOpd3Q+Y98qaMnaj+SQZIiG9q06ZFBslktXqgMABBKzPz+Nt1z88ILL9R43WKxKCIiQh07dtQll1wim8254Qz4N3pvAACBxnTPTXJysg4dOqSjR4+qefPmkqTffvtNkZGRatKkiQ4ePKj27dtr2bJlfnnUAT035pnpvWkW2UDrJ15G7w0AwK08OqH4iSee0AUXXKCdO3fq119/1a+//qodO3YoLS1Nzz//vHJzcxUfH68HHnjA5S8A/2Jm35vCoye1NueIhysCAODsTPfcdOjQQR988IF69uxZ7XpWVpauu+467dmzR99++62uu+465efnu7NWt6DnxjWVdkPdp3yhYyftdbZ97voUXdO7jReqAgCECo/23OTn56uiouKM6xUVFY4dihMTE1VSUmL2o+HHbFaLhp6f4FTb1bsOe7gaAADOznS46d+/v+6++25lZWU5rmVlZemee+7RgAEDJEmbNm1ScnKy+6qEX+jXybkdqBdtLmDPGwCAz5gON6+99ppiYmLUp08fhYeHKzw8XH379lVMTIxee+01SVKTJk307LPPur1Y+FZ8VETdjcR5UwAA33J5n5tt27Zpx44dkqTOnTurc+fObi3MU5hz47pKu6E+jy1W4bGTdbZlzxsAgDt5dJ+bKl26dFGXLl1cvR0ByGa16LZ+7djzBgDg11wKNz///LM++eQT5ebm6sSJE9Xemz59ulsKg38aO6CTXl6xx6k9b+Ys36WxAzrSewMA8CrT4WbJkiW66qqr1L59e23btk3du3fX3r17ZRiGevfu7Yka4UfM7Fh87KRd4+ZlaeYI/l4AALzH9ITizMxMPfTQQ9q0aZMiIiL0wQcfKC8vT5deeqn+/Oc/e6JG+Bkzp4V/tjFfizb6335HAIDgZTrcbN26VSNHjpQkhYWF6dixY2rSpIkeffRRPfXUU24vEP7HzI7FkvS3DzayNBwA4DWmw03jxo0d82wSEhK0e/dux3uHD7N5W6gw03tTWl7B0nAAgNeYDjcXXnihVq1aJUkaMmSIHnzwQT3++OO6/fbbdeGFF7q9QPgns703L6/YTe8NAMArTIeb6dOnKy0tTZI0depUDRw4UO+++67atWvn2MQPoWHsgE5qHO5c7w0b+wEAvMXUJn6VlZVavXq1evTooWbNmnmwLM9hEz/3WrRxv+59J6vuhmJjPwCA6zx2cKbNZtPll1+u3377rV4FIngM6ZGoYT3inWpL7w0AwBtMD0t1795de/bs8UQtCFAzbuzt9ORi5t4AADzNdLj5xz/+oYceekifffaZ8vPzVVxcXO2F0GNmcjG9NwAATzN9cKbV+t88ZLH8d+6EYRiyWCyqrKx7W35fYs6NZ1TaDZ3/yJdOHcvA3BsAgFkePThz2bJlLheG4GXmWAYO1QQAeJLpnptAR8+N55jpvWkW2UDrJ15G7w0AwCkeWy1VZeXKlbrlllt00UUX6ZdffpEk/ec//3Fs7ofQZGbuTeHRk1qbc8TDFQEAQpHpcPPBBx9o0KBBatSokTZs2KDy8nJJUlFRkZ544gm3F4jAMnZAJzVq4Nxfq69+4kBNAID7ubRaas6cOXr11VfVoEEDx/V+/fppw4YNbi0OgcdmtWjo+QlOtZ33Qx7LwgEAbmc63Gzfvl2XXHLJGdejo6NVWFjojpoQ4Pp1auVUu2Mn7SwLBwC4nelwEx8fr127zvyFtGrVKrVv7/xBighe8VERTreds3wXvTcAALcyHW7uvPNOjRs3Tt9//70sFov279+vt99+Ww899JDuueceT9SIAJOaHKOYxg3qbqhTvTfj5jl3NhUAAM4wvc/Nww8/LLvdroEDB+ro0aO65JJLFB4eroceekj33XefJ2pEgLFZLfrH1d2dPlDzs435GtI9X0N6ODdXBwCA2ri8z82JEye0a9culZaWqmvXrmrSpIm7a/MI9rnxnvveWa9PNxY41bZJeJh+nHI5+94AAGrk0X1u3nrrLR09elQNGzZU165dlZqaGjDBBt5l5kDN0vIKJhcDANzCdLh54IEHFBsbqxEjRmjRokV+f5YUfMfMpn4Sk4sBAO5hOtzk5+dr3rx5slgsuv7665WQkKAxY8bo22+/9UR9CHBjB3RS43Dnem+YXAwAcAfT4SYsLExXXnml3n77bR08eFDPPfec9u7dq/79+6tDhw6eqBEBzGa16Onrejjd/rON+Vq0kZ2LAQCuc+lsqSqRkZEaNGiQBg8erE6dOmnv3r1uKgvBZEiPRA3rEe90+/99/0eGpwAALnMp3Bw9elRvv/22hgwZotatW2vGjBm65ppr9NNPP7m7PgQJM5OLy05UMjwFAHCZ6XBz4403KjY2Vg888IDat2+vb775Rrt27dJjjz2mLl26eKJGBAGzk4sZngIAuMp0uLHZbJo/f77y8/M1c+ZMpaenO97bvHmzW4tDcDEzuViSJn28meEpAIBppsNN1XCUzXbql1RJSYleeeUVpaamKiUlxe0FIniYnVz8a9kJrc054sGKAADByOUJxStWrNCoUaOUkJCgZ555RgMGDNB3333nztoQhMxOLv7qJ4amAADmmAo3BQUFevLJJ9WpUyf9+c9/VlRUlMrLy7VgwQI9+eSTuuCCC0z98BUrVmjYsGFKTEyUxWLRggUL6rznm2++Ue/evRUeHq6OHTvqzTffNPUz4XszbuytiAbO/dWbuzaXoSkAgClOh5thw4apc+fO2rhxo2bMmKH9+/frxRdfrNcPLysrU0pKimbNmuVU+5ycHA0dOlT9+/dXdna27r//ft1xxx368ssv61UHvMtmteimC5Kcanu8wmDlFADAFKdPBf/888/1P//zP7rnnnvUqVMnt/zwwYMHa/DgwU63nzNnjpKTk/Xss89Kks477zytWrVKzz33nAYNGuSWmuAdl3dL0Bvf7nOqLaeGAwDMcLrnZtWqVSopKVGfPn2UlpammTNn6vDhw56s7Qxr1qxRRkZGtWuDBg3SmjVrznpPeXm5iouLq73ge6nJMYpp3MDp9mzsBwBwltPh5sILL9Srr76q/Px83X333Zo3b54SExNlt9u1ePFilZSUeLJOSafm/MTFxVW7FhcXp+LiYh07dqzGe6ZNm6bo6GjHKynJueEQeJbNatE/ru7udPuyE5X6n7kbPFgRACBYmF4t1bhxY91+++1atWqVNm3apAcffFBPPvmkYmNjddVVV3mixnrJzMxUUVGR45WXl+frkvD/mV05tXBTgR5fuMWDFQEAgkG9zpbq3Lmz/u///k8///yz5s6d666azio+Pl4HDhyodu3AgQOKiopSo0aNarwnPDxcUVFR1V7wHzNu7G1qY79XV+awczEAoFb1CjdVbDabhg8frk8++cQdH3dW6enpWrJkSbVrixcvrrZLMgKL2Y39JObfAABq55Zw46rS0lJlZ2crOztb0qml3tnZ2crNzZV0akhp5MiRjvajR4/Wnj179Le//U3btm3TSy+9pPnz5+uBBx7wRflwE7PDU8y/AQDUxqfhZt26derVq5d69eolSRo/frx69eqlyZMnS5Ly8/MdQUeSkpOTtXDhQi1evFgpKSl69tln9c9//pNl4EHA7PAU828AAGdjMQwjpPr3i4uLFR0draKiIubf+JlFG/fr3nfMbdj30oje7H8DACHAzO9vn/bcAKcb0iNRd17cztQ9f/tgI/NvAADVEG7gVyYM7aah5zs//6a0vEIzl+7yYEUAgEBDuIHfeeEmc/NvZi7bSe8NAMCBcAO/Y3Z5+MlKQ//zDqunAACnEG7gl8zOv1m4uUCPffaT5woCAAQMwg381oSh3dQzKdrp9q+t2svycAAA4Qb+7X8HdTHVnuMZAACEG/i1C9u3MDW5WOJ4BgAIdYQb+DVXzp7ieAYACG2EG/g9Vzb3W7iJCcYAEKoINwgIE4Z201//0M7UPa+t2kvAAYAQRLhBwJh0pWsBhxVUABBaCDcIKJOuNHc8g8QKKgAINYQbBByzxzNI0gPvZrGCCgBCBOEGAceVFVTlHNEAACGDcIOA5NIKKo5oAICQQLhBwGIFFQCgJoQbBLRJV3bTkO5xpu5hBRUABDfCDQLeiyP6KCLM3F9lVlABQPAi3CDg2awWTb8+xfR9nEEFAMGJcIOg4MoEY86gAoDgRLhB0HBlgvHCTQWa+ulmzxQEAPAJwg2CiitHNLyxep/++uZazxQEAPA6wg2CjisBZ8m2QwQcAAgShBsEJVfOoFqy7ZCmfsoeOAAQ6Ag3CFqunEH1xmo2+QOAQEe4QdBy5QwqiV2MASDQEW4Q1FxZIi4RcAAgkBFuEPQmDO2m2/qdY/o+Ag4ABCbCDULClGHdNbBLK9P3EXAAIPAQbhAyXrs1lYADACGAcIOQ8tqtqbqtXzvz9xFwACBgEG4QcqYMM7/Jn3Qq4Dy+cIv7CwIAuBXhBiHJlV2MJenVlTn6LHu/+wsCALgN4QYhy9WAM3ZeFgEHAPwY4QYhrT4B5/GFzMEBAH9EuEHIc32Iaq/ufWudKu2G+4sCALiMcAPI9YCzaPMBdZv8hRZtZJgKAPwF4Qb4/yZd2U1DuseZvu94hV33vsMwFQD4C8INcJoXR/RRRJhr/1m8upK9cADAHxBugNPYrBZNvz7F5fvZ7A8AfI9wA/zOkB6JuvuSZJfvJ+AAgG8RboAaZA7pqpdG9Fa4zeLS/a+tYiUVAPgK4QY4iyE9ErTlscEa2j3epftZSQUAvuEX4WbWrFlq166dIiIilJaWprVr15617cmTJ/Xoo4+qQ4cOioiIUEpKir744gsvVotQYrNaNOuWPrrzYteGqVhJBQDe5/Nw8+6772r8+PGaMmWKNmzYoJSUFA0aNEgHDx6ssf3EiRP18ssv68UXX9SWLVs0evRoXXPNNcrKyvJy5QglE4bWb5iKDf8AwHsshmH49H9t09LSdMEFF2jmzJmSJLvdrqSkJN133316+OGHz2ifmJioCRMmaMyYMY5r1113nRo1aqS33nqrzp9XXFys6OhoFRUVKSoqyn1fBCGh0m7o5lfW6Lu9v7l0f0SYVdOvT9GQHolurgwAgpuZ398+7bk5ceKE1q9fr4yMDMc1q9WqjIwMrVmzpsZ7ysvLFRERUe1ao0aNtGrVqrO2Ly4urvYCXGWzWvT2XemKbODafzoMUwGA5/k03Bw+fFiVlZWKi6u+K2xcXJwKCgpqvGfQoEGaPn26du7cKbvdrsWLF+vDDz9Ufn5+je2nTZum6OhoxyspKcnt3wOhxWa1aPoNPev1GWz4BwCe4/M5N2Y9//zz6tSpk7p06aKGDRtq7Nixuu2222S11vxVMjMzVVRU5Hjl5eV5uWIEoyu6J2jOLb3VyMUeHInl4gDgKT4NNy1btpTNZtOBAweqXT9w4IDi42teftuqVSstWLBAZWVl2rdvn7Zt26YmTZqoffv2NbYPDw9XVFRUtRfgDld0T9DmqVfoyvMTXP4MlosDgPv5NNw0bNhQffr00ZIlSxzX7Ha7lixZovT09FrvjYiIUOvWrVVRUaEPPvhAV199tafLBc5gs1o08+be9VpJVTUPZ8zb9OIAgDv4fFhq/PjxevXVV/Wvf/1LW7du1T333KOysjLddtttkqSRI0cqMzPT0f7777/Xhx9+qD179mjlypW64oorZLfb9be//c1XXwGo94Z/krRwE704AOAOYb4u4IYbbtChQ4c0efJkFRQUqGfPnvriiy8ck4xzc3Orzac5fvy4Jk6cqD179qhJkyYaMmSI/vOf/6hZs2Y++gbAKVUb/iUu3KJXV+a49BlVvTh35v2mCUO7ublCAAgNPt/nxtvY5wbe8Fn2fo2dV7+NJW/rd46mDOvupooAILAFzD43QLC6smeiXhrRu16f8cbqffrrm2c/igQAUDPCDeAhQ3rUf7n4km2HdO2sVUw0BgATCDeAB7ljufiGvCImGgOACYQbwMNYLg4A3kW4Abykarl4fXpxWC4OAHUj3ABeRC8OAHge4QbwgapenCHd4upufBYLNx3Q+VO+0Bebaz40FgBCFeEG8BGb1aKX/tJXA7u0cvkzjp60a/RbGximAoDTEG4AH3vt1tR6BRxJuvedLE3/cjvDVAAgwg3gF167NVV//UNyvT7jhWW71GXi55qxmJADILQRbgA/MenKrvWaaCxJJ+2GZiwh5AAIbZwtBfiZSruhcXOz9Nmm+k8UbmC1aEz/Drpv4LmyWV0PTQDga5wtBQQwdywXr1LVk9N10udMOgYQMgg3gJ+qWi4+tHt8vT+rvNJgbxwAIYNwA/gxm9WiWbf00V//0M4tn7dw0wHm4wAIeoQbIABMurKb7ry4fqupqjDpGECwY0IxEEAWbczX+PnZOl5hd9tn2izS8F6JmnZtihqG8f93APgnM7+/CTdAgKm0G3pxyU7NWrZLJ93c6zL0/Di9cFMfVlYB8DuEm1oQbhAsPBVybBbpvgEdWT4OwK8QbmpBuEGwqQo5M5fuVIUb/2u2WaTLusbpL+ntdGH7FgQdAD5FuKkF4QbByp2b//1eeJhF91zKZoAAfIdwUwvCDYKdJyYdVwmzSGMZsgLgA4SbWhBuEAo8OelYYsgKgPcRbmpBuEEo8XTIkRiyAuAdhJtaEG4QirwRcujNAeBJhJtaEG4QyqpCzotLd6rSg//lh9ssuueP9OYAcB/CTS0IN8CpkPP84h2atXyXKt0/79iB3hwA7kK4qQXhBvgvbwxXVWlglQaeR9AB4BrCTS0IN8CZqkLOnOW7PbKE/PdsVimjS6xGXpRM0AHgFMJNLQg3wNlV2g19t/tXPf3VNmXnFXnlZ4ZZpF7nNFdqcowu6tCSsAOgRoSbWhBuAOcs2pivv32wUaXlFV79uTaL1K11lIb1aK1RF7XjpHIAkgg3tSLcAM6r6sn593d7tXTbQZ305BKrs+jQKlJXdE+gVwcIcYSbWhBuANecHnS+3nrAo6uszoYhLCB0EW5qQbgB6s+bq6xqY7VIfc9ppvsGnKuLOrYk6ABBjHBTC8IN4D5VIWf28t0q98Iqq9pYJfVpR68OEKwIN7Ug3ADu5w9zc36PfXWA4EK4qQXhBvAsfww6EQ2seua6HrqyZ2tflwLARYSbWhBuAO/xxb45tendNlrvje5HLw4QgAg3tSDcAL5xosKuf32bo09/3K8t+SWq8NFE5AZWi56/saeG9Ej0yc8H4BrCTS0IN4DvVfXorN59SD/kHFH2z0VeH74aen6cXripD704QIAg3NSCcAP4n6qw8681OVq67ZDXenU4tRwIHISbWhBuAP/mq14d9swB/BvhphaEGyCw/D7sbMgr9PjuyOyZA/gfwk0tCDdAYKu0G/p252G9sHSHV4KOJNkkdYxrorYtIpXargUHegI+EHDhZtasWXr66adVUFCglJQUvfjii0pNTT1r+xkzZmj27NnKzc1Vy5Yt9ac//UnTpk1TREREnT+LcAMED19OTG7fspF6tGmu1s0b0bsDeEFAhZt3331XI0eO1Jw5c5SWlqYZM2bovffe0/bt2xUbG3tG+3feeUe33367Xn/9dV100UXasWOHbr31Vt14442aPn16nT+PcAMEr6qwM2HBJu399ahXf7ZNUofYxmoSEaZGDcKU0qaZ+nUi9ADuElDhJi0tTRdccIFmzpwpSbLb7UpKStJ9992nhx9++Iz2Y8eO1datW7VkyRLHtQcffFDff/+9Vq1aVefPI9wAoeHTH/frgXez5OMjrxRmkXq2babWzRrJYrHQ0wO4yMzv7zAv1VSjEydOaP369crMzHRcs1qtysjI0Jo1a2q856KLLtJbb72ltWvXKjU1VXv27NGiRYv0l7/8pcb25eXlKi8vd/y5uLjYvV8CgF8alpKoIecnaNzcLH22Kd9ndVQY0rp9hVq3r9Bxbday3bJIOjc2Up3jo2WxiOADuJFPw83hw4dVWVmpuLi4atfj4uK0bdu2Gu8ZMWKEDh8+rD/84Q8yDEMVFRUaPXq0/v73v9fYftq0aZo6darbawfg/2xWi2be3FtDNuZr/PxsHfd1N85pDEnbDx7V9oPVh89mLdtdbYgrIsymVk0j1CaG4AM4y6fhxhXffPONnnjiCb300ktKS0vTrl27NG7cOD322GOaNGnSGe0zMzM1fvx4x5+Li4uVlJTkzZIB+NiQHgka1D1eLy7ZqTnLd/tVyKlJpaQdB8vOuD5r2W7HMFdidIQOl57Q8YpKAhDwOz6dc3PixAlFRkbq/fff1/Dhwx3XR40apcLCQn388cdn3HPxxRfrwgsv1NNPP+249tZbb+muu+5SaWmprNbal2cy5wYIbaefWv711gNeWUrubafP8zEMgxCEoBAwc24aNmyoPn36aMmSJY5wY7fbtWTJEo0dO7bGe44ePXpGgLHZbJIkP1jVDsDP2awW9evUUv06tfTJnjneUNM8n9+rrReoZZNw5gEhoPl8WGr8+PEaNWqU+vbtq9TUVM2YMUNlZWW67bbbJEkjR45U69atNW3aNEnSsGHDNH36dPXq1csxLDVp0iQNGzbMEXIAwBk2q0UXd26lizu38ovDPL2tKgTVpSoIpSRFKzzMdkYIOh2BCP7A5+Hmhhtu0KFDhzR58mQVFBSoZ8+e+uKLLxyTjHNzc6v11EycOFEWi0UTJ07UL7/8olatWmnYsGF6/PHHffUVAASB03t0pOobBK7NOaLsvCKvHejpjyoMaX1ukdPtZy3bLatF6p0UrdbNI6u99/uhspZNwmW1EorgPj7f58bbmHMDwBWnh51ffjum/YXHQqJ3xxfqGi473elBic0Tg1tAbeLnbYQbAO5yeuD5+chRHS49ofzi48o7ciyke3n8QW2TqmsKSRI9Sv6OcFMLwg0AT/t9L48k7S88FlSTlkORs/OOpJqDEr1O9UO4qQXhBoCvVK3Oen9Dnn7+7ZjCw6xq2SRc+UUMceG/3NXrVFuYCsQJ4YSbWhBuAPijmoa4jldUqqy8UnsOlxF84HVnmxAu1R6UPBWQCDe1INwACDQ1DXOd/suFAAR/1SyygZ689nxd0T2h3p9FuKkF4QZAMKopAEmEIPiHObf0rnfACZgdigEA7vH7fXrOpq5eoKohBuYBwZ2mfrpFl3WN99ocHsINAIQQZ0OQdGYQqmtC6ub9Rdp96OjZPxAhK7/ouNbmHFF6hxZe+XmEGwBAjcwEoSonKuz617c5WptzREfLK9TCiVU7Jyrt+pFeoqB3sOS4134W4QYA4DYNw6y685IOuvOSDqbuc3a47GxBic0T/V9s0wiv/SzCDQDA51zpJfo9ZyZVm9krhh4l90mIjlBqcozXfh7hBgAQFNwRkH7P7LyjKvQ6VTdlWFevbghIuAEA4Cw8EZhq4u5eJ1d2KPbEhPDmkQ00zU373JhBuAEAwMe8FaLq4syEcMk3OxSbwSZ+AADA75n5/W31Uk0AAABeQbgBAABBhXADAACCCuEGAAAEFcINAAAIKoQbAAAQVAg3AAAgqBBuAABAUCHcAACAoBJyxy9UbchcXFzs40oAAICzqn5vO3OwQsiFm5KSEklSUlKSjysBAABmlZSUKDo6utY2IXe2lN1u1/79+9W0aVNZajoNzEXFxcVKSkpSXl4eZ1Z5EM/Ze3jW3sFz9g6es/d46lkbhqGSkhIlJibKaq19Vk3I9dxYrVa1adPGY58fFRXFfzhewHP2Hp61d/CcvYPn7D2eeNZ19dhUYUIxAAAIKoQbAAAQVAg3bhIeHq4pU6YoPDzc16UENZ6z9/CsvYPn7B08Z+/xh2cdchOKAQBAcKPnBgAABBXCDQAACCqEGwAAEFQINwAAIKgQbtxk1qxZateunSIiIpSWlqa1a9f6uqSAsmLFCg0bNkyJiYmyWCxasGBBtfcNw9DkyZOVkJCgRo0aKSMjQzt37qzW5siRI7r55psVFRWlZs2a6a9//atKS0u9+C3837Rp03TBBReoadOmio2N1fDhw7V9+/ZqbY4fP64xY8aoRYsWatKkia677jodOHCgWpvc3FwNHTpUkZGRio2N1f/+7/+qoqLCm1/Fr82ePVs9evRwbGKWnp6uzz//3PE+z9gznnzySVksFt1///2Oazxr93jkkUdksViqvbp06eJ43++es4F6mzdvntGwYUPj9ddfN3766SfjzjvvNJo1a2YcOHDA16UFjEWLFhkTJkwwPvzwQ0OS8dFHH1V7/8knnzSio6ONBQsWGD/++KNx1VVXGcnJycaxY8ccba644gojJSXF+O6774yVK1caHTt2NG666SYvfxP/NmjQIOONN94wNm/ebGRnZxtDhgwx2rZta5SWljrajB492khKSjKWLFlirFu3zrjwwguNiy66yPF+RUWF0b17dyMjI8PIysoyFi1aZLRs2dLIzMz0xVfyS5988omxcOFCY8eOHcb27duNv//970aDBg2MzZs3G4bBM/aEtWvXGu3atTN69OhhjBs3znGdZ+0eU6ZMMbp162bk5+c7XocOHXK872/PmXDjBqmpqcaYMWMcf66srDQSExONadOm+bCqwPX7cGO32434+Hjj6aefdlwrLCw0wsPDjblz5xqGYRhbtmwxJBk//PCDo83nn39uWCwW45dffvFa7YHm4MGDhiRj+fLlhmGceq4NGjQw3nvvPUebrVu3GpKMNWvWGIZxKoharVajoKDA0Wb27NlGVFSUUV5e7t0vEECaN29u/POf/+QZe0BJSYnRqVMnY/Hixcall17qCDc8a/eZMmWKkZKSUuN7/vicGZaqpxMnTmj9+vXKyMhwXLNarcrIyNCaNWt8WFnwyMnJUUFBQbVnHB0drbS0NMczXrNmjZo1a6a+ffs62mRkZMhqter777/3es2BoqioSJIUExMjSVq/fr1OnjxZ7Vl36dJFbdu2rfaszz//fMXFxTnaDBo0SMXFxfrpp5+8WH1gqKys1Lx581RWVqb09HSesQeMGTNGQ4cOrfZMJf4+u9vOnTuVmJio9u3b6+abb1Zubq4k/3zOIXdwprsdPnxYlZWV1f6FSVJcXJy2bdvmo6qCS0FBgSTV+Iyr3isoKFBsbGy198PCwhQTE+Nog+rsdrvuv/9+9evXT927d5d06jk2bNhQzZo1q9b298+6pn8XVe/hlE2bNik9PV3Hjx9XkyZN9NFHH6lr167Kzs7mGbvRvHnztGHDBv3www9nvMffZ/dJS0vTm2++qc6dOys/P19Tp07VxRdfrM2bN/vlcybcACFqzJgx2rx5s1atWuXrUoJS586dlZ2draKiIr3//vsaNWqUli9f7uuygkpeXp7GjRunxYsXKyIiwtflBLXBgwc7/rlHjx5KS0vTOeeco/nz56tRo0Y+rKxmDEvVU8uWLWWz2c6YFX7gwAHFx8f7qKrgUvUca3vG8fHxOnjwYLX3KyoqdOTIEf491GDs2LH67LPPtGzZMrVp08ZxPT4+XidOnFBhYWG19r9/1jX9u6h6D6c0bNhQHTt2VJ8+fTRt2jSlpKTo+eef5xm70fr163Xw4EH17t1bYWFhCgsL0/Lly/XCCy8oLCxMcXFxPGsPadasmc4991zt2rXLL/9OE27qqWHDhurTp4+WLFniuGa327VkyRKlp6f7sLLgkZycrPj4+GrPuLi4WN9//73jGaenp6uwsFDr1693tFm6dKnsdrvS0tK8XrO/MgxDY8eO1UcffaSlS5cqOTm52vt9+vRRgwYNqj3r7du3Kzc3t9qz3rRpU7UwuXjxYkVFRalr167e+SIByG63q7y8nGfsRgMHDtSmTZuUnZ3tePXt21c333yz45951p5RWlqq3bt3KyEhwT//Trt9inIImjdvnhEeHm68+eabxpYtW4y77rrLaNasWbVZ4ahdSUmJkZWVZWRlZRmSjOnTpxtZWVnGvn37DMM4tRS8WbNmxscff2xs3LjRuPrqq2tcCt6rVy/j+++/N1atWmV06tSJpeC/c8899xjR0dHGN998U21J59GjRx1tRo8ebbRt29ZYunSpsW7dOiM9Pd1IT093vF+1pPPyyy83srOzjS+++MJo1aoVS2dP8/DDDxvLly83cnJyjI0bNxoPP/ywYbFYjK+++sowDJ6xJ52+WsoweNbu8uCDDxrffPONkZOTY6xevdrIyMgwWrZsaRw8eNAwDP97zoQbN3nxxReNtm3bGg0bNjRSU1ON7777ztclBZRly5YZks54jRo1yjCMU8vBJ02aZMTFxRnh4eHGwIEDje3bt1f7jF9//dW46aabjCZNmhhRUVHGbbfdZpSUlPjg2/ivmp6xJOONN95wtDl27Jhx7733Gs2bNzciIyONa665xsjPz6/2OXv37jUGDx5sNGrUyGjZsqXx4IMPGidPnvTyt/Fft99+u3HOOecYDRs2NFq1amUMHDjQEWwMg2fsSb8PNzxr97jhhhuMhIQEo2HDhkbr1q2NG264wdi1a5fjfX97zhbDMAz39wcBAAD4BnNuAABAUCHcAACAoEK4AQAAQYVwAwAAggrhBgAABBXCDQAACCqEGwAAEFQINwAAIKgQbgBAksVi0YIFC3xdBgA3INwA8Llbb71VFovljNcVV1zh69IABKAwXxcAAJJ0xRVX6I033qh2LTw83EfVAAhk9NwA8Avh4eGKj4+v9mrevLmkU0NGs2fP1uDBg9WoUSO1b99e77//frX7N23apAEDBqhRo0Zq0aKF7rrrLpWWllZr8/rrr6tbt24KDw9XQkKCxo4dW+39w4cP65prrlFkZKQ6deqkTz75xLNfGoBHEG4ABIRJkybpuuuu048//qibb75ZN954o7Zu3SpJKisr06BBg9S8eXP98MMPeu+99/T1119XCy+zZ8/WmDFjdNddd2nTpk365JNP1LFjx2o/Y+rUqbr++uu1ceNGDRkyRDfffLOOHDni1e8JwA08ctY4AJgwatQow2azGY0bN672evzxxw3DMAxJxujRo6vdk5aWZtxzzz2GYRjGK6+8YjRv3twoLS11vL9w4ULDarUaBQUFhmEYRmJiojFhwoSz1iDJmDhxouPPpaWlhiTj888/d9v3BOAdzLkB4Bf69++v2bNnV7sWExPj+Of09PRq76Wnpys7O1uStHXrVqWkpKhx48aO9/v16ye73a7t27fLYrFo//79GjhwYK019OjRw/HPjRs3VlRUlA4ePOjqVwLgI4QbAH6hcePGZwwTuUujRo2categQYNqf7ZYLLLb7Z4oCYAHMecGQED47rvvzvjzeeedJ0k677zz9OOPP6qsrMzx/urVq2W1WtW5c2c1bdpU7dq105IlS7xaMwDfoOcGgF8oLy9XQUFBtWthYWFq2bKlJOm9995T37599Yc//EFvv/221q5dq9dee02SdPPNN2vKlCkaNWqUHnnkER06dEj33Xef/vKXvyguLk6S9Mgjj2j06NGKjY3V4MGDVVJSotWrV+u+++7z7hcF4HGEGwB+4YsvvlBCQkK1a507d9a2bdsknVrJNG/ePN17771KSEjQ3Llz1bVrV0lSZGSkvvzyS40bN04XXHCBIiMjdd1112n69OmOzxo1apSOHz+u5557Tg899JBatmypP/3pT977ggC8xmIYhuHrIgCgNhaLRR999JGGDx/u61IABADm3AAAgKBCuAEAAEGFOTcA/B6j5wDMoOcGAAAEFcINAAAIKoQbAAAQVAg3AAAgqBBuAABAUCHcAACAoEK4AQAAQYVwAwAAgsr/A8OuyDQIDtCTAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model\n",
    "trained_model = train_model(input_data, desired_output, 500, learning_rate=0.0005, viz_accuracy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(trained_model, new_data, expected_result):\n",
    "    test_input = torch.tensor(new_data, dtype=torch.float32)\n",
    "\n",
    "    test_output = trained_model(test_input)\n",
    "\n",
    "    predicted_classes = torch.argmax(test_output, dim=1)\n",
    "\n",
    "    test_output_compare = torch.tensor(expected_result, dtype=torch.int)\n",
    "\n",
    "    return float(sum(test_output_compare == predicted_classes)/len(test_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "just_input_test = test_df.drop('first_move_by_Q_learning', axis=1)\n",
    "just_input_test = just_input_test.drop('num_moves_Q_learning_needs', axis=1)\n",
    "\n",
    "input_data_test = np.array(just_input_test.values.tolist())\n",
    "desired_output = torch.tensor(test_df['first_move_by_Q_learning'].tolist(), dtype=torch.float32).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16089\\AppData\\Local\\Temp\\ipykernel_29400\\875939923.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_output_compare = torch.tensor(expected_result, dtype=torch.int)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9911110997200012"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(trained_model, input_data, torch.tensor(train['first_move_by_Q_learning'].tolist(), dtype=torch.float32).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16089\\AppData\\Local\\Temp\\ipykernel_29400\\875939923.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_output_compare = torch.tensor(expected_result, dtype=torch.int)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9833887219429016"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(trained_model, input_data_test, desired_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1, 2], [0, 1]])\n",
    "B = np.array([[1, 0], [2, 1]])\n",
    "\n",
    "# elements on the diagonal are 1 mod 4. \n",
    "# elements not on the diagonal are 0 mod 2. \n",
    "\n",
    "# C is the inverse of A\n",
    "# D is the inverse of B\n",
    "C = np.array([[1, -2], [0, 1]])\n",
    "D = np.array([[1, 0], [-2, 1]])\n",
    "\n",
    "\n",
    "identity = np.array([[1, 0], [0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how long does it take us to get back to the origin? \n",
    "def matrix_to_tuple(matrix):\n",
    "    return (matrix[0][0], matrix[0][1], matrix[0][1], matrix[1][1])\n",
    "\n",
    "def num_steps_to_origin_w_nn(test_mat):\n",
    "    for i in range(100):\n",
    "        if (test_mat==identity).all():\n",
    "            return i\n",
    "        demo_test_type = torch.tensor([matrix_to_tuple(test_mat)], dtype=torch.float32)\n",
    "        action = int(torch.argmax(trained_model(demo_test_type)))\n",
    "\n",
    "        if action==0:\n",
    "            test_mat = test_mat @ A\n",
    "        elif action==1:\n",
    "            test_mat = test_mat @ B\n",
    "        elif action==2:\n",
    "            test_mat = test_mat @ C\n",
    "        else:\n",
    "            test_mat = test_mat @ D\n",
    "\n",
    "    return 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_mat = np.array([[1, 1, 1], [0, 1, 1], [0, 0, 1]])\n",
    "# num_steps_to_origin_w_nn(A@A@A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_wrapper_for_num_steps(row):\n",
    "    arr = np.array([\n",
    "        [int(row['val1']), int(row['val2'])],\n",
    "        [int(row['val3']), int(row['val4'])]\n",
    "        ])\n",
    "    return num_steps_to_origin_w_nn(arr)\n",
    "\n",
    "test_df['num_steps_to_origin'] = test_df.apply(row_wrapper_for_num_steps, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val1</th>\n",
       "      <th>val2</th>\n",
       "      <th>val3</th>\n",
       "      <th>val4</th>\n",
       "      <th>num_moves_Q_learning_needs</th>\n",
       "      <th>first_move_by_Q_learning</th>\n",
       "      <th>num_steps_to_origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-7.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>-7.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>-3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>5.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>301 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     val1  val2  val3  val4  num_moves_Q_learning_needs  \\\n",
       "0     1.0  -4.0   0.0   1.0                           2   \n",
       "1     1.0  -2.0   0.0   1.0                           1   \n",
       "2    -7.0  24.0   2.0  -7.0                           5   \n",
       "3     1.0  -2.0  -2.0   5.0                           2   \n",
       "4     1.0  -2.0   0.0   1.0                           1   \n",
       "..    ...   ...   ...   ...                         ...   \n",
       "296  -7.0  -4.0  16.0   9.0                           4   \n",
       "297   1.0   0.0   0.0   1.0                           0   \n",
       "298  -3.0   2.0  -2.0   1.0                           2   \n",
       "299   5.0  -8.0   2.0  -3.0                           3   \n",
       "300   1.0  -4.0   0.0   1.0                           2   \n",
       "\n",
       "     first_move_by_Q_learning  num_steps_to_origin  \n",
       "0                           0                 1000  \n",
       "1                           0                 1000  \n",
       "2                           0                 1000  \n",
       "3                           0                 1000  \n",
       "4                           0                 1000  \n",
       "..                        ...                  ...  \n",
       "296                         3                 1000  \n",
       "297                         0                    0  \n",
       "298                         1                 1000  \n",
       "299                         0                 1000  \n",
       "300                         0                 1000  \n",
       "\n",
       "[301 rows x 7 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(test_df['num_steps_to_origin'] != 1000)\n",
    "\n",
    "just_a_few = test_df[test_df['num_steps_to_origin'] != 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val1</th>\n",
       "      <th>val2</th>\n",
       "      <th>val3</th>\n",
       "      <th>val4</th>\n",
       "      <th>num_moves_Q_learning_needs</th>\n",
       "      <th>first_move_by_Q_learning</th>\n",
       "      <th>num_steps_to_origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>-7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>-23.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>-35.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>-23.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>9.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>9.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     val1  val2  val3  val4  num_moves_Q_learning_needs  \\\n",
       "11    1.0   2.0   0.0   1.0                           1   \n",
       "24    1.0   0.0   0.0   1.0                           0   \n",
       "25    1.0   0.0   0.0   1.0                           0   \n",
       "43    1.0   0.0   0.0   1.0                           0   \n",
       "49    9.0   2.0   4.0   1.0                           3   \n",
       "56    1.0   0.0   0.0   1.0                           0   \n",
       "62    1.0   4.0   0.0   1.0                           2   \n",
       "63    1.0   2.0   0.0   1.0                           1   \n",
       "64   -7.0   4.0  -2.0   1.0                           3   \n",
       "70    1.0   6.0   0.0   1.0                           3   \n",
       "72    1.0   2.0   0.0   1.0                           1   \n",
       "75    1.0   8.0   0.0   1.0                           4   \n",
       "83    1.0   0.0   0.0   1.0                           0   \n",
       "89    1.0   4.0   0.0   1.0                           2   \n",
       "95  -23.0   4.0  -6.0   1.0                           5   \n",
       "113   1.0   6.0   0.0   1.0                           3   \n",
       "118   1.0   2.0   0.0   1.0                           1   \n",
       "130 -35.0   6.0  -6.0   1.0                           6   \n",
       "139   5.0   2.0   2.0   1.0                           2   \n",
       "149   9.0   2.0   4.0   1.0                           3   \n",
       "151 -23.0   6.0  -4.0   1.0                           5   \n",
       "155   1.0   0.0   0.0   1.0                           0   \n",
       "171   9.0  20.0   4.0   9.0                           4   \n",
       "177   1.0   0.0   0.0   1.0                           0   \n",
       "182   9.0  22.0   2.0   5.0                           4   \n",
       "183   1.0   2.0   0.0   1.0                           1   \n",
       "197   5.0  12.0   2.0   5.0                           3   \n",
       "199   5.0   2.0   2.0   1.0                           2   \n",
       "219   5.0   2.0   2.0   1.0                           2   \n",
       "235   1.0   0.0   0.0   1.0                           0   \n",
       "236   1.0   0.0   0.0   1.0                           0   \n",
       "253   1.0   2.0   0.0   1.0                           1   \n",
       "261   1.0   0.0   0.0   1.0                           0   \n",
       "262   5.0   2.0   2.0   1.0                           2   \n",
       "269   5.0   2.0   2.0   1.0                           2   \n",
       "270   1.0   0.0   0.0   1.0                           0   \n",
       "279   1.0   0.0   0.0   1.0                           0   \n",
       "282   5.0   2.0   2.0   1.0                           2   \n",
       "292   5.0   2.0   2.0   1.0                           2   \n",
       "294   1.0   0.0   0.0   1.0                           0   \n",
       "297   1.0   0.0   0.0   1.0                           0   \n",
       "\n",
       "     first_move_by_Q_learning  num_steps_to_origin  \n",
       "11                          2                    1  \n",
       "24                          0                    0  \n",
       "25                          0                    0  \n",
       "43                          0                    0  \n",
       "49                          3                    3  \n",
       "56                          0                    0  \n",
       "62                          2                    2  \n",
       "63                          2                    1  \n",
       "64                          1                    3  \n",
       "70                          2                    3  \n",
       "72                          2                    1  \n",
       "75                          2                    4  \n",
       "83                          0                    0  \n",
       "89                          2                    2  \n",
       "95                          1                    5  \n",
       "113                         2                    3  \n",
       "118                         2                    1  \n",
       "130                         1                    6  \n",
       "139                         3                    2  \n",
       "149                         3                    3  \n",
       "151                         1                    5  \n",
       "155                         0                    0  \n",
       "171                         2                    4  \n",
       "177                         0                    0  \n",
       "182                         2                    4  \n",
       "183                         2                    1  \n",
       "197                         2                    3  \n",
       "199                         3                    2  \n",
       "219                         3                    2  \n",
       "235                         0                    0  \n",
       "236                         0                    0  \n",
       "253                         2                    1  \n",
       "261                         0                    0  \n",
       "262                         3                    2  \n",
       "269                         3                    2  \n",
       "270                         0                    0  \n",
       "279                         0                    0  \n",
       "282                         3                    2  \n",
       "292                         3                    2  \n",
       "294                         0                    0  \n",
       "297                         0                    0  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "just_a_few"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_action(cur_matrix, action):\n",
    "    if action==0:\n",
    "        return cur_matrix @ A\n",
    "    elif action==1:\n",
    "        return cur_matrix @ B\n",
    "    elif action==2:\n",
    "        return cur_matrix @ C\n",
    "    elif action==3:\n",
    "        return cur_matrix @ D\n",
    "    assert 1==2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# going to get to the origin and track where we've been\n",
    "already_seen = set()\n",
    "start_matrix = np.array([[1, 5, -8], [0, 1, 1], [0, 0, 1]])\n",
    "already_seen.add(matrix_to_tuple(start_matrix))\n",
    "\n",
    "cur_matrix = start_matrix\n",
    "i=0\n",
    "\n",
    "while not (cur_matrix==identity).all():\n",
    "\n",
    "    if i>10000:\n",
    "        break\n",
    "\n",
    "    demo_test_type = torch.tensor([matrix_to_tuple(start_matrix)], dtype=torch.float32)\n",
    "    action = int(torch.argmax(trained_model(demo_test_type)))\n",
    "\n",
    "    new_mat = apply_action(cur_matrix, action)\n",
    "\n",
    "    # if we're about to cycle\n",
    "    if matrix_to_tuple(new_mat) in already_seen:\n",
    "        action = int(trained_model(demo_test_type).topk(2, largest=True)[1][0][-1])\n",
    "        new_mat = apply_action(new_mat, action)\n",
    "\n",
    "        if matrix_to_tuple(new_mat) in already_seen:\n",
    "            action = int(trained_model(demo_test_type).topk(3, largest=True)[1][0][-1])\n",
    "            new_mat = apply_action(new_mat, action)\n",
    "\n",
    "            if matrix_to_tuple(new_mat) in already_seen:\n",
    "                action = int(torch.argmax(trained_model(demo_test_type)))\n",
    "                new_mat = apply_action(new_mat, action)\n",
    "\n",
    "                if matrix_to_tuple(new_mat) in already_seen:\n",
    "                    action = random.randint(1, 4)\n",
    "                    new_mat = apply_action(new_mat, action)\n",
    "\n",
    "    \n",
    "    cur_matrix = new_mat\n",
    "    already_seen.add(matrix_to_tuple(cur_matrix))\n",
    "    i+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(mat):\n",
    "    demo_test_type = torch.tensor([matrix_to_tuple(mat)], dtype=torch.float32)\n",
    "    return int(torch.argmax(trained_model(demo_test_type)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
