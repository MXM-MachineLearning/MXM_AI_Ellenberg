{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from util import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code adapted from ChatGPT\n",
    "\n",
    "# Define your neural network model\n",
    "class SL2Z_NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SL2Z_NN, self).__init__()\n",
    "\n",
    "        self.step1 = nn.Linear(4, 128, bias=True)\n",
    "        self.step2 = nn.ReLU()\n",
    "        self.step3 = nn.Linear(128, 64, bias=True)\n",
    "        self.step4 = nn.ReLU()\n",
    "        self.step5 = nn.Linear(64, 16, bias=True)\n",
    "        self.step6 = nn.ReLU()\n",
    "        self.step7 = nn.Linear(16, 4, bias=True)\n",
    "\n",
    "        # multi-class classification adapted from ChatGPT\n",
    "        self.step8 = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # RUN IT ON A GPU if it exists\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.to(\"cuda\")\n",
    "\n",
    "        x = self.step1(x)\n",
    "        x = self.step2(x)\n",
    "        x = self.step3(x)\n",
    "        x = self.step4(x)\n",
    "        x = self.step5(x)\n",
    "        x = self.step6(x)\n",
    "        x = self.step7(x)\n",
    "        x = self.step8(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def train_model(model, inputs, desired_outputs, num_epochs=100, learning_rate=0.01, viz_accuracy=False):\n",
    "    # Create a DataLoader to handle batching (if needed)\n",
    "    dataset = TensorDataset(inputs, desired_outputs)\n",
    "    dataloader = DataLoader(dataset, batch_size=1000, shuffle=True)  # Adjust batch_size as needed\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()  # Mean Squared Error loss\n",
    "    # optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "    # criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.995)\n",
    "    losses = []\n",
    "\n",
    "    inputs = inputs.float()\n",
    "    desired_outputs = desired_outputs.long()\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for batch_inputs, batch_desired_outputs in dataloader:\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            outputs = model(batch_inputs)  # Forward pass\n",
    "\n",
    "            batch_desired_outputs = batch_desired_outputs.long()\n",
    "\n",
    "            loss = criterion(outputs, batch_desired_outputs)  # Compute the loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update the model's parameters\n",
    "            cur_item = loss.item()\n",
    "            total_loss += cur_item\n",
    "        \n",
    "        # Print the average loss for this epoch\n",
    "        scheduler.step()\n",
    "        print(f\"total loss: {total_loss}\")\n",
    "        average_loss = total_loss / len(dataloader)\n",
    "        if viz_accuracy:\n",
    "            losses.append(average_loss)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.4f}')\n",
    "    \n",
    "    if viz_accuracy:\n",
    "        plt.scatter(x=range(1, len(losses)+1), y=losses)\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Average loss\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Data_Generation/Data_files/train_rows_SL2Z_Q_learn.csv\")\n",
    "test_df = pd.read_csv(\"../Data_Generation/Data_files/test_rows_SL2Z_Q_learn.csv\")\n",
    "\n",
    "train = df.drop(\"num_moves_Q_learning_needs\", axis=1)\n",
    "just_input = train.drop('first_move_by_Q_learning', axis=1)\n",
    "\n",
    "# adapted from https://stackoverflow.com/questions/43898035/pandas-combine-column-values-into-a-list-in-a-new-column\n",
    "input_data = torch.tensor(just_input.values.tolist())\n",
    "desired_output = torch.tensor(train['first_move_by_Q_learning'].tolist(), dtype=torch.float32).long()\n",
    "\n",
    "nn_full = SL2Z_NN()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total loss: 1.3933751583099365\n",
      "Epoch [1/500], Loss: 1.3934\n",
      "total loss: 1.3864703178405762\n",
      "Epoch [2/500], Loss: 1.3865\n",
      "total loss: 1.3799066543579102\n",
      "Epoch [3/500], Loss: 1.3799\n",
      "total loss: 1.3738642930984497\n",
      "Epoch [4/500], Loss: 1.3739\n",
      "total loss: 1.3683652877807617\n",
      "Epoch [5/500], Loss: 1.3684\n",
      "total loss: 1.3632173538208008\n",
      "Epoch [6/500], Loss: 1.3632\n",
      "total loss: 1.358130931854248\n",
      "Epoch [7/500], Loss: 1.3581\n",
      "total loss: 1.3529621362686157\n",
      "Epoch [8/500], Loss: 1.3530\n",
      "total loss: 1.3476883172988892\n",
      "Epoch [9/500], Loss: 1.3477\n",
      "total loss: 1.342360019683838\n",
      "Epoch [10/500], Loss: 1.3424\n",
      "total loss: 1.3374078273773193\n",
      "Epoch [11/500], Loss: 1.3374\n",
      "total loss: 1.332899570465088\n",
      "Epoch [12/500], Loss: 1.3329\n",
      "total loss: 1.32855224609375\n",
      "Epoch [13/500], Loss: 1.3286\n",
      "total loss: 1.3241173028945923\n",
      "Epoch [14/500], Loss: 1.3241\n",
      "total loss: 1.3195573091506958\n",
      "Epoch [15/500], Loss: 1.3196\n",
      "total loss: 1.3150558471679688\n",
      "Epoch [16/500], Loss: 1.3151\n",
      "total loss: 1.3107185363769531\n",
      "Epoch [17/500], Loss: 1.3107\n",
      "total loss: 1.3064124584197998\n",
      "Epoch [18/500], Loss: 1.3064\n",
      "total loss: 1.3021490573883057\n",
      "Epoch [19/500], Loss: 1.3021\n",
      "total loss: 1.2980307340621948\n",
      "Epoch [20/500], Loss: 1.2980\n",
      "total loss: 1.2939951419830322\n",
      "Epoch [21/500], Loss: 1.2940\n",
      "total loss: 1.2899518013000488\n",
      "Epoch [22/500], Loss: 1.2900\n",
      "total loss: 1.2858043909072876\n",
      "Epoch [23/500], Loss: 1.2858\n",
      "total loss: 1.2815800905227661\n",
      "Epoch [24/500], Loss: 1.2816\n",
      "total loss: 1.2772188186645508\n",
      "Epoch [25/500], Loss: 1.2772\n",
      "total loss: 1.2726407051086426\n",
      "Epoch [26/500], Loss: 1.2726\n",
      "total loss: 1.267858862876892\n",
      "Epoch [27/500], Loss: 1.2679\n",
      "total loss: 1.2628992795944214\n",
      "Epoch [28/500], Loss: 1.2629\n",
      "total loss: 1.257896065711975\n",
      "Epoch [29/500], Loss: 1.2579\n",
      "total loss: 1.252938151359558\n",
      "Epoch [30/500], Loss: 1.2529\n",
      "total loss: 1.248160481452942\n",
      "Epoch [31/500], Loss: 1.2482\n",
      "total loss: 1.2435451745986938\n",
      "Epoch [32/500], Loss: 1.2435\n",
      "total loss: 1.2390155792236328\n",
      "Epoch [33/500], Loss: 1.2390\n",
      "total loss: 1.2344776391983032\n",
      "Epoch [34/500], Loss: 1.2345\n",
      "total loss: 1.2299236059188843\n",
      "Epoch [35/500], Loss: 1.2299\n",
      "total loss: 1.2253761291503906\n",
      "Epoch [36/500], Loss: 1.2254\n",
      "total loss: 1.2208638191223145\n",
      "Epoch [37/500], Loss: 1.2209\n",
      "total loss: 1.216411828994751\n",
      "Epoch [38/500], Loss: 1.2164\n",
      "total loss: 1.2119990587234497\n",
      "Epoch [39/500], Loss: 1.2120\n",
      "total loss: 1.2076630592346191\n",
      "Epoch [40/500], Loss: 1.2077\n",
      "total loss: 1.2034043073654175\n",
      "Epoch [41/500], Loss: 1.2034\n",
      "total loss: 1.1992120742797852\n",
      "Epoch [42/500], Loss: 1.1992\n",
      "total loss: 1.1950852870941162\n",
      "Epoch [43/500], Loss: 1.1951\n",
      "total loss: 1.1909784078598022\n",
      "Epoch [44/500], Loss: 1.1910\n",
      "total loss: 1.1868788003921509\n",
      "Epoch [45/500], Loss: 1.1869\n",
      "total loss: 1.1827867031097412\n",
      "Epoch [46/500], Loss: 1.1828\n",
      "total loss: 1.1787117719650269\n",
      "Epoch [47/500], Loss: 1.1787\n",
      "total loss: 1.1746219396591187\n",
      "Epoch [48/500], Loss: 1.1746\n",
      "total loss: 1.1704953908920288\n",
      "Epoch [49/500], Loss: 1.1705\n",
      "total loss: 1.1663590669631958\n",
      "Epoch [50/500], Loss: 1.1664\n",
      "total loss: 1.1622364521026611\n",
      "Epoch [51/500], Loss: 1.1622\n",
      "total loss: 1.1581227779388428\n",
      "Epoch [52/500], Loss: 1.1581\n",
      "total loss: 1.1540241241455078\n",
      "Epoch [53/500], Loss: 1.1540\n",
      "total loss: 1.149942398071289\n",
      "Epoch [54/500], Loss: 1.1499\n",
      "total loss: 1.1458559036254883\n",
      "Epoch [55/500], Loss: 1.1459\n",
      "total loss: 1.141777515411377\n",
      "Epoch [56/500], Loss: 1.1418\n",
      "total loss: 1.1377127170562744\n",
      "Epoch [57/500], Loss: 1.1377\n",
      "total loss: 1.1336580514907837\n",
      "Epoch [58/500], Loss: 1.1337\n",
      "total loss: 1.1296298503875732\n",
      "Epoch [59/500], Loss: 1.1296\n",
      "total loss: 1.1256065368652344\n",
      "Epoch [60/500], Loss: 1.1256\n",
      "total loss: 1.121622920036316\n",
      "Epoch [61/500], Loss: 1.1216\n",
      "total loss: 1.1176389455795288\n",
      "Epoch [62/500], Loss: 1.1176\n",
      "total loss: 1.1136584281921387\n",
      "Epoch [63/500], Loss: 1.1137\n",
      "total loss: 1.1096992492675781\n",
      "Epoch [64/500], Loss: 1.1097\n",
      "total loss: 1.1057825088500977\n",
      "Epoch [65/500], Loss: 1.1058\n",
      "total loss: 1.1019092798233032\n",
      "Epoch [66/500], Loss: 1.1019\n",
      "total loss: 1.0980706214904785\n",
      "Epoch [67/500], Loss: 1.0981\n",
      "total loss: 1.0942957401275635\n",
      "Epoch [68/500], Loss: 1.0943\n",
      "total loss: 1.0905617475509644\n",
      "Epoch [69/500], Loss: 1.0906\n",
      "total loss: 1.0868616104125977\n",
      "Epoch [70/500], Loss: 1.0869\n",
      "total loss: 1.0831912755966187\n",
      "Epoch [71/500], Loss: 1.0832\n",
      "total loss: 1.079537034034729\n",
      "Epoch [72/500], Loss: 1.0795\n",
      "total loss: 1.0759015083312988\n",
      "Epoch [73/500], Loss: 1.0759\n",
      "total loss: 1.0722733736038208\n",
      "Epoch [74/500], Loss: 1.0723\n",
      "total loss: 1.0686442852020264\n",
      "Epoch [75/500], Loss: 1.0686\n",
      "total loss: 1.0650018453598022\n",
      "Epoch [76/500], Loss: 1.0650\n",
      "total loss: 1.0613707304000854\n",
      "Epoch [77/500], Loss: 1.0614\n",
      "total loss: 1.0577821731567383\n",
      "Epoch [78/500], Loss: 1.0578\n",
      "total loss: 1.0542969703674316\n",
      "Epoch [79/500], Loss: 1.0543\n",
      "total loss: 1.0509239435195923\n",
      "Epoch [80/500], Loss: 1.0509\n",
      "total loss: 1.0476421117782593\n",
      "Epoch [81/500], Loss: 1.0476\n",
      "total loss: 1.0444087982177734\n",
      "Epoch [82/500], Loss: 1.0444\n",
      "total loss: 1.0412102937698364\n",
      "Epoch [83/500], Loss: 1.0412\n",
      "total loss: 1.0380383729934692\n",
      "Epoch [84/500], Loss: 1.0380\n",
      "total loss: 1.0348997116088867\n",
      "Epoch [85/500], Loss: 1.0349\n",
      "total loss: 1.0317986011505127\n",
      "Epoch [86/500], Loss: 1.0318\n",
      "total loss: 1.028732180595398\n",
      "Epoch [87/500], Loss: 1.0287\n",
      "total loss: 1.0257024765014648\n",
      "Epoch [88/500], Loss: 1.0257\n",
      "total loss: 1.0226900577545166\n",
      "Epoch [89/500], Loss: 1.0227\n",
      "total loss: 1.019705057144165\n",
      "Epoch [90/500], Loss: 1.0197\n",
      "total loss: 1.0167741775512695\n",
      "Epoch [91/500], Loss: 1.0168\n",
      "total loss: 1.013869285583496\n",
      "Epoch [92/500], Loss: 1.0139\n",
      "total loss: 1.0109643936157227\n",
      "Epoch [93/500], Loss: 1.0110\n",
      "total loss: 1.008049726486206\n",
      "Epoch [94/500], Loss: 1.0080\n",
      "total loss: 1.0051593780517578\n",
      "Epoch [95/500], Loss: 1.0052\n",
      "total loss: 1.002302885055542\n",
      "Epoch [96/500], Loss: 1.0023\n",
      "total loss: 0.9994727969169617\n",
      "Epoch [97/500], Loss: 0.9995\n",
      "total loss: 0.9966426491737366\n",
      "Epoch [98/500], Loss: 0.9966\n",
      "total loss: 0.9938287734985352\n",
      "Epoch [99/500], Loss: 0.9938\n",
      "total loss: 0.9910374879837036\n",
      "Epoch [100/500], Loss: 0.9910\n",
      "total loss: 0.9882779121398926\n",
      "Epoch [101/500], Loss: 0.9883\n",
      "total loss: 0.9855294227600098\n",
      "Epoch [102/500], Loss: 0.9855\n",
      "total loss: 0.9828048944473267\n",
      "Epoch [103/500], Loss: 0.9828\n",
      "total loss: 0.9800922274589539\n",
      "Epoch [104/500], Loss: 0.9801\n",
      "total loss: 0.9774116277694702\n",
      "Epoch [105/500], Loss: 0.9774\n",
      "total loss: 0.9747536182403564\n",
      "Epoch [106/500], Loss: 0.9748\n",
      "total loss: 0.9721366167068481\n",
      "Epoch [107/500], Loss: 0.9721\n",
      "total loss: 0.9695515036582947\n",
      "Epoch [108/500], Loss: 0.9696\n",
      "total loss: 0.9669978618621826\n",
      "Epoch [109/500], Loss: 0.9670\n",
      "total loss: 0.9644759297370911\n",
      "Epoch [110/500], Loss: 0.9645\n",
      "total loss: 0.9619871973991394\n",
      "Epoch [111/500], Loss: 0.9620\n",
      "total loss: 0.9595386981964111\n",
      "Epoch [112/500], Loss: 0.9595\n",
      "total loss: 0.9571163058280945\n",
      "Epoch [113/500], Loss: 0.9571\n",
      "total loss: 0.9547091126441956\n",
      "Epoch [114/500], Loss: 0.9547\n",
      "total loss: 0.9523311853408813\n",
      "Epoch [115/500], Loss: 0.9523\n",
      "total loss: 0.949998140335083\n",
      "Epoch [116/500], Loss: 0.9500\n",
      "total loss: 0.9476956129074097\n",
      "Epoch [117/500], Loss: 0.9477\n",
      "total loss: 0.9453862309455872\n",
      "Epoch [118/500], Loss: 0.9454\n",
      "total loss: 0.9430932998657227\n",
      "Epoch [119/500], Loss: 0.9431\n",
      "total loss: 0.9408354163169861\n",
      "Epoch [120/500], Loss: 0.9408\n",
      "total loss: 0.9386256337165833\n",
      "Epoch [121/500], Loss: 0.9386\n",
      "total loss: 0.9364259243011475\n",
      "Epoch [122/500], Loss: 0.9364\n",
      "total loss: 0.9342271089553833\n",
      "Epoch [123/500], Loss: 0.9342\n",
      "total loss: 0.9320363402366638\n",
      "Epoch [124/500], Loss: 0.9320\n",
      "total loss: 0.9298824667930603\n",
      "Epoch [125/500], Loss: 0.9299\n",
      "total loss: 0.9277682900428772\n",
      "Epoch [126/500], Loss: 0.9278\n",
      "total loss: 0.9256854057312012\n",
      "Epoch [127/500], Loss: 0.9257\n",
      "total loss: 0.9236407279968262\n",
      "Epoch [128/500], Loss: 0.9236\n",
      "total loss: 0.9216294288635254\n",
      "Epoch [129/500], Loss: 0.9216\n",
      "total loss: 0.9196460843086243\n",
      "Epoch [130/500], Loss: 0.9196\n",
      "total loss: 0.9176754355430603\n",
      "Epoch [131/500], Loss: 0.9177\n",
      "total loss: 0.9157310128211975\n",
      "Epoch [132/500], Loss: 0.9157\n",
      "total loss: 0.9138134121894836\n",
      "Epoch [133/500], Loss: 0.9138\n",
      "total loss: 0.9119294285774231\n",
      "Epoch [134/500], Loss: 0.9119\n",
      "total loss: 0.9100757241249084\n",
      "Epoch [135/500], Loss: 0.9101\n",
      "total loss: 0.9082526564598083\n",
      "Epoch [136/500], Loss: 0.9083\n",
      "total loss: 0.9064586758613586\n",
      "Epoch [137/500], Loss: 0.9065\n",
      "total loss: 0.9047003984451294\n",
      "Epoch [138/500], Loss: 0.9047\n",
      "total loss: 0.9029971361160278\n",
      "Epoch [139/500], Loss: 0.9030\n",
      "total loss: 0.9013246893882751\n",
      "Epoch [140/500], Loss: 0.9013\n",
      "total loss: 0.899671196937561\n",
      "Epoch [141/500], Loss: 0.8997\n",
      "total loss: 0.8980443477630615\n",
      "Epoch [142/500], Loss: 0.8980\n",
      "total loss: 0.8964468836784363\n",
      "Epoch [143/500], Loss: 0.8964\n",
      "total loss: 0.8948707580566406\n",
      "Epoch [144/500], Loss: 0.8949\n",
      "total loss: 0.8933297991752625\n",
      "Epoch [145/500], Loss: 0.8933\n",
      "total loss: 0.8918147683143616\n",
      "Epoch [146/500], Loss: 0.8918\n",
      "total loss: 0.8903335928916931\n",
      "Epoch [147/500], Loss: 0.8903\n",
      "total loss: 0.8888802528381348\n",
      "Epoch [148/500], Loss: 0.8889\n",
      "total loss: 0.8874506950378418\n",
      "Epoch [149/500], Loss: 0.8875\n",
      "total loss: 0.8860518336296082\n",
      "Epoch [150/500], Loss: 0.8861\n",
      "total loss: 0.88468337059021\n",
      "Epoch [151/500], Loss: 0.8847\n",
      "total loss: 0.8833327293395996\n",
      "Epoch [152/500], Loss: 0.8833\n",
      "total loss: 0.8820049166679382\n",
      "Epoch [153/500], Loss: 0.8820\n",
      "total loss: 0.8807007074356079\n",
      "Epoch [154/500], Loss: 0.8807\n",
      "total loss: 0.8794151544570923\n",
      "Epoch [155/500], Loss: 0.8794\n",
      "total loss: 0.8781452775001526\n",
      "Epoch [156/500], Loss: 0.8781\n",
      "total loss: 0.8768964409828186\n",
      "Epoch [157/500], Loss: 0.8769\n",
      "total loss: 0.8756607174873352\n",
      "Epoch [158/500], Loss: 0.8757\n",
      "total loss: 0.87444669008255\n",
      "Epoch [159/500], Loss: 0.8744\n",
      "total loss: 0.8732630014419556\n",
      "Epoch [160/500], Loss: 0.8733\n",
      "total loss: 0.8721005320549011\n",
      "Epoch [161/500], Loss: 0.8721\n",
      "total loss: 0.8709530234336853\n",
      "Epoch [162/500], Loss: 0.8710\n",
      "total loss: 0.8698254823684692\n",
      "Epoch [163/500], Loss: 0.8698\n",
      "total loss: 0.8687127828598022\n",
      "Epoch [164/500], Loss: 0.8687\n",
      "total loss: 0.8676137328147888\n",
      "Epoch [165/500], Loss: 0.8676\n",
      "total loss: 0.8665307760238647\n",
      "Epoch [166/500], Loss: 0.8665\n",
      "total loss: 0.8654650449752808\n",
      "Epoch [167/500], Loss: 0.8655\n",
      "total loss: 0.8644163608551025\n",
      "Epoch [168/500], Loss: 0.8644\n",
      "total loss: 0.8633861541748047\n",
      "Epoch [169/500], Loss: 0.8634\n",
      "total loss: 0.8623757362365723\n",
      "Epoch [170/500], Loss: 0.8624\n",
      "total loss: 0.8613877892494202\n",
      "Epoch [171/500], Loss: 0.8614\n",
      "total loss: 0.8604216575622559\n",
      "Epoch [172/500], Loss: 0.8604\n",
      "total loss: 0.8594664931297302\n",
      "Epoch [173/500], Loss: 0.8595\n",
      "total loss: 0.8585306406021118\n",
      "Epoch [174/500], Loss: 0.8585\n",
      "total loss: 0.8576135635375977\n",
      "Epoch [175/500], Loss: 0.8576\n",
      "total loss: 0.8567100167274475\n",
      "Epoch [176/500], Loss: 0.8567\n",
      "total loss: 0.8558158874511719\n",
      "Epoch [177/500], Loss: 0.8558\n",
      "total loss: 0.8549386858940125\n",
      "Epoch [178/500], Loss: 0.8549\n",
      "total loss: 0.8540748357772827\n",
      "Epoch [179/500], Loss: 0.8541\n",
      "total loss: 0.8532232046127319\n",
      "Epoch [180/500], Loss: 0.8532\n",
      "total loss: 0.852387011051178\n",
      "Epoch [181/500], Loss: 0.8524\n",
      "total loss: 0.8515628576278687\n",
      "Epoch [182/500], Loss: 0.8516\n",
      "total loss: 0.8507497906684875\n",
      "Epoch [183/500], Loss: 0.8507\n",
      "total loss: 0.8499518632888794\n",
      "Epoch [184/500], Loss: 0.8500\n",
      "total loss: 0.8491701483726501\n",
      "Epoch [185/500], Loss: 0.8492\n",
      "total loss: 0.8483972549438477\n",
      "Epoch [186/500], Loss: 0.8484\n",
      "total loss: 0.8476381301879883\n",
      "Epoch [187/500], Loss: 0.8476\n",
      "total loss: 0.846891462802887\n",
      "Epoch [188/500], Loss: 0.8469\n",
      "total loss: 0.8461552858352661\n",
      "Epoch [189/500], Loss: 0.8462\n",
      "total loss: 0.8454318046569824\n",
      "Epoch [190/500], Loss: 0.8454\n",
      "total loss: 0.8447257280349731\n",
      "Epoch [191/500], Loss: 0.8447\n",
      "total loss: 0.8440369367599487\n",
      "Epoch [192/500], Loss: 0.8440\n",
      "total loss: 0.8433595895767212\n",
      "Epoch [193/500], Loss: 0.8434\n",
      "total loss: 0.8426911234855652\n",
      "Epoch [194/500], Loss: 0.8427\n",
      "total loss: 0.8420318365097046\n",
      "Epoch [195/500], Loss: 0.8420\n",
      "total loss: 0.8413835763931274\n",
      "Epoch [196/500], Loss: 0.8414\n",
      "total loss: 0.8407431840896606\n",
      "Epoch [197/500], Loss: 0.8407\n",
      "total loss: 0.8401123881340027\n",
      "Epoch [198/500], Loss: 0.8401\n",
      "total loss: 0.8394944071769714\n",
      "Epoch [199/500], Loss: 0.8395\n",
      "total loss: 0.8388863205909729\n",
      "Epoch [200/500], Loss: 0.8389\n",
      "total loss: 0.8382871747016907\n",
      "Epoch [201/500], Loss: 0.8383\n",
      "total loss: 0.8376937508583069\n",
      "Epoch [202/500], Loss: 0.8377\n",
      "total loss: 0.8371114730834961\n",
      "Epoch [203/500], Loss: 0.8371\n",
      "total loss: 0.8365375399589539\n",
      "Epoch [204/500], Loss: 0.8365\n",
      "total loss: 0.835971474647522\n",
      "Epoch [205/500], Loss: 0.8360\n",
      "total loss: 0.8354153633117676\n",
      "Epoch [206/500], Loss: 0.8354\n",
      "total loss: 0.834864616394043\n",
      "Epoch [207/500], Loss: 0.8349\n",
      "total loss: 0.8343229293823242\n",
      "Epoch [208/500], Loss: 0.8343\n",
      "total loss: 0.8337950110435486\n",
      "Epoch [209/500], Loss: 0.8338\n",
      "total loss: 0.8332716822624207\n",
      "Epoch [210/500], Loss: 0.8333\n",
      "total loss: 0.8327563405036926\n",
      "Epoch [211/500], Loss: 0.8328\n",
      "total loss: 0.8322463035583496\n",
      "Epoch [212/500], Loss: 0.8322\n",
      "total loss: 0.8317450881004333\n",
      "Epoch [213/500], Loss: 0.8317\n",
      "total loss: 0.8312525749206543\n",
      "Epoch [214/500], Loss: 0.8313\n",
      "total loss: 0.8307644128799438\n",
      "Epoch [215/500], Loss: 0.8308\n",
      "total loss: 0.8302813172340393\n",
      "Epoch [216/500], Loss: 0.8303\n",
      "total loss: 0.8298044204711914\n",
      "Epoch [217/500], Loss: 0.8298\n",
      "total loss: 0.8293368816375732\n",
      "Epoch [218/500], Loss: 0.8293\n",
      "total loss: 0.8288741111755371\n",
      "Epoch [219/500], Loss: 0.8289\n",
      "total loss: 0.8284170627593994\n",
      "Epoch [220/500], Loss: 0.8284\n",
      "total loss: 0.8279669284820557\n",
      "Epoch [221/500], Loss: 0.8280\n",
      "total loss: 0.8275231122970581\n",
      "Epoch [222/500], Loss: 0.8275\n",
      "total loss: 0.8270882964134216\n",
      "Epoch [223/500], Loss: 0.8271\n",
      "total loss: 0.8266573548316956\n",
      "Epoch [224/500], Loss: 0.8267\n",
      "total loss: 0.826231837272644\n",
      "Epoch [225/500], Loss: 0.8262\n",
      "total loss: 0.8258123993873596\n",
      "Epoch [226/500], Loss: 0.8258\n",
      "total loss: 0.8253989219665527\n",
      "Epoch [227/500], Loss: 0.8254\n",
      "total loss: 0.8249891400337219\n",
      "Epoch [228/500], Loss: 0.8250\n",
      "total loss: 0.8245843052864075\n",
      "Epoch [229/500], Loss: 0.8246\n",
      "total loss: 0.8241854310035706\n",
      "Epoch [230/500], Loss: 0.8242\n",
      "total loss: 0.823789656162262\n",
      "Epoch [231/500], Loss: 0.8238\n",
      "total loss: 0.8233981728553772\n",
      "Epoch [232/500], Loss: 0.8234\n",
      "total loss: 0.8230109810829163\n",
      "Epoch [233/500], Loss: 0.8230\n",
      "total loss: 0.8226283192634583\n",
      "Epoch [234/500], Loss: 0.8226\n",
      "total loss: 0.8222503662109375\n",
      "Epoch [235/500], Loss: 0.8223\n",
      "total loss: 0.8218787312507629\n",
      "Epoch [236/500], Loss: 0.8219\n",
      "total loss: 0.8215109705924988\n",
      "Epoch [237/500], Loss: 0.8215\n",
      "total loss: 0.8211444020271301\n",
      "Epoch [238/500], Loss: 0.8211\n",
      "total loss: 0.8207838535308838\n",
      "Epoch [239/500], Loss: 0.8208\n",
      "total loss: 0.8204265236854553\n",
      "Epoch [240/500], Loss: 0.8204\n",
      "total loss: 0.8200731873512268\n",
      "Epoch [241/500], Loss: 0.8201\n",
      "total loss: 0.8197247385978699\n",
      "Epoch [242/500], Loss: 0.8197\n",
      "total loss: 0.8193811178207397\n",
      "Epoch [243/500], Loss: 0.8194\n",
      "total loss: 0.819040060043335\n",
      "Epoch [244/500], Loss: 0.8190\n",
      "total loss: 0.8187028169631958\n",
      "Epoch [245/500], Loss: 0.8187\n",
      "total loss: 0.8183706402778625\n",
      "Epoch [246/500], Loss: 0.8184\n",
      "total loss: 0.8180404901504517\n",
      "Epoch [247/500], Loss: 0.8180\n",
      "total loss: 0.8177136778831482\n",
      "Epoch [248/500], Loss: 0.8177\n",
      "total loss: 0.8173879384994507\n",
      "Epoch [249/500], Loss: 0.8174\n",
      "total loss: 0.8170679807662964\n",
      "Epoch [250/500], Loss: 0.8171\n",
      "total loss: 0.8167508840560913\n",
      "Epoch [251/500], Loss: 0.8168\n",
      "total loss: 0.8164352178573608\n",
      "Epoch [252/500], Loss: 0.8164\n",
      "total loss: 0.8161223530769348\n",
      "Epoch [253/500], Loss: 0.8161\n",
      "total loss: 0.8158150911331177\n",
      "Epoch [254/500], Loss: 0.8158\n",
      "total loss: 0.8155079483985901\n",
      "Epoch [255/500], Loss: 0.8155\n",
      "total loss: 0.8152050971984863\n",
      "Epoch [256/500], Loss: 0.8152\n",
      "total loss: 0.8149049878120422\n",
      "Epoch [257/500], Loss: 0.8149\n",
      "total loss: 0.8146056532859802\n",
      "Epoch [258/500], Loss: 0.8146\n",
      "total loss: 0.8143097758293152\n",
      "Epoch [259/500], Loss: 0.8143\n",
      "total loss: 0.8140152096748352\n",
      "Epoch [260/500], Loss: 0.8140\n",
      "total loss: 0.8137232065200806\n",
      "Epoch [261/500], Loss: 0.8137\n",
      "total loss: 0.8134317398071289\n",
      "Epoch [262/500], Loss: 0.8134\n",
      "total loss: 0.813143789768219\n",
      "Epoch [263/500], Loss: 0.8131\n",
      "total loss: 0.8128567337989807\n",
      "Epoch [264/500], Loss: 0.8129\n",
      "total loss: 0.8125723600387573\n",
      "Epoch [265/500], Loss: 0.8126\n",
      "total loss: 0.8122882843017578\n",
      "Epoch [266/500], Loss: 0.8123\n",
      "total loss: 0.8120047450065613\n",
      "Epoch [267/500], Loss: 0.8120\n",
      "total loss: 0.8117223381996155\n",
      "Epoch [268/500], Loss: 0.8117\n",
      "total loss: 0.8114399313926697\n",
      "Epoch [269/500], Loss: 0.8114\n",
      "total loss: 0.8111568093299866\n",
      "Epoch [270/500], Loss: 0.8112\n",
      "total loss: 0.8108728528022766\n",
      "Epoch [271/500], Loss: 0.8109\n",
      "total loss: 0.8105887770652771\n",
      "Epoch [272/500], Loss: 0.8106\n",
      "total loss: 0.8103022575378418\n",
      "Epoch [273/500], Loss: 0.8103\n",
      "total loss: 0.8100104331970215\n",
      "Epoch [274/500], Loss: 0.8100\n",
      "total loss: 0.8097096681594849\n",
      "Epoch [275/500], Loss: 0.8097\n",
      "total loss: 0.8093979358673096\n",
      "Epoch [276/500], Loss: 0.8094\n",
      "total loss: 0.809070348739624\n",
      "Epoch [277/500], Loss: 0.8091\n",
      "total loss: 0.8087195754051208\n",
      "Epoch [278/500], Loss: 0.8087\n",
      "total loss: 0.8083310723304749\n",
      "Epoch [279/500], Loss: 0.8083\n",
      "total loss: 0.8078781962394714\n",
      "Epoch [280/500], Loss: 0.8079\n",
      "total loss: 0.8073305487632751\n",
      "Epoch [281/500], Loss: 0.8073\n",
      "total loss: 0.8066641092300415\n",
      "Epoch [282/500], Loss: 0.8067\n",
      "total loss: 0.8058725595474243\n",
      "Epoch [283/500], Loss: 0.8059\n",
      "total loss: 0.8049773573875427\n",
      "Epoch [284/500], Loss: 0.8050\n",
      "total loss: 0.8039860725402832\n",
      "Epoch [285/500], Loss: 0.8040\n",
      "total loss: 0.8028587698936462\n",
      "Epoch [286/500], Loss: 0.8029\n",
      "total loss: 0.8016166687011719\n",
      "Epoch [287/500], Loss: 0.8016\n",
      "total loss: 0.8004333972930908\n",
      "Epoch [288/500], Loss: 0.8004\n",
      "total loss: 0.7994852066040039\n",
      "Epoch [289/500], Loss: 0.7995\n",
      "total loss: 0.7987866997718811\n",
      "Epoch [290/500], Loss: 0.7988\n",
      "total loss: 0.7982547283172607\n",
      "Epoch [291/500], Loss: 0.7983\n",
      "total loss: 0.797812283039093\n",
      "Epoch [292/500], Loss: 0.7978\n",
      "total loss: 0.7974202036857605\n",
      "Epoch [293/500], Loss: 0.7974\n",
      "total loss: 0.7970679998397827\n",
      "Epoch [294/500], Loss: 0.7971\n",
      "total loss: 0.7967368960380554\n",
      "Epoch [295/500], Loss: 0.7967\n",
      "total loss: 0.7964208126068115\n",
      "Epoch [296/500], Loss: 0.7964\n",
      "total loss: 0.7961169481277466\n",
      "Epoch [297/500], Loss: 0.7961\n",
      "total loss: 0.7958216071128845\n",
      "Epoch [298/500], Loss: 0.7958\n",
      "total loss: 0.7955348491668701\n",
      "Epoch [299/500], Loss: 0.7955\n",
      "total loss: 0.795256495475769\n",
      "Epoch [300/500], Loss: 0.7953\n",
      "total loss: 0.7949860692024231\n",
      "Epoch [301/500], Loss: 0.7950\n",
      "total loss: 0.7947220206260681\n",
      "Epoch [302/500], Loss: 0.7947\n",
      "total loss: 0.7944650053977966\n",
      "Epoch [303/500], Loss: 0.7945\n",
      "total loss: 0.7942156195640564\n",
      "Epoch [304/500], Loss: 0.7942\n",
      "total loss: 0.7939700484275818\n",
      "Epoch [305/500], Loss: 0.7940\n",
      "total loss: 0.7937290668487549\n",
      "Epoch [306/500], Loss: 0.7937\n",
      "total loss: 0.7934918403625488\n",
      "Epoch [307/500], Loss: 0.7935\n",
      "total loss: 0.7932575941085815\n",
      "Epoch [308/500], Loss: 0.7933\n",
      "total loss: 0.7930259108543396\n",
      "Epoch [309/500], Loss: 0.7930\n",
      "total loss: 0.7927976250648499\n",
      "Epoch [310/500], Loss: 0.7928\n",
      "total loss: 0.7925729155540466\n",
      "Epoch [311/500], Loss: 0.7926\n",
      "total loss: 0.7923495173454285\n",
      "Epoch [312/500], Loss: 0.7923\n",
      "total loss: 0.7921294569969177\n",
      "Epoch [313/500], Loss: 0.7921\n",
      "total loss: 0.7919107675552368\n",
      "Epoch [314/500], Loss: 0.7919\n",
      "total loss: 0.791692852973938\n",
      "Epoch [315/500], Loss: 0.7917\n",
      "total loss: 0.7914772629737854\n",
      "Epoch [316/500], Loss: 0.7915\n",
      "total loss: 0.7912668585777283\n",
      "Epoch [317/500], Loss: 0.7913\n",
      "total loss: 0.7910600900650024\n",
      "Epoch [318/500], Loss: 0.7911\n",
      "total loss: 0.790856122970581\n",
      "Epoch [319/500], Loss: 0.7909\n",
      "total loss: 0.7906539440155029\n",
      "Epoch [320/500], Loss: 0.7907\n",
      "total loss: 0.7904536128044128\n",
      "Epoch [321/500], Loss: 0.7905\n",
      "total loss: 0.7902564406394958\n",
      "Epoch [322/500], Loss: 0.7903\n",
      "total loss: 0.7900610566139221\n",
      "Epoch [323/500], Loss: 0.7901\n",
      "total loss: 0.7898680567741394\n",
      "Epoch [324/500], Loss: 0.7899\n",
      "total loss: 0.7896761894226074\n",
      "Epoch [325/500], Loss: 0.7897\n",
      "total loss: 0.78948575258255\n",
      "Epoch [326/500], Loss: 0.7895\n",
      "total loss: 0.7892969846725464\n",
      "Epoch [327/500], Loss: 0.7893\n",
      "total loss: 0.7891111969947815\n",
      "Epoch [328/500], Loss: 0.7891\n",
      "total loss: 0.7889289855957031\n",
      "Epoch [329/500], Loss: 0.7889\n",
      "total loss: 0.7887498140335083\n",
      "Epoch [330/500], Loss: 0.7887\n",
      "total loss: 0.7885716557502747\n",
      "Epoch [331/500], Loss: 0.7886\n",
      "total loss: 0.7883937358856201\n",
      "Epoch [332/500], Loss: 0.7884\n",
      "total loss: 0.7882179617881775\n",
      "Epoch [333/500], Loss: 0.7882\n",
      "total loss: 0.7880440354347229\n",
      "Epoch [334/500], Loss: 0.7880\n",
      "total loss: 0.7878718972206116\n",
      "Epoch [335/500], Loss: 0.7879\n",
      "total loss: 0.7877023220062256\n",
      "Epoch [336/500], Loss: 0.7877\n",
      "total loss: 0.7875346541404724\n",
      "Epoch [337/500], Loss: 0.7875\n",
      "total loss: 0.7873674035072327\n",
      "Epoch [338/500], Loss: 0.7874\n",
      "total loss: 0.7872015833854675\n",
      "Epoch [339/500], Loss: 0.7872\n",
      "total loss: 0.7870358228683472\n",
      "Epoch [340/500], Loss: 0.7870\n",
      "total loss: 0.7868710160255432\n",
      "Epoch [341/500], Loss: 0.7869\n",
      "total loss: 0.7867059707641602\n",
      "Epoch [342/500], Loss: 0.7867\n",
      "total loss: 0.7865405678749084\n",
      "Epoch [343/500], Loss: 0.7865\n",
      "total loss: 0.7863756418228149\n",
      "Epoch [344/500], Loss: 0.7864\n",
      "total loss: 0.786211371421814\n",
      "Epoch [345/500], Loss: 0.7862\n",
      "total loss: 0.7860479354858398\n",
      "Epoch [346/500], Loss: 0.7860\n",
      "total loss: 0.7858846783638\n",
      "Epoch [347/500], Loss: 0.7859\n",
      "total loss: 0.7857208251953125\n",
      "Epoch [348/500], Loss: 0.7857\n",
      "total loss: 0.7855552434921265\n",
      "Epoch [349/500], Loss: 0.7856\n",
      "total loss: 0.785388708114624\n",
      "Epoch [350/500], Loss: 0.7854\n",
      "total loss: 0.785221517086029\n",
      "Epoch [351/500], Loss: 0.7852\n",
      "total loss: 0.7850525975227356\n",
      "Epoch [352/500], Loss: 0.7851\n",
      "total loss: 0.7848811149597168\n",
      "Epoch [353/500], Loss: 0.7849\n",
      "total loss: 0.784707248210907\n",
      "Epoch [354/500], Loss: 0.7847\n",
      "total loss: 0.7845314741134644\n",
      "Epoch [355/500], Loss: 0.7845\n",
      "total loss: 0.7843519449234009\n",
      "Epoch [356/500], Loss: 0.7844\n",
      "total loss: 0.7841687798500061\n",
      "Epoch [357/500], Loss: 0.7842\n",
      "total loss: 0.783980667591095\n",
      "Epoch [358/500], Loss: 0.7840\n",
      "total loss: 0.7837855219841003\n",
      "Epoch [359/500], Loss: 0.7838\n",
      "total loss: 0.7835835814476013\n",
      "Epoch [360/500], Loss: 0.7836\n",
      "total loss: 0.7833720445632935\n",
      "Epoch [361/500], Loss: 0.7834\n",
      "total loss: 0.7831498384475708\n",
      "Epoch [362/500], Loss: 0.7831\n",
      "total loss: 0.7829166054725647\n",
      "Epoch [363/500], Loss: 0.7829\n",
      "total loss: 0.7826707363128662\n",
      "Epoch [364/500], Loss: 0.7827\n",
      "total loss: 0.7824088931083679\n",
      "Epoch [365/500], Loss: 0.7824\n",
      "total loss: 0.7821301221847534\n",
      "Epoch [366/500], Loss: 0.7821\n",
      "total loss: 0.7818341851234436\n",
      "Epoch [367/500], Loss: 0.7818\n",
      "total loss: 0.7815207839012146\n",
      "Epoch [368/500], Loss: 0.7815\n",
      "total loss: 0.781191885471344\n",
      "Epoch [369/500], Loss: 0.7812\n",
      "total loss: 0.780848503112793\n",
      "Epoch [370/500], Loss: 0.7808\n",
      "total loss: 0.7804955244064331\n",
      "Epoch [371/500], Loss: 0.7805\n",
      "total loss: 0.7801352739334106\n",
      "Epoch [372/500], Loss: 0.7801\n",
      "total loss: 0.7797748446464539\n",
      "Epoch [373/500], Loss: 0.7798\n",
      "total loss: 0.7794179320335388\n",
      "Epoch [374/500], Loss: 0.7794\n",
      "total loss: 0.7790689468383789\n",
      "Epoch [375/500], Loss: 0.7791\n",
      "total loss: 0.7787327766418457\n",
      "Epoch [376/500], Loss: 0.7787\n",
      "total loss: 0.7784115076065063\n",
      "Epoch [377/500], Loss: 0.7784\n",
      "total loss: 0.7781063318252563\n",
      "Epoch [378/500], Loss: 0.7781\n",
      "total loss: 0.7778187990188599\n",
      "Epoch [379/500], Loss: 0.7778\n",
      "total loss: 0.7775478959083557\n",
      "Epoch [380/500], Loss: 0.7775\n",
      "total loss: 0.7772940993309021\n",
      "Epoch [381/500], Loss: 0.7773\n",
      "total loss: 0.7770592570304871\n",
      "Epoch [382/500], Loss: 0.7771\n",
      "total loss: 0.7768408060073853\n",
      "Epoch [383/500], Loss: 0.7768\n",
      "total loss: 0.776636004447937\n",
      "Epoch [384/500], Loss: 0.7766\n",
      "total loss: 0.776443362236023\n",
      "Epoch [385/500], Loss: 0.7764\n",
      "total loss: 0.7762622833251953\n",
      "Epoch [386/500], Loss: 0.7763\n",
      "total loss: 0.7760891914367676\n",
      "Epoch [387/500], Loss: 0.7761\n",
      "total loss: 0.7759258151054382\n",
      "Epoch [388/500], Loss: 0.7759\n",
      "total loss: 0.7757701277732849\n",
      "Epoch [389/500], Loss: 0.7758\n",
      "total loss: 0.7756195068359375\n",
      "Epoch [390/500], Loss: 0.7756\n",
      "total loss: 0.7754735946655273\n",
      "Epoch [391/500], Loss: 0.7755\n",
      "total loss: 0.7753314971923828\n",
      "Epoch [392/500], Loss: 0.7753\n",
      "total loss: 0.7751925587654114\n",
      "Epoch [393/500], Loss: 0.7752\n",
      "total loss: 0.7750568389892578\n",
      "Epoch [394/500], Loss: 0.7751\n",
      "total loss: 0.7749244570732117\n",
      "Epoch [395/500], Loss: 0.7749\n",
      "total loss: 0.774795413017273\n",
      "Epoch [396/500], Loss: 0.7748\n",
      "total loss: 0.774669349193573\n",
      "Epoch [397/500], Loss: 0.7747\n",
      "total loss: 0.7745449542999268\n",
      "Epoch [398/500], Loss: 0.7745\n",
      "total loss: 0.7744224071502686\n",
      "Epoch [399/500], Loss: 0.7744\n",
      "total loss: 0.7743015289306641\n",
      "Epoch [400/500], Loss: 0.7743\n",
      "total loss: 0.7741817235946655\n",
      "Epoch [401/500], Loss: 0.7742\n",
      "total loss: 0.774063229560852\n",
      "Epoch [402/500], Loss: 0.7741\n",
      "total loss: 0.773947536945343\n",
      "Epoch [403/500], Loss: 0.7739\n",
      "total loss: 0.7738333344459534\n",
      "Epoch [404/500], Loss: 0.7738\n",
      "total loss: 0.7737206220626831\n",
      "Epoch [405/500], Loss: 0.7737\n",
      "total loss: 0.7736089825630188\n",
      "Epoch [406/500], Loss: 0.7736\n",
      "total loss: 0.7734986543655396\n",
      "Epoch [407/500], Loss: 0.7735\n",
      "total loss: 0.7733903527259827\n",
      "Epoch [408/500], Loss: 0.7734\n",
      "total loss: 0.7732840776443481\n",
      "Epoch [409/500], Loss: 0.7733\n",
      "total loss: 0.7731813788414001\n",
      "Epoch [410/500], Loss: 0.7732\n",
      "total loss: 0.7730792760848999\n",
      "Epoch [411/500], Loss: 0.7731\n",
      "total loss: 0.7729782462120056\n",
      "Epoch [412/500], Loss: 0.7730\n",
      "total loss: 0.7728787660598755\n",
      "Epoch [413/500], Loss: 0.7729\n",
      "total loss: 0.7727797627449036\n",
      "Epoch [414/500], Loss: 0.7728\n",
      "total loss: 0.7726815342903137\n",
      "Epoch [415/500], Loss: 0.7727\n",
      "total loss: 0.7725845575332642\n",
      "Epoch [416/500], Loss: 0.7726\n",
      "total loss: 0.772488534450531\n",
      "Epoch [417/500], Loss: 0.7725\n",
      "total loss: 0.7723937034606934\n",
      "Epoch [418/500], Loss: 0.7724\n",
      "total loss: 0.7722994089126587\n",
      "Epoch [419/500], Loss: 0.7723\n",
      "total loss: 0.7722064852714539\n",
      "Epoch [420/500], Loss: 0.7722\n",
      "total loss: 0.7721166610717773\n",
      "Epoch [421/500], Loss: 0.7721\n",
      "total loss: 0.7720277309417725\n",
      "Epoch [422/500], Loss: 0.7720\n",
      "total loss: 0.7719392776489258\n",
      "Epoch [423/500], Loss: 0.7719\n",
      "total loss: 0.7718512415885925\n",
      "Epoch [424/500], Loss: 0.7719\n",
      "total loss: 0.7717640995979309\n",
      "Epoch [425/500], Loss: 0.7718\n",
      "total loss: 0.7716776132583618\n",
      "Epoch [426/500], Loss: 0.7717\n",
      "total loss: 0.77159184217453\n",
      "Epoch [427/500], Loss: 0.7716\n",
      "total loss: 0.7715068459510803\n",
      "Epoch [428/500], Loss: 0.7715\n",
      "total loss: 0.7714223861694336\n",
      "Epoch [429/500], Loss: 0.7714\n",
      "total loss: 0.7713395953178406\n",
      "Epoch [430/500], Loss: 0.7713\n",
      "total loss: 0.7712569832801819\n",
      "Epoch [431/500], Loss: 0.7713\n",
      "total loss: 0.7711760401725769\n",
      "Epoch [432/500], Loss: 0.7712\n",
      "total loss: 0.7710962295532227\n",
      "Epoch [433/500], Loss: 0.7711\n",
      "total loss: 0.7710164785385132\n",
      "Epoch [434/500], Loss: 0.7710\n",
      "total loss: 0.7709376215934753\n",
      "Epoch [435/500], Loss: 0.7709\n",
      "total loss: 0.7708591818809509\n",
      "Epoch [436/500], Loss: 0.7709\n",
      "total loss: 0.770781934261322\n",
      "Epoch [437/500], Loss: 0.7708\n",
      "total loss: 0.7707055807113647\n",
      "Epoch [438/500], Loss: 0.7707\n",
      "total loss: 0.7706297039985657\n",
      "Epoch [439/500], Loss: 0.7706\n",
      "total loss: 0.7705540657043457\n",
      "Epoch [440/500], Loss: 0.7706\n",
      "total loss: 0.7704789638519287\n",
      "Epoch [441/500], Loss: 0.7705\n",
      "total loss: 0.7704049348831177\n",
      "Epoch [442/500], Loss: 0.7704\n",
      "total loss: 0.7703316807746887\n",
      "Epoch [443/500], Loss: 0.7703\n",
      "total loss: 0.7702586650848389\n",
      "Epoch [444/500], Loss: 0.7703\n",
      "total loss: 0.7701863646507263\n",
      "Epoch [445/500], Loss: 0.7702\n",
      "total loss: 0.7701154351234436\n",
      "Epoch [446/500], Loss: 0.7701\n",
      "total loss: 0.7700452208518982\n",
      "Epoch [447/500], Loss: 0.7700\n",
      "total loss: 0.7699742913246155\n",
      "Epoch [448/500], Loss: 0.7700\n",
      "total loss: 0.7699047327041626\n",
      "Epoch [449/500], Loss: 0.7699\n",
      "total loss: 0.7698357701301575\n",
      "Epoch [450/500], Loss: 0.7698\n",
      "total loss: 0.7697678208351135\n",
      "Epoch [451/500], Loss: 0.7698\n",
      "total loss: 0.7697007060050964\n",
      "Epoch [452/500], Loss: 0.7697\n",
      "total loss: 0.7696340084075928\n",
      "Epoch [453/500], Loss: 0.7696\n",
      "total loss: 0.769567608833313\n",
      "Epoch [454/500], Loss: 0.7696\n",
      "total loss: 0.7695015072822571\n",
      "Epoch [455/500], Loss: 0.7695\n",
      "total loss: 0.7694361805915833\n",
      "Epoch [456/500], Loss: 0.7694\n",
      "total loss: 0.7693711519241333\n",
      "Epoch [457/500], Loss: 0.7694\n",
      "total loss: 0.7693064212799072\n",
      "Epoch [458/500], Loss: 0.7693\n",
      "total loss: 0.7692428231239319\n",
      "Epoch [459/500], Loss: 0.7692\n",
      "total loss: 0.7691795229911804\n",
      "Epoch [460/500], Loss: 0.7692\n",
      "total loss: 0.7691166400909424\n",
      "Epoch [461/500], Loss: 0.7691\n",
      "total loss: 0.7690539956092834\n",
      "Epoch [462/500], Loss: 0.7691\n",
      "total loss: 0.7689916491508484\n",
      "Epoch [463/500], Loss: 0.7690\n",
      "total loss: 0.7689303755760193\n",
      "Epoch [464/500], Loss: 0.7689\n",
      "total loss: 0.7688694000244141\n",
      "Epoch [465/500], Loss: 0.7689\n",
      "total loss: 0.7688088417053223\n",
      "Epoch [466/500], Loss: 0.7688\n",
      "total loss: 0.7687485218048096\n",
      "Epoch [467/500], Loss: 0.7687\n",
      "total loss: 0.7686888575553894\n",
      "Epoch [468/500], Loss: 0.7687\n",
      "total loss: 0.768629789352417\n",
      "Epoch [469/500], Loss: 0.7686\n",
      "total loss: 0.7685707211494446\n",
      "Epoch [470/500], Loss: 0.7686\n",
      "total loss: 0.7685125470161438\n",
      "Epoch [471/500], Loss: 0.7685\n",
      "total loss: 0.7684547305107117\n",
      "Epoch [472/500], Loss: 0.7685\n",
      "total loss: 0.7683972716331482\n",
      "Epoch [473/500], Loss: 0.7684\n",
      "total loss: 0.7683402895927429\n",
      "Epoch [474/500], Loss: 0.7683\n",
      "total loss: 0.768283486366272\n",
      "Epoch [475/500], Loss: 0.7683\n",
      "total loss: 0.7682275176048279\n",
      "Epoch [476/500], Loss: 0.7682\n",
      "total loss: 0.7681718468666077\n",
      "Epoch [477/500], Loss: 0.7682\n",
      "total loss: 0.768116295337677\n",
      "Epoch [478/500], Loss: 0.7681\n",
      "total loss: 0.7680611610412598\n",
      "Epoch [479/500], Loss: 0.7681\n",
      "total loss: 0.768006443977356\n",
      "Epoch [480/500], Loss: 0.7680\n",
      "total loss: 0.7679518461227417\n",
      "Epoch [481/500], Loss: 0.7680\n",
      "total loss: 0.7678967714309692\n",
      "Epoch [482/500], Loss: 0.7679\n",
      "total loss: 0.7678428888320923\n",
      "Epoch [483/500], Loss: 0.7678\n",
      "total loss: 0.7677891254425049\n",
      "Epoch [484/500], Loss: 0.7678\n",
      "total loss: 0.7677354216575623\n",
      "Epoch [485/500], Loss: 0.7677\n",
      "total loss: 0.7676821947097778\n",
      "Epoch [486/500], Loss: 0.7677\n",
      "total loss: 0.7676291465759277\n",
      "Epoch [487/500], Loss: 0.7676\n",
      "total loss: 0.7675767540931702\n",
      "Epoch [488/500], Loss: 0.7676\n",
      "total loss: 0.7675246000289917\n",
      "Epoch [489/500], Loss: 0.7675\n",
      "total loss: 0.7674728631973267\n",
      "Epoch [490/500], Loss: 0.7675\n",
      "total loss: 0.767421543598175\n",
      "Epoch [491/500], Loss: 0.7674\n",
      "total loss: 0.7673708200454712\n",
      "Epoch [492/500], Loss: 0.7674\n",
      "total loss: 0.7673202753067017\n",
      "Epoch [493/500], Loss: 0.7673\n",
      "total loss: 0.7672696709632874\n",
      "Epoch [494/500], Loss: 0.7673\n",
      "total loss: 0.767220139503479\n",
      "Epoch [495/500], Loss: 0.7672\n",
      "total loss: 0.7671709060668945\n",
      "Epoch [496/500], Loss: 0.7672\n",
      "total loss: 0.7671217322349548\n",
      "Epoch [497/500], Loss: 0.7671\n",
      "total loss: 0.7670727968215942\n",
      "Epoch [498/500], Loss: 0.7671\n",
      "total loss: 0.767024040222168\n",
      "Epoch [499/500], Loss: 0.7670\n",
      "total loss: 0.7669757604598999\n",
      "Epoch [500/500], Loss: 0.7670\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_model(nn_full, input_data, desired_output, 500, learning_rate=0.0005, viz_accuracy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    torch.save(nn_full.state_dict(), 'weights/sl2z_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    nn_full.load_state_dict(torch.load('weights/sl2z_weights.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(trained_model, new_data, expected_result):\n",
    "    test_input = new_data.float()\n",
    "\n",
    "    test_output = trained_model(test_input)\n",
    "\n",
    "    predicted_classes = torch.argmax(test_output, dim=1)\n",
    "\n",
    "    test_output_compare = expected_result.int()\n",
    "\n",
    "    return float(sum(test_output_compare == predicted_classes)/len(test_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "just_input_test = test_df.drop('first_move_by_Q_learning', axis=1)\n",
    "just_input_test = just_input_test.drop('num_moves_Q_learning_needs', axis=1)\n",
    "\n",
    "input_data_test = torch.tensor(just_input_test.values.tolist(), dtype=torch.float32)\n",
    "desired_output = torch.tensor(test_df['first_move_by_Q_learning'].tolist(), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how often model's first move matches Q learning's on train set\n",
    "test_model(nn_full, input_data, torch.tensor(train['first_move_by_Q_learning'].tolist(), dtype=torch.int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how often model's first move matches Q learning's on test set\n",
    "test_model(nn_full, input_data_test, desired_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_steps_to_origin_w_nn(test_mat):\n",
    "    for i in range(100):\n",
    "        if is_done(test_mat):\n",
    "            return i\n",
    "        demo_test_type = torch.tensor([matrix_to_tuple(test_mat)], dtype=torch.float32)\n",
    "        action = int(torch.argmax(nn_full(demo_test_type)))\n",
    "\n",
    "        test_mat = test_mat @ k_sl2z_2s_gen[action]\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301\n"
     ]
    }
   ],
   "source": [
    "test_df['num_steps_to_origin'] = test_df.apply(lambda row: num_steps_to_origin_w_nn((df_row_to_mat(row))), axis=1)\n",
    "print(\"Num successful: \" + int(sum(test_df['num_steps_to_origin'] != -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_group_test = pd.read_csv(\"../Data_Generation/Data_files/test_rows_SL2Z_Q_learn.csv\")\n",
    "just_input = torch.from_numpy(whole_group_test.drop('first_move_by_Q_learning', axis=1).drop('num_moves_Q_learning_needs', axis=1).to_numpy())\n",
    "# input_data = np.array(just_input.values.tolist())\n",
    "desired_output = torch.tensor(whole_group_test['first_move_by_Q_learning'].tolist(), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(nn_full, just_input, desired_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
