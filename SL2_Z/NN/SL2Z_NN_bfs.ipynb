{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from util import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code adapted from ChatGPT\n",
    "\n",
    "# Define your neural network model\n",
    "class SL2Z_NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SL2Z_NN, self).__init__()\n",
    "\n",
    "        self.step1 = nn.Linear(4, 128, bias=True)\n",
    "        self.step2 = nn.ReLU()\n",
    "        self.step3 = nn.Linear(128, 64, bias=True)\n",
    "        self.step4 = nn.ReLU()\n",
    "        self.step5 = nn.Linear(64, 16, bias=True)\n",
    "        self.step6 = nn.ReLU()\n",
    "        self.step7 = nn.Linear(16, 4, bias=True)\n",
    "\n",
    "        # multi-class classification adapted from ChatGPT\n",
    "        self.step8 = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # RUN IT ON A GPU if it exists\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.to(\"cuda\")\n",
    "\n",
    "        x = self.step1(x)\n",
    "        x = self.step2(x)\n",
    "        x = self.step3(x)\n",
    "        x = self.step4(x)\n",
    "        x = self.step5(x)\n",
    "        x = self.step6(x)\n",
    "        x = self.step7(x)\n",
    "        x = self.step8(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def train_model(model, inputs, desired_outputs, num_epochs=100, learning_rate=0.01, viz_accuracy=False):\n",
    "    # Create a DataLoader to handle batching (if needed)\n",
    "    dataset = TensorDataset(inputs, desired_outputs)\n",
    "    dataloader = DataLoader(dataset, batch_size=1000, shuffle=True)  # Adjust batch_size as needed\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()  # Mean Squared Error loss\n",
    "    # optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "    # criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.995)\n",
    "    losses = []\n",
    "\n",
    "    inputs = inputs.float()\n",
    "    desired_outputs = desired_outputs.long()\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for batch_inputs, batch_desired_outputs in dataloader:\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            outputs = model(batch_inputs)  # Forward pass\n",
    "\n",
    "            batch_desired_outputs = batch_desired_outputs.long()\n",
    "\n",
    "            loss = criterion(outputs, batch_desired_outputs)  # Compute the loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update the model's parameters\n",
    "            cur_item = loss.item()\n",
    "            total_loss += cur_item\n",
    "        \n",
    "        # Print the average loss for this epoch\n",
    "        scheduler.step()\n",
    "        print(f\"total loss: {total_loss}\")\n",
    "        average_loss = total_loss / len(dataloader)\n",
    "        if viz_accuracy:\n",
    "            losses.append(average_loss)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.4f}')\n",
    "    \n",
    "    if viz_accuracy:\n",
    "        plt.scatter(x=range(1, len(losses)+1), y=losses)\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Average loss\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"../Data_Generation/Data_files/labeled_points/sl2_Z_3s_bfs_medium.csv\")\n",
    "test_df = pd.read_csv(\n",
    "    \"../Data_Generation/Data_files/labeled_points/sl2_Z_3s_bfs_xtra_small.csv\")\n",
    "\n",
    "train = df.drop(\"dist_to_id\", axis=1)\n",
    "just_input = train.drop('best_move', axis=1)\n",
    "\n",
    "# adapted from https://stackoverflow.com/questions/43898035/pandas-combine-column-values-into-a-list-in-a-new-column\n",
    "input_data = torch.tensor(just_input.values.tolist())\n",
    "desired_output = torch.tensor(\n",
    "    train['best_move'].tolist(), dtype=torch.float32).long()\n",
    "\n",
    "nn_full: SL2Z_NN = SL2Z_NN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total loss: 28.56427526473999\n",
      "Epoch [1/500], Loss: 1.2984\n",
      "total loss: 24.98064088821411\n",
      "Epoch [2/500], Loss: 1.1355\n",
      "total loss: 23.66472816467285\n",
      "Epoch [3/500], Loss: 1.0757\n",
      "total loss: 23.103639721870422\n",
      "Epoch [4/500], Loss: 1.0502\n",
      "total loss: 22.742682576179504\n",
      "Epoch [5/500], Loss: 1.0338\n",
      "total loss: 22.46726107597351\n",
      "Epoch [6/500], Loss: 1.0212\n",
      "total loss: 22.27172476053238\n",
      "Epoch [7/500], Loss: 1.0124\n",
      "total loss: 22.116879642009735\n",
      "Epoch [8/500], Loss: 1.0053\n",
      "total loss: 22.001472175121307\n",
      "Epoch [9/500], Loss: 1.0001\n",
      "total loss: 21.90415495634079\n",
      "Epoch [10/500], Loss: 0.9956\n",
      "total loss: 21.832955956459045\n",
      "Epoch [11/500], Loss: 0.9924\n",
      "total loss: 21.773249626159668\n",
      "Epoch [12/500], Loss: 0.9897\n",
      "total loss: 21.724237978458405\n",
      "Epoch [13/500], Loss: 0.9875\n",
      "total loss: 21.680665969848633\n",
      "Epoch [14/500], Loss: 0.9855\n",
      "total loss: 21.648603975772858\n",
      "Epoch [15/500], Loss: 0.9840\n",
      "total loss: 21.625654757022858\n",
      "Epoch [16/500], Loss: 0.9830\n",
      "total loss: 21.601115107536316\n",
      "Epoch [17/500], Loss: 0.9819\n",
      "total loss: 21.5876082777977\n",
      "Epoch [18/500], Loss: 0.9813\n",
      "total loss: 21.58261227607727\n",
      "Epoch [19/500], Loss: 0.9810\n",
      "total loss: 21.564803957939148\n",
      "Epoch [20/500], Loss: 0.9802\n",
      "total loss: 21.556006908416748\n",
      "Epoch [21/500], Loss: 0.9798\n",
      "total loss: 21.549553990364075\n",
      "Epoch [22/500], Loss: 0.9795\n",
      "total loss: 21.543656945228577\n",
      "Epoch [23/500], Loss: 0.9793\n",
      "total loss: 21.53125786781311\n",
      "Epoch [24/500], Loss: 0.9787\n",
      "total loss: 21.533834755420685\n",
      "Epoch [25/500], Loss: 0.9788\n",
      "total loss: 21.53011190891266\n",
      "Epoch [26/500], Loss: 0.9786\n",
      "total loss: 21.52623224258423\n",
      "Epoch [27/500], Loss: 0.9785\n",
      "total loss: 21.52632200717926\n",
      "Epoch [28/500], Loss: 0.9785\n",
      "total loss: 21.517670512199402\n",
      "Epoch [29/500], Loss: 0.9781\n",
      "total loss: 21.5124973654747\n",
      "Epoch [30/500], Loss: 0.9778\n",
      "total loss: 21.51326733827591\n",
      "Epoch [31/500], Loss: 0.9779\n",
      "total loss: 21.51278454065323\n",
      "Epoch [32/500], Loss: 0.9779\n",
      "total loss: 21.512954354286194\n",
      "Epoch [33/500], Loss: 0.9779\n",
      "total loss: 21.5114905834198\n",
      "Epoch [34/500], Loss: 0.9778\n",
      "total loss: 21.51146411895752\n",
      "Epoch [35/500], Loss: 0.9778\n",
      "total loss: 21.50868183374405\n",
      "Epoch [36/500], Loss: 0.9777\n",
      "total loss: 21.50576823949814\n",
      "Epoch [37/500], Loss: 0.9775\n",
      "total loss: 21.50614857673645\n",
      "Epoch [38/500], Loss: 0.9776\n",
      "total loss: 21.503026485443115\n",
      "Epoch [39/500], Loss: 0.9774\n",
      "total loss: 21.508829712867737\n",
      "Epoch [40/500], Loss: 0.9777\n",
      "total loss: 21.505375921726227\n",
      "Epoch [41/500], Loss: 0.9775\n",
      "total loss: 21.508168041706085\n",
      "Epoch [42/500], Loss: 0.9776\n",
      "total loss: 21.505769968032837\n",
      "Epoch [43/500], Loss: 0.9775\n",
      "total loss: 21.501169800758362\n",
      "Epoch [44/500], Loss: 0.9773\n",
      "total loss: 21.50195825099945\n",
      "Epoch [45/500], Loss: 0.9774\n",
      "total loss: 21.50445932149887\n",
      "Epoch [46/500], Loss: 0.9775\n",
      "total loss: 21.501947045326233\n",
      "Epoch [47/500], Loss: 0.9774\n",
      "total loss: 21.503234267234802\n",
      "Epoch [48/500], Loss: 0.9774\n",
      "total loss: 21.503049731254578\n",
      "Epoch [49/500], Loss: 0.9774\n",
      "total loss: 21.50132668018341\n",
      "Epoch [50/500], Loss: 0.9773\n",
      "total loss: 21.498014986515045\n",
      "Epoch [51/500], Loss: 0.9772\n",
      "total loss: 21.503329813480377\n",
      "Epoch [52/500], Loss: 0.9774\n",
      "total loss: 21.499625086784363\n",
      "Epoch [53/500], Loss: 0.9773\n",
      "total loss: 21.501736521720886\n",
      "Epoch [54/500], Loss: 0.9774\n",
      "total loss: 21.501714289188385\n",
      "Epoch [55/500], Loss: 0.9774\n",
      "total loss: 21.501459419727325\n",
      "Epoch [56/500], Loss: 0.9773\n",
      "total loss: 21.50160402059555\n",
      "Epoch [57/500], Loss: 0.9773\n",
      "total loss: 21.499365389347076\n",
      "Epoch [58/500], Loss: 0.9772\n",
      "total loss: 21.497939705848694\n",
      "Epoch [59/500], Loss: 0.9772\n",
      "total loss: 21.49852806329727\n",
      "Epoch [60/500], Loss: 0.9772\n",
      "total loss: 21.500917553901672\n",
      "Epoch [61/500], Loss: 0.9773\n",
      "total loss: 21.500479459762573\n",
      "Epoch [62/500], Loss: 0.9773\n",
      "total loss: 21.49779325723648\n",
      "Epoch [63/500], Loss: 0.9772\n",
      "total loss: 21.498924136161804\n",
      "Epoch [64/500], Loss: 0.9772\n",
      "total loss: 21.49509310722351\n",
      "Epoch [65/500], Loss: 0.9770\n",
      "total loss: 21.502960681915283\n",
      "Epoch [66/500], Loss: 0.9774\n",
      "total loss: 21.501281440258026\n",
      "Epoch [67/500], Loss: 0.9773\n",
      "total loss: 21.499513626098633\n",
      "Epoch [68/500], Loss: 0.9773\n",
      "total loss: 21.503029108047485\n",
      "Epoch [69/500], Loss: 0.9774\n",
      "total loss: 21.498095870018005\n",
      "Epoch [70/500], Loss: 0.9772\n",
      "total loss: 21.498106241226196\n",
      "Epoch [71/500], Loss: 0.9772\n",
      "total loss: 21.49566090106964\n",
      "Epoch [72/500], Loss: 0.9771\n",
      "total loss: 21.500494062900543\n",
      "Epoch [73/500], Loss: 0.9773\n",
      "total loss: 21.498244404792786\n",
      "Epoch [74/500], Loss: 0.9772\n",
      "total loss: 21.499668419361115\n",
      "Epoch [75/500], Loss: 0.9773\n",
      "total loss: 21.496675670146942\n",
      "Epoch [76/500], Loss: 0.9771\n",
      "total loss: 21.496544361114502\n",
      "Epoch [77/500], Loss: 0.9771\n",
      "total loss: 21.498924255371094\n",
      "Epoch [78/500], Loss: 0.9772\n",
      "total loss: 21.498383939266205\n",
      "Epoch [79/500], Loss: 0.9772\n",
      "total loss: 21.499145209789276\n",
      "Epoch [80/500], Loss: 0.9772\n",
      "total loss: 21.494236767292023\n",
      "Epoch [81/500], Loss: 0.9770\n",
      "total loss: 21.49683302640915\n",
      "Epoch [82/500], Loss: 0.9771\n",
      "total loss: 21.499781250953674\n",
      "Epoch [83/500], Loss: 0.9773\n",
      "total loss: 21.49961483478546\n",
      "Epoch [84/500], Loss: 0.9773\n",
      "total loss: 21.499024152755737\n",
      "Epoch [85/500], Loss: 0.9772\n",
      "total loss: 21.49971169233322\n",
      "Epoch [86/500], Loss: 0.9773\n",
      "total loss: 21.497834265232086\n",
      "Epoch [87/500], Loss: 0.9772\n",
      "total loss: 21.498290479183197\n",
      "Epoch [88/500], Loss: 0.9772\n",
      "total loss: 21.49876606464386\n",
      "Epoch [89/500], Loss: 0.9772\n",
      "total loss: 21.4959699511528\n",
      "Epoch [90/500], Loss: 0.9771\n",
      "total loss: 21.496385395526886\n",
      "Epoch [91/500], Loss: 0.9771\n",
      "total loss: 21.49818444252014\n",
      "Epoch [92/500], Loss: 0.9772\n",
      "total loss: 21.49713897705078\n",
      "Epoch [93/500], Loss: 0.9771\n",
      "total loss: 21.4991255402565\n",
      "Epoch [94/500], Loss: 0.9772\n",
      "total loss: 21.497657775878906\n",
      "Epoch [95/500], Loss: 0.9772\n",
      "total loss: 21.49826741218567\n",
      "Epoch [96/500], Loss: 0.9772\n",
      "total loss: 21.495517134666443\n",
      "Epoch [97/500], Loss: 0.9771\n",
      "total loss: 21.495198249816895\n",
      "Epoch [98/500], Loss: 0.9771\n",
      "total loss: 21.50003969669342\n",
      "Epoch [99/500], Loss: 0.9773\n",
      "total loss: 21.50160849094391\n",
      "Epoch [100/500], Loss: 0.9773\n",
      "total loss: 21.49602049589157\n",
      "Epoch [101/500], Loss: 0.9771\n",
      "total loss: 21.499498784542084\n",
      "Epoch [102/500], Loss: 0.9772\n",
      "total loss: 21.498182833194733\n",
      "Epoch [103/500], Loss: 0.9772\n",
      "total loss: 21.49858593940735\n",
      "Epoch [104/500], Loss: 0.9772\n",
      "total loss: 21.499548375606537\n",
      "Epoch [105/500], Loss: 0.9773\n",
      "total loss: 21.496929466724396\n",
      "Epoch [106/500], Loss: 0.9771\n",
      "total loss: 21.49667924642563\n",
      "Epoch [107/500], Loss: 0.9771\n",
      "total loss: 21.495303452014923\n",
      "Epoch [108/500], Loss: 0.9771\n",
      "total loss: 21.496488749980927\n",
      "Epoch [109/500], Loss: 0.9771\n",
      "total loss: 21.49415534734726\n",
      "Epoch [110/500], Loss: 0.9770\n",
      "total loss: 21.50131058692932\n",
      "Epoch [111/500], Loss: 0.9773\n",
      "total loss: 21.499237656593323\n",
      "Epoch [112/500], Loss: 0.9772\n",
      "total loss: 21.495006322860718\n",
      "Epoch [113/500], Loss: 0.9770\n",
      "total loss: 21.49703675508499\n",
      "Epoch [114/500], Loss: 0.9771\n",
      "total loss: 21.495854675769806\n",
      "Epoch [115/500], Loss: 0.9771\n",
      "total loss: 21.495202362537384\n",
      "Epoch [116/500], Loss: 0.9771\n",
      "total loss: 21.49570143222809\n",
      "Epoch [117/500], Loss: 0.9771\n",
      "total loss: 21.49490261077881\n",
      "Epoch [118/500], Loss: 0.9770\n",
      "total loss: 21.49526035785675\n",
      "Epoch [119/500], Loss: 0.9771\n",
      "total loss: 21.501745641231537\n",
      "Epoch [120/500], Loss: 0.9774\n",
      "total loss: 21.501302361488342\n",
      "Epoch [121/500], Loss: 0.9773\n",
      "total loss: 21.49648016691208\n",
      "Epoch [122/500], Loss: 0.9771\n",
      "total loss: 21.495044767856598\n",
      "Epoch [123/500], Loss: 0.9770\n",
      "total loss: 21.49747258424759\n",
      "Epoch [124/500], Loss: 0.9772\n",
      "total loss: 21.501413226127625\n",
      "Epoch [125/500], Loss: 0.9773\n",
      "total loss: 21.497955679893494\n",
      "Epoch [126/500], Loss: 0.9772\n",
      "total loss: 21.49588507413864\n",
      "Epoch [127/500], Loss: 0.9771\n",
      "total loss: 21.49694174528122\n",
      "Epoch [128/500], Loss: 0.9771\n",
      "total loss: 21.495560884475708\n",
      "Epoch [129/500], Loss: 0.9771\n",
      "total loss: 21.49805521965027\n",
      "Epoch [130/500], Loss: 0.9772\n",
      "total loss: 21.496321082115173\n",
      "Epoch [131/500], Loss: 0.9771\n",
      "total loss: 21.496837615966797\n",
      "Epoch [132/500], Loss: 0.9771\n",
      "total loss: 21.496710240840912\n",
      "Epoch [133/500], Loss: 0.9771\n",
      "total loss: 21.49790793657303\n",
      "Epoch [134/500], Loss: 0.9772\n",
      "total loss: 21.494303464889526\n",
      "Epoch [135/500], Loss: 0.9770\n",
      "total loss: 21.49746423959732\n",
      "Epoch [136/500], Loss: 0.9772\n",
      "total loss: 21.496606707572937\n",
      "Epoch [137/500], Loss: 0.9771\n",
      "total loss: 21.496423542499542\n",
      "Epoch [138/500], Loss: 0.9771\n",
      "total loss: 21.498998880386353\n",
      "Epoch [139/500], Loss: 0.9772\n",
      "total loss: 21.495141446590424\n",
      "Epoch [140/500], Loss: 0.9771\n",
      "total loss: 21.498617589473724\n",
      "Epoch [141/500], Loss: 0.9772\n",
      "total loss: 21.497306883335114\n",
      "Epoch [142/500], Loss: 0.9772\n",
      "total loss: 21.49749219417572\n",
      "Epoch [143/500], Loss: 0.9772\n",
      "total loss: 21.4952871799469\n",
      "Epoch [144/500], Loss: 0.9771\n",
      "total loss: 21.498846769332886\n",
      "Epoch [145/500], Loss: 0.9772\n",
      "total loss: 21.49477183818817\n",
      "Epoch [146/500], Loss: 0.9770\n",
      "total loss: 21.49692451953888\n",
      "Epoch [147/500], Loss: 0.9771\n",
      "total loss: 21.494278609752655\n",
      "Epoch [148/500], Loss: 0.9770\n",
      "total loss: 21.498091340065002\n",
      "Epoch [149/500], Loss: 0.9772\n",
      "total loss: 21.49845391511917\n",
      "Epoch [150/500], Loss: 0.9772\n",
      "total loss: 21.4971866607666\n",
      "Epoch [151/500], Loss: 0.9771\n",
      "total loss: 21.498993754386902\n",
      "Epoch [152/500], Loss: 0.9772\n",
      "total loss: 21.4942729473114\n",
      "Epoch [153/500], Loss: 0.9770\n",
      "total loss: 21.498678386211395\n",
      "Epoch [154/500], Loss: 0.9772\n",
      "total loss: 21.49423211812973\n",
      "Epoch [155/500], Loss: 0.9770\n",
      "total loss: 21.494358479976654\n",
      "Epoch [156/500], Loss: 0.9770\n",
      "total loss: 21.496032178401947\n",
      "Epoch [157/500], Loss: 0.9771\n",
      "total loss: 21.495571315288544\n",
      "Epoch [158/500], Loss: 0.9771\n",
      "total loss: 21.49489176273346\n",
      "Epoch [159/500], Loss: 0.9770\n",
      "total loss: 21.50111335515976\n",
      "Epoch [160/500], Loss: 0.9773\n",
      "total loss: 21.498642683029175\n",
      "Epoch [161/500], Loss: 0.9772\n",
      "total loss: 21.496263027191162\n",
      "Epoch [162/500], Loss: 0.9771\n",
      "total loss: 21.499481678009033\n",
      "Epoch [163/500], Loss: 0.9772\n",
      "total loss: 21.496300995349884\n",
      "Epoch [164/500], Loss: 0.9771\n",
      "total loss: 21.498166799545288\n",
      "Epoch [165/500], Loss: 0.9772\n",
      "total loss: 21.49739843606949\n",
      "Epoch [166/500], Loss: 0.9772\n",
      "total loss: 21.493985891342163\n",
      "Epoch [167/500], Loss: 0.9770\n",
      "total loss: 21.497637689113617\n",
      "Epoch [168/500], Loss: 0.9772\n",
      "total loss: 21.496114373207092\n",
      "Epoch [169/500], Loss: 0.9771\n",
      "total loss: 21.49761724472046\n",
      "Epoch [170/500], Loss: 0.9772\n",
      "total loss: 21.49509906768799\n",
      "Epoch [171/500], Loss: 0.9770\n",
      "total loss: 21.497339189052582\n",
      "Epoch [172/500], Loss: 0.9772\n",
      "total loss: 21.497742354869843\n",
      "Epoch [173/500], Loss: 0.9772\n",
      "total loss: 21.500487565994263\n",
      "Epoch [174/500], Loss: 0.9773\n",
      "total loss: 21.49943518638611\n",
      "Epoch [175/500], Loss: 0.9772\n",
      "total loss: 21.49451994895935\n",
      "Epoch [176/500], Loss: 0.9770\n",
      "total loss: 21.496732115745544\n",
      "Epoch [177/500], Loss: 0.9771\n",
      "total loss: 21.501232385635376\n",
      "Epoch [178/500], Loss: 0.9773\n",
      "total loss: 21.500012695789337\n",
      "Epoch [179/500], Loss: 0.9773\n",
      "total loss: 21.495310366153717\n",
      "Epoch [180/500], Loss: 0.9771\n",
      "total loss: 21.494339644908905\n",
      "Epoch [181/500], Loss: 0.9770\n",
      "total loss: 21.495330274105072\n",
      "Epoch [182/500], Loss: 0.9771\n",
      "total loss: 21.497298002243042\n",
      "Epoch [183/500], Loss: 0.9771\n",
      "total loss: 21.496352434158325\n",
      "Epoch [184/500], Loss: 0.9771\n",
      "total loss: 21.495487868785858\n",
      "Epoch [185/500], Loss: 0.9771\n",
      "total loss: 21.49698317050934\n",
      "Epoch [186/500], Loss: 0.9771\n",
      "total loss: 21.50118750333786\n",
      "Epoch [187/500], Loss: 0.9773\n",
      "total loss: 21.498748004436493\n",
      "Epoch [188/500], Loss: 0.9772\n",
      "total loss: 21.49567550420761\n",
      "Epoch [189/500], Loss: 0.9771\n",
      "total loss: 21.49991101026535\n",
      "Epoch [190/500], Loss: 0.9773\n",
      "total loss: 21.499294996261597\n",
      "Epoch [191/500], Loss: 0.9772\n",
      "total loss: 21.497839093208313\n",
      "Epoch [192/500], Loss: 0.9772\n",
      "total loss: 21.501106917858124\n",
      "Epoch [193/500], Loss: 0.9773\n",
      "total loss: 21.499993681907654\n",
      "Epoch [194/500], Loss: 0.9773\n",
      "total loss: 21.49750977754593\n",
      "Epoch [195/500], Loss: 0.9772\n",
      "total loss: 21.497942090034485\n",
      "Epoch [196/500], Loss: 0.9772\n",
      "total loss: 21.499648332595825\n",
      "Epoch [197/500], Loss: 0.9773\n",
      "total loss: 21.497914254665375\n",
      "Epoch [198/500], Loss: 0.9772\n",
      "total loss: 21.497511327266693\n",
      "Epoch [199/500], Loss: 0.9772\n",
      "total loss: 21.497425973415375\n",
      "Epoch [200/500], Loss: 0.9772\n",
      "total loss: 21.498103499412537\n",
      "Epoch [201/500], Loss: 0.9772\n",
      "total loss: 21.496513724327087\n",
      "Epoch [202/500], Loss: 0.9771\n",
      "total loss: 21.495910465717316\n",
      "Epoch [203/500], Loss: 0.9771\n",
      "total loss: 21.49171406030655\n",
      "Epoch [204/500], Loss: 0.9769\n",
      "total loss: 21.496534526348114\n",
      "Epoch [205/500], Loss: 0.9771\n",
      "total loss: 21.494605600833893\n",
      "Epoch [206/500], Loss: 0.9770\n",
      "total loss: 21.49364972114563\n",
      "Epoch [207/500], Loss: 0.9770\n",
      "total loss: 21.500227749347687\n",
      "Epoch [208/500], Loss: 0.9773\n",
      "total loss: 21.494412660598755\n",
      "Epoch [209/500], Loss: 0.9770\n",
      "total loss: 21.495927453041077\n",
      "Epoch [210/500], Loss: 0.9771\n",
      "total loss: 21.497668266296387\n",
      "Epoch [211/500], Loss: 0.9772\n",
      "total loss: 21.49602562189102\n",
      "Epoch [212/500], Loss: 0.9771\n",
      "total loss: 21.493836760520935\n",
      "Epoch [213/500], Loss: 0.9770\n",
      "total loss: 21.497391879558563\n",
      "Epoch [214/500], Loss: 0.9772\n",
      "total loss: 21.494777739048004\n",
      "Epoch [215/500], Loss: 0.9770\n",
      "total loss: 21.49469220638275\n",
      "Epoch [216/500], Loss: 0.9770\n",
      "total loss: 21.495312869548798\n",
      "Epoch [217/500], Loss: 0.9771\n",
      "total loss: 21.501150131225586\n",
      "Epoch [218/500], Loss: 0.9773\n",
      "total loss: 21.496017575263977\n",
      "Epoch [219/500], Loss: 0.9771\n",
      "total loss: 21.494433343410492\n",
      "Epoch [220/500], Loss: 0.9770\n",
      "total loss: 21.496214389801025\n",
      "Epoch [221/500], Loss: 0.9771\n",
      "total loss: 21.497667253017426\n",
      "Epoch [222/500], Loss: 0.9772\n",
      "total loss: 21.49846464395523\n",
      "Epoch [223/500], Loss: 0.9772\n",
      "total loss: 21.502513706684113\n",
      "Epoch [224/500], Loss: 0.9774\n",
      "total loss: 21.49613106250763\n",
      "Epoch [225/500], Loss: 0.9771\n",
      "total loss: 21.49471813440323\n",
      "Epoch [226/500], Loss: 0.9770\n",
      "total loss: 21.498697698116302\n",
      "Epoch [227/500], Loss: 0.9772\n",
      "total loss: 21.49722957611084\n",
      "Epoch [228/500], Loss: 0.9771\n",
      "total loss: 21.49697434902191\n",
      "Epoch [229/500], Loss: 0.9771\n",
      "total loss: 21.497809946537018\n",
      "Epoch [230/500], Loss: 0.9772\n",
      "total loss: 21.49930500984192\n",
      "Epoch [231/500], Loss: 0.9772\n",
      "total loss: 21.495896100997925\n",
      "Epoch [232/500], Loss: 0.9771\n",
      "total loss: 21.495963096618652\n",
      "Epoch [233/500], Loss: 0.9771\n",
      "total loss: 21.495805501937866\n",
      "Epoch [234/500], Loss: 0.9771\n",
      "total loss: 21.499521493911743\n",
      "Epoch [235/500], Loss: 0.9773\n",
      "total loss: 21.49454391002655\n",
      "Epoch [236/500], Loss: 0.9770\n",
      "total loss: 21.49674564599991\n",
      "Epoch [237/500], Loss: 0.9771\n",
      "total loss: 21.49553096294403\n",
      "Epoch [238/500], Loss: 0.9771\n",
      "total loss: 21.495431005954742\n",
      "Epoch [239/500], Loss: 0.9771\n",
      "total loss: 21.496206164360046\n",
      "Epoch [240/500], Loss: 0.9771\n",
      "total loss: 21.501961410045624\n",
      "Epoch [241/500], Loss: 0.9774\n",
      "total loss: 21.49728751182556\n",
      "Epoch [242/500], Loss: 0.9771\n",
      "total loss: 21.499081552028656\n",
      "Epoch [243/500], Loss: 0.9772\n",
      "total loss: 21.495636522769928\n",
      "Epoch [244/500], Loss: 0.9771\n",
      "total loss: 21.49978107213974\n",
      "Epoch [245/500], Loss: 0.9773\n",
      "total loss: 21.501698672771454\n",
      "Epoch [246/500], Loss: 0.9773\n",
      "total loss: 21.49717676639557\n",
      "Epoch [247/500], Loss: 0.9771\n",
      "total loss: 21.495285987854004\n",
      "Epoch [248/500], Loss: 0.9771\n",
      "total loss: 21.497144758701324\n",
      "Epoch [249/500], Loss: 0.9771\n",
      "total loss: 21.497976899147034\n",
      "Epoch [250/500], Loss: 0.9772\n",
      "total loss: 21.49450969696045\n",
      "Epoch [251/500], Loss: 0.9770\n",
      "total loss: 21.496288239955902\n",
      "Epoch [252/500], Loss: 0.9771\n",
      "total loss: 21.491154551506042\n",
      "Epoch [253/500], Loss: 0.9769\n",
      "total loss: 21.498012959957123\n",
      "Epoch [254/500], Loss: 0.9772\n",
      "total loss: 21.496397793293\n",
      "Epoch [255/500], Loss: 0.9771\n",
      "total loss: 21.498407244682312\n",
      "Epoch [256/500], Loss: 0.9772\n",
      "total loss: 21.498573184013367\n",
      "Epoch [257/500], Loss: 0.9772\n",
      "total loss: 21.497492134571075\n",
      "Epoch [258/500], Loss: 0.9772\n",
      "total loss: 21.49640828371048\n",
      "Epoch [259/500], Loss: 0.9771\n",
      "total loss: 21.49701702594757\n",
      "Epoch [260/500], Loss: 0.9771\n",
      "total loss: 21.494391560554504\n",
      "Epoch [261/500], Loss: 0.9770\n",
      "total loss: 21.499507904052734\n",
      "Epoch [262/500], Loss: 0.9773\n",
      "total loss: 21.500199794769287\n",
      "Epoch [263/500], Loss: 0.9773\n",
      "total loss: 21.499009609222412\n",
      "Epoch [264/500], Loss: 0.9772\n",
      "total loss: 21.495498776435852\n",
      "Epoch [265/500], Loss: 0.9771\n",
      "total loss: 21.494584381580353\n",
      "Epoch [266/500], Loss: 0.9770\n",
      "total loss: 21.495462715625763\n",
      "Epoch [267/500], Loss: 0.9771\n",
      "total loss: 21.496019959449768\n",
      "Epoch [268/500], Loss: 0.9771\n",
      "total loss: 21.494977831840515\n",
      "Epoch [269/500], Loss: 0.9770\n",
      "total loss: 21.497798681259155\n",
      "Epoch [270/500], Loss: 0.9772\n",
      "total loss: 21.496735334396362\n",
      "Epoch [271/500], Loss: 0.9771\n",
      "total loss: 21.496844172477722\n",
      "Epoch [272/500], Loss: 0.9771\n",
      "total loss: 21.494545817375183\n",
      "Epoch [273/500], Loss: 0.9770\n",
      "total loss: 21.495156407356262\n",
      "Epoch [274/500], Loss: 0.9771\n",
      "total loss: 21.49880975484848\n",
      "Epoch [275/500], Loss: 0.9772\n",
      "total loss: 21.496190011501312\n",
      "Epoch [276/500], Loss: 0.9771\n",
      "total loss: 21.500991582870483\n",
      "Epoch [277/500], Loss: 0.9773\n",
      "total loss: 21.499095141887665\n",
      "Epoch [278/500], Loss: 0.9772\n",
      "total loss: 21.497577607631683\n",
      "Epoch [279/500], Loss: 0.9772\n",
      "total loss: 21.4978044629097\n",
      "Epoch [280/500], Loss: 0.9772\n",
      "total loss: 21.49553006887436\n",
      "Epoch [281/500], Loss: 0.9771\n",
      "total loss: 21.49385380744934\n",
      "Epoch [282/500], Loss: 0.9770\n",
      "total loss: 21.49518609046936\n",
      "Epoch [283/500], Loss: 0.9771\n",
      "total loss: 21.49725216627121\n",
      "Epoch [284/500], Loss: 0.9771\n",
      "total loss: 21.494820296764374\n",
      "Epoch [285/500], Loss: 0.9770\n",
      "total loss: 21.49802428483963\n",
      "Epoch [286/500], Loss: 0.9772\n",
      "total loss: 21.495430529117584\n",
      "Epoch [287/500], Loss: 0.9771\n",
      "total loss: 21.496212661266327\n",
      "Epoch [288/500], Loss: 0.9771\n",
      "total loss: 21.4951388835907\n",
      "Epoch [289/500], Loss: 0.9771\n",
      "total loss: 21.493781089782715\n",
      "Epoch [290/500], Loss: 0.9770\n",
      "total loss: 21.499396204948425\n",
      "Epoch [291/500], Loss: 0.9772\n",
      "total loss: 21.49855238199234\n",
      "Epoch [292/500], Loss: 0.9772\n",
      "total loss: 21.496156096458435\n",
      "Epoch [293/500], Loss: 0.9771\n",
      "total loss: 21.493999361991882\n",
      "Epoch [294/500], Loss: 0.9770\n",
      "total loss: 21.492952346801758\n",
      "Epoch [295/500], Loss: 0.9770\n",
      "total loss: 21.49664855003357\n",
      "Epoch [296/500], Loss: 0.9771\n",
      "total loss: 21.494955897331238\n",
      "Epoch [297/500], Loss: 0.9770\n",
      "total loss: 21.495771169662476\n",
      "Epoch [298/500], Loss: 0.9771\n",
      "total loss: 21.496325969696045\n",
      "Epoch [299/500], Loss: 0.9771\n",
      "total loss: 21.495638132095337\n",
      "Epoch [300/500], Loss: 0.9771\n",
      "total loss: 21.49600774049759\n",
      "Epoch [301/500], Loss: 0.9771\n",
      "total loss: 21.49634325504303\n",
      "Epoch [302/500], Loss: 0.9771\n",
      "total loss: 21.49948364496231\n",
      "Epoch [303/500], Loss: 0.9772\n",
      "total loss: 21.50119823217392\n",
      "Epoch [304/500], Loss: 0.9773\n",
      "total loss: 21.49903380870819\n",
      "Epoch [305/500], Loss: 0.9772\n",
      "total loss: 21.498201966285706\n",
      "Epoch [306/500], Loss: 0.9772\n",
      "total loss: 21.49963140487671\n",
      "Epoch [307/500], Loss: 0.9773\n",
      "total loss: 21.494492650032043\n",
      "Epoch [308/500], Loss: 0.9770\n",
      "total loss: 21.501743376255035\n",
      "Epoch [309/500], Loss: 0.9774\n",
      "total loss: 21.494700253009796\n",
      "Epoch [310/500], Loss: 0.9770\n",
      "total loss: 21.499468326568604\n",
      "Epoch [311/500], Loss: 0.9772\n",
      "total loss: 21.49660986661911\n",
      "Epoch [312/500], Loss: 0.9771\n",
      "total loss: 21.494691252708435\n",
      "Epoch [313/500], Loss: 0.9770\n",
      "total loss: 21.494725823402405\n",
      "Epoch [314/500], Loss: 0.9770\n",
      "total loss: 21.502627789974213\n",
      "Epoch [315/500], Loss: 0.9774\n",
      "total loss: 21.4931640625\n",
      "Epoch [316/500], Loss: 0.9770\n",
      "total loss: 21.49826407432556\n",
      "Epoch [317/500], Loss: 0.9772\n",
      "total loss: 21.495643734931946\n",
      "Epoch [318/500], Loss: 0.9771\n",
      "total loss: 21.496761560440063\n",
      "Epoch [319/500], Loss: 0.9771\n",
      "total loss: 21.494955837726593\n",
      "Epoch [320/500], Loss: 0.9770\n",
      "total loss: 21.496247708797455\n",
      "Epoch [321/500], Loss: 0.9771\n",
      "total loss: 21.496658265590668\n",
      "Epoch [322/500], Loss: 0.9771\n",
      "total loss: 21.498115956783295\n",
      "Epoch [323/500], Loss: 0.9772\n",
      "total loss: 21.497536420822144\n",
      "Epoch [324/500], Loss: 0.9772\n",
      "total loss: 21.493601977825165\n",
      "Epoch [325/500], Loss: 0.9770\n",
      "total loss: 21.5010786652565\n",
      "Epoch [326/500], Loss: 0.9773\n",
      "total loss: 21.493497908115387\n",
      "Epoch [327/500], Loss: 0.9770\n",
      "total loss: 21.495822250843048\n",
      "Epoch [328/500], Loss: 0.9771\n",
      "total loss: 21.49432110786438\n",
      "Epoch [329/500], Loss: 0.9770\n",
      "total loss: 21.49507474899292\n",
      "Epoch [330/500], Loss: 0.9770\n",
      "total loss: 21.4969402551651\n",
      "Epoch [331/500], Loss: 0.9771\n",
      "total loss: 21.493989050388336\n",
      "Epoch [332/500], Loss: 0.9770\n",
      "total loss: 21.494545578956604\n",
      "Epoch [333/500], Loss: 0.9770\n",
      "total loss: 21.494923055171967\n",
      "Epoch [334/500], Loss: 0.9770\n",
      "total loss: 21.499943614006042\n",
      "Epoch [335/500], Loss: 0.9773\n",
      "total loss: 21.496917605400085\n",
      "Epoch [336/500], Loss: 0.9771\n",
      "total loss: 21.49039590358734\n",
      "Epoch [337/500], Loss: 0.9768\n",
      "total loss: 21.49653333425522\n",
      "Epoch [338/500], Loss: 0.9771\n",
      "total loss: 21.496383249759674\n",
      "Epoch [339/500], Loss: 0.9771\n",
      "total loss: 21.495766699314117\n",
      "Epoch [340/500], Loss: 0.9771\n",
      "total loss: 21.495370864868164\n",
      "Epoch [341/500], Loss: 0.9771\n",
      "total loss: 21.499867796897888\n",
      "Epoch [342/500], Loss: 0.9773\n",
      "total loss: 21.493533849716187\n",
      "Epoch [343/500], Loss: 0.9770\n",
      "total loss: 21.496459424495697\n",
      "Epoch [344/500], Loss: 0.9771\n",
      "total loss: 21.498038947582245\n",
      "Epoch [345/500], Loss: 0.9772\n",
      "total loss: 21.497976422309875\n",
      "Epoch [346/500], Loss: 0.9772\n",
      "total loss: 21.498507738113403\n",
      "Epoch [347/500], Loss: 0.9772\n",
      "total loss: 21.496992886066437\n",
      "Epoch [348/500], Loss: 0.9771\n",
      "total loss: 21.498068153858185\n",
      "Epoch [349/500], Loss: 0.9772\n",
      "total loss: 21.497739255428314\n",
      "Epoch [350/500], Loss: 0.9772\n",
      "total loss: 21.493559181690216\n",
      "Epoch [351/500], Loss: 0.9770\n",
      "total loss: 21.500982522964478\n",
      "Epoch [352/500], Loss: 0.9773\n",
      "total loss: 21.494198977947235\n",
      "Epoch [353/500], Loss: 0.9770\n",
      "total loss: 21.49852752685547\n",
      "Epoch [354/500], Loss: 0.9772\n",
      "total loss: 21.494025707244873\n",
      "Epoch [355/500], Loss: 0.9770\n",
      "total loss: 21.49884557723999\n",
      "Epoch [356/500], Loss: 0.9772\n",
      "total loss: 21.49788588285446\n",
      "Epoch [357/500], Loss: 0.9772\n",
      "total loss: 21.49778378009796\n",
      "Epoch [358/500], Loss: 0.9772\n",
      "total loss: 21.496976912021637\n",
      "Epoch [359/500], Loss: 0.9771\n",
      "total loss: 21.494662404060364\n",
      "Epoch [360/500], Loss: 0.9770\n",
      "total loss: 21.49935805797577\n",
      "Epoch [361/500], Loss: 0.9772\n",
      "total loss: 21.498704493045807\n",
      "Epoch [362/500], Loss: 0.9772\n",
      "total loss: 21.499667942523956\n",
      "Epoch [363/500], Loss: 0.9773\n",
      "total loss: 21.494251251220703\n",
      "Epoch [364/500], Loss: 0.9770\n",
      "total loss: 21.498840928077698\n",
      "Epoch [365/500], Loss: 0.9772\n",
      "total loss: 21.496276021003723\n",
      "Epoch [366/500], Loss: 0.9771\n",
      "total loss: 21.495681881904602\n",
      "Epoch [367/500], Loss: 0.9771\n",
      "total loss: 21.492290377616882\n",
      "Epoch [368/500], Loss: 0.9769\n",
      "total loss: 21.497147738933563\n",
      "Epoch [369/500], Loss: 0.9771\n",
      "total loss: 21.496852457523346\n",
      "Epoch [370/500], Loss: 0.9771\n",
      "total loss: 21.493140816688538\n",
      "Epoch [371/500], Loss: 0.9770\n",
      "total loss: 21.49466371536255\n",
      "Epoch [372/500], Loss: 0.9770\n",
      "total loss: 21.49184340238571\n",
      "Epoch [373/500], Loss: 0.9769\n",
      "total loss: 21.49691277742386\n",
      "Epoch [374/500], Loss: 0.9771\n",
      "total loss: 21.49733853340149\n",
      "Epoch [375/500], Loss: 0.9772\n",
      "total loss: 21.499724447727203\n",
      "Epoch [376/500], Loss: 0.9773\n",
      "total loss: 21.496917247772217\n",
      "Epoch [377/500], Loss: 0.9771\n",
      "total loss: 21.498313426971436\n",
      "Epoch [378/500], Loss: 0.9772\n",
      "total loss: 21.49594295024872\n",
      "Epoch [379/500], Loss: 0.9771\n",
      "total loss: 21.498959600925446\n",
      "Epoch [380/500], Loss: 0.9772\n",
      "total loss: 21.497855722904205\n",
      "Epoch [381/500], Loss: 0.9772\n",
      "total loss: 21.498382031917572\n",
      "Epoch [382/500], Loss: 0.9772\n",
      "total loss: 21.49378877878189\n",
      "Epoch [383/500], Loss: 0.9770\n",
      "total loss: 21.499228298664093\n",
      "Epoch [384/500], Loss: 0.9772\n",
      "total loss: 21.494126558303833\n",
      "Epoch [385/500], Loss: 0.9770\n",
      "total loss: 21.497679114341736\n",
      "Epoch [386/500], Loss: 0.9772\n",
      "total loss: 21.497085392475128\n",
      "Epoch [387/500], Loss: 0.9771\n",
      "total loss: 21.49781084060669\n",
      "Epoch [388/500], Loss: 0.9772\n",
      "total loss: 21.494730710983276\n",
      "Epoch [389/500], Loss: 0.9770\n",
      "total loss: 21.494129955768585\n",
      "Epoch [390/500], Loss: 0.9770\n",
      "total loss: 21.495582044124603\n",
      "Epoch [391/500], Loss: 0.9771\n",
      "total loss: 21.496735274791718\n",
      "Epoch [392/500], Loss: 0.9771\n",
      "total loss: 21.502499878406525\n",
      "Epoch [393/500], Loss: 0.9774\n",
      "total loss: 21.495910584926605\n",
      "Epoch [394/500], Loss: 0.9771\n",
      "total loss: 21.495274007320404\n",
      "Epoch [395/500], Loss: 0.9771\n",
      "total loss: 21.495504319667816\n",
      "Epoch [396/500], Loss: 0.9771\n",
      "total loss: 21.495369374752045\n",
      "Epoch [397/500], Loss: 0.9771\n",
      "total loss: 21.497653543949127\n",
      "Epoch [398/500], Loss: 0.9772\n",
      "total loss: 21.4997661113739\n",
      "Epoch [399/500], Loss: 0.9773\n",
      "total loss: 21.494764387607574\n",
      "Epoch [400/500], Loss: 0.9770\n",
      "total loss: 21.49904102087021\n",
      "Epoch [401/500], Loss: 0.9772\n",
      "total loss: 21.499284029006958\n",
      "Epoch [402/500], Loss: 0.9772\n",
      "total loss: 21.49417269229889\n",
      "Epoch [403/500], Loss: 0.9770\n",
      "total loss: 21.493956446647644\n",
      "Epoch [404/500], Loss: 0.9770\n",
      "total loss: 21.496353149414062\n",
      "Epoch [405/500], Loss: 0.9771\n",
      "total loss: 21.495482623577118\n",
      "Epoch [406/500], Loss: 0.9771\n",
      "total loss: 21.498806834220886\n",
      "Epoch [407/500], Loss: 0.9772\n",
      "total loss: 21.49536645412445\n",
      "Epoch [408/500], Loss: 0.9771\n",
      "total loss: 21.496350526809692\n",
      "Epoch [409/500], Loss: 0.9771\n",
      "total loss: 21.5008447766304\n",
      "Epoch [410/500], Loss: 0.9773\n",
      "total loss: 21.495969474315643\n",
      "Epoch [411/500], Loss: 0.9771\n",
      "total loss: 21.49532437324524\n",
      "Epoch [412/500], Loss: 0.9771\n",
      "total loss: 21.493894159793854\n",
      "Epoch [413/500], Loss: 0.9770\n",
      "total loss: 21.494669556617737\n",
      "Epoch [414/500], Loss: 0.9770\n",
      "total loss: 21.498870968818665\n",
      "Epoch [415/500], Loss: 0.9772\n",
      "total loss: 21.49410754442215\n",
      "Epoch [416/500], Loss: 0.9770\n",
      "total loss: 21.496381163597107\n",
      "Epoch [417/500], Loss: 0.9771\n",
      "total loss: 21.495415091514587\n",
      "Epoch [418/500], Loss: 0.9771\n",
      "total loss: 21.49293577671051\n",
      "Epoch [419/500], Loss: 0.9770\n",
      "total loss: 21.49479603767395\n",
      "Epoch [420/500], Loss: 0.9770\n",
      "total loss: 21.497098922729492\n",
      "Epoch [421/500], Loss: 0.9771\n",
      "total loss: 21.49544656276703\n",
      "Epoch [422/500], Loss: 0.9771\n",
      "total loss: 21.498836278915405\n",
      "Epoch [423/500], Loss: 0.9772\n",
      "total loss: 21.497686088085175\n",
      "Epoch [424/500], Loss: 0.9772\n",
      "total loss: 21.496844589710236\n",
      "Epoch [425/500], Loss: 0.9771\n",
      "total loss: 21.495876252651215\n",
      "Epoch [426/500], Loss: 0.9771\n",
      "total loss: 21.498628735542297\n",
      "Epoch [427/500], Loss: 0.9772\n",
      "total loss: 21.5004905462265\n",
      "Epoch [428/500], Loss: 0.9773\n",
      "total loss: 21.494683146476746\n",
      "Epoch [429/500], Loss: 0.9770\n",
      "total loss: 21.496602714061737\n",
      "Epoch [430/500], Loss: 0.9771\n",
      "total loss: 21.492996275424957\n",
      "Epoch [431/500], Loss: 0.9770\n",
      "total loss: 21.493982672691345\n",
      "Epoch [432/500], Loss: 0.9770\n",
      "total loss: 21.497949481010437\n",
      "Epoch [433/500], Loss: 0.9772\n",
      "total loss: 21.496632277965546\n",
      "Epoch [434/500], Loss: 0.9771\n",
      "total loss: 21.494925796985626\n",
      "Epoch [435/500], Loss: 0.9770\n",
      "total loss: 21.49645221233368\n",
      "Epoch [436/500], Loss: 0.9771\n",
      "total loss: 21.495424032211304\n",
      "Epoch [437/500], Loss: 0.9771\n",
      "total loss: 21.49865424633026\n",
      "Epoch [438/500], Loss: 0.9772\n",
      "total loss: 21.497455179691315\n",
      "Epoch [439/500], Loss: 0.9772\n",
      "total loss: 21.497899055480957\n",
      "Epoch [440/500], Loss: 0.9772\n",
      "total loss: 21.490648210048676\n",
      "Epoch [441/500], Loss: 0.9768\n",
      "total loss: 21.495466232299805\n",
      "Epoch [442/500], Loss: 0.9771\n",
      "total loss: 21.49698770046234\n",
      "Epoch [443/500], Loss: 0.9771\n",
      "total loss: 21.495567798614502\n",
      "Epoch [444/500], Loss: 0.9771\n",
      "total loss: 21.49532240629196\n",
      "Epoch [445/500], Loss: 0.9771\n",
      "total loss: 21.501500368118286\n",
      "Epoch [446/500], Loss: 0.9773\n",
      "total loss: 21.498818814754486\n",
      "Epoch [447/500], Loss: 0.9772\n",
      "total loss: 21.495742857456207\n",
      "Epoch [448/500], Loss: 0.9771\n",
      "total loss: 21.49576872587204\n",
      "Epoch [449/500], Loss: 0.9771\n",
      "total loss: 21.4970703125\n",
      "Epoch [450/500], Loss: 0.9771\n",
      "total loss: 21.494305729866028\n",
      "Epoch [451/500], Loss: 0.9770\n",
      "total loss: 21.49630331993103\n",
      "Epoch [452/500], Loss: 0.9771\n",
      "total loss: 21.497320353984833\n",
      "Epoch [453/500], Loss: 0.9772\n",
      "total loss: 21.496069371700287\n",
      "Epoch [454/500], Loss: 0.9771\n",
      "total loss: 21.49770826101303\n",
      "Epoch [455/500], Loss: 0.9772\n",
      "total loss: 21.498691618442535\n",
      "Epoch [456/500], Loss: 0.9772\n",
      "total loss: 21.49675840139389\n",
      "Epoch [457/500], Loss: 0.9771\n",
      "total loss: 21.498647272586823\n",
      "Epoch [458/500], Loss: 0.9772\n",
      "total loss: 21.490746796131134\n",
      "Epoch [459/500], Loss: 0.9769\n",
      "total loss: 21.494455933570862\n",
      "Epoch [460/500], Loss: 0.9770\n",
      "total loss: 21.496671736240387\n",
      "Epoch [461/500], Loss: 0.9771\n",
      "total loss: 21.49828940629959\n",
      "Epoch [462/500], Loss: 0.9772\n",
      "total loss: 21.49790072441101\n",
      "Epoch [463/500], Loss: 0.9772\n",
      "total loss: 21.49394792318344\n",
      "Epoch [464/500], Loss: 0.9770\n",
      "total loss: 21.494351089000702\n",
      "Epoch [465/500], Loss: 0.9770\n",
      "total loss: 21.493109941482544\n",
      "Epoch [466/500], Loss: 0.9770\n",
      "total loss: 21.496642529964447\n",
      "Epoch [467/500], Loss: 0.9771\n",
      "total loss: 21.499989688396454\n",
      "Epoch [468/500], Loss: 0.9773\n",
      "total loss: 21.499286711215973\n",
      "Epoch [469/500], Loss: 0.9772\n",
      "total loss: 21.498190879821777\n",
      "Epoch [470/500], Loss: 0.9772\n",
      "total loss: 21.49401146173477\n",
      "Epoch [471/500], Loss: 0.9770\n",
      "total loss: 21.501307547092438\n",
      "Epoch [472/500], Loss: 0.9773\n",
      "total loss: 21.494426608085632\n",
      "Epoch [473/500], Loss: 0.9770\n",
      "total loss: 21.49782156944275\n",
      "Epoch [474/500], Loss: 0.9772\n",
      "total loss: 21.499100148677826\n",
      "Epoch [475/500], Loss: 0.9772\n",
      "total loss: 21.495726704597473\n",
      "Epoch [476/500], Loss: 0.9771\n",
      "total loss: 21.49835377931595\n",
      "Epoch [477/500], Loss: 0.9772\n",
      "total loss: 21.494978964328766\n",
      "Epoch [478/500], Loss: 0.9770\n",
      "total loss: 21.495741069316864\n",
      "Epoch [479/500], Loss: 0.9771\n",
      "total loss: 21.496980249881744\n",
      "Epoch [480/500], Loss: 0.9771\n",
      "total loss: 21.50083029270172\n",
      "Epoch [481/500], Loss: 0.9773\n",
      "total loss: 21.4991317987442\n",
      "Epoch [482/500], Loss: 0.9772\n",
      "total loss: 21.49861568212509\n",
      "Epoch [483/500], Loss: 0.9772\n",
      "total loss: 21.494706571102142\n",
      "Epoch [484/500], Loss: 0.9770\n",
      "total loss: 21.49471604824066\n",
      "Epoch [485/500], Loss: 0.9770\n",
      "total loss: 21.499911546707153\n",
      "Epoch [486/500], Loss: 0.9773\n",
      "total loss: 21.49525487422943\n",
      "Epoch [487/500], Loss: 0.9771\n",
      "total loss: 21.498791575431824\n",
      "Epoch [488/500], Loss: 0.9772\n",
      "total loss: 21.493890464305878\n",
      "Epoch [489/500], Loss: 0.9770\n",
      "total loss: 21.499435544013977\n",
      "Epoch [490/500], Loss: 0.9772\n",
      "total loss: 21.497286319732666\n",
      "Epoch [491/500], Loss: 0.9771\n",
      "total loss: 21.49825131893158\n",
      "Epoch [492/500], Loss: 0.9772\n",
      "total loss: 21.497469007968903\n",
      "Epoch [493/500], Loss: 0.9772\n",
      "total loss: 21.50087743997574\n",
      "Epoch [494/500], Loss: 0.9773\n",
      "total loss: 21.49978393316269\n",
      "Epoch [495/500], Loss: 0.9773\n",
      "total loss: 21.49509298801422\n",
      "Epoch [496/500], Loss: 0.9770\n",
      "total loss: 21.498317658901215\n",
      "Epoch [497/500], Loss: 0.9772\n",
      "total loss: 21.49684864282608\n",
      "Epoch [498/500], Loss: 0.9771\n",
      "total loss: 21.49337202310562\n",
      "Epoch [499/500], Loss: 0.9770\n",
      "total loss: 21.493574619293213\n",
      "Epoch [500/500], Loss: 0.9770\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_model(nn_full, input_data, desired_output, 500, learning_rate=0.0005, viz_accuracy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    torch.save(nn_full.state_dict(), 'weights/sl2z_bfs_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    nn_full.load_state_dict(torch.load('weights/sl2z_bfs_weights.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(trained_model, new_data, expected_result):\n",
    "    test_input = new_data.float()\n",
    "\n",
    "    test_output = trained_model(test_input)\n",
    "\n",
    "    predicted_classes = torch.argmax(test_output, dim=1)\n",
    "\n",
    "    test_output_compare = expected_result.int()\n",
    "\n",
    "    return float(sum(test_output_compare == predicted_classes)/len(test_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "just_input_test = test_df.drop('dist_to_id', axis=1)\n",
    "just_input_test = just_input_test.drop('best_move', axis=1)\n",
    "\n",
    "input_data_test = torch.tensor(just_input_test.values.tolist(), dtype=torch.float32)\n",
    "desired_output = torch.tensor(test_df['best_move'].tolist(), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7651529312133789"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how often model's first move matches Q learning's on train set\n",
    "test_model(nn_full, input_data, torch.tensor(\n",
    "    train['best_move'].tolist(), dtype=torch.int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_steps_to_origin_w_nn(test_mat):\n",
    "    original = test_mat\n",
    "    for i in range(100):\n",
    "        if is_done(test_mat):\n",
    "            print(\"we were successful with\", matrix_to_tuple(original), \"with\", i, \"steps!\")\n",
    "            return i\n",
    "        demo_test_type = torch.tensor([matrix_to_tuple(test_mat)], dtype=torch.float32)\n",
    "        action = int(torch.argmax(nn_full(demo_test_type)))\n",
    "\n",
    "        test_mat = test_mat @ k_sl2z_2s_gen[action]\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we were successful with (1.0, 0.0, 0.0, 1.0) with 3 steps!\n",
      "we were successful with (1.0, 0.0, 0.0, 1.0) with 6 steps!\n",
      "we were successful with (1.0, 0.0, 0.0, 1.0) with 3 steps!\n",
      "we were successful with (1.0, 0.0, 0.0, 1.0) with 6 steps!\n",
      "we were successful with (1, 0, 0, 1) with 0 steps!\n",
      "we were successful with (1.0, 0.0, 0.0, 1.0) with 3 steps!\n",
      "we were successful with (1.0, 0.0, 0.0, 1.0) with 6 steps!\n",
      "we were successful with (1.0, 0.0, 0.0, 1.0) with 3 steps!\n",
      "we were successful with (1, 0, 0, 1) with 0 steps!\n",
      "we were successful with (1.0, 0.0, 0.0, 1.0) with 3 steps!\n",
      "we were successful with (1, 0, 0, 1) with 0 steps!\n",
      "we were successful with (1.0, 0.0, 0.0, 1.0) with 3 steps!\n",
      "we were successful with (1, 0, 0, 1) with 0 steps!\n",
      "we were successful with (1.0, 0.0, 0.0, 1.0) with 3 steps!\n",
      "we were successful with (1.0, 0.0, 0.0, 1.0) with 3 steps!\n",
      "we were successful with (1, 0, 0, 1) with 0 steps!\n",
      "we were successful with (1.0, 0.0, 0.0, 1.0) with 3 steps!\n",
      "we were successful with (1, 0, 0, 1) with 0 steps!\n",
      "we were successful with (1, 0, 0, 1) with 0 steps!\n",
      "we were successful with (1.0, 0.0, 0.0, 1.0) with 3 steps!\n",
      "we were successful with (1, 0, 0, 1) with 0 steps!\n",
      "we were successful with (1.0, 0.0, 0.0, 1.0) with 3 steps!\n",
      "we were successful with (1.0, 0.0, 0.0, 1.0) with 3 steps!\n",
      "we were successful with (1.0, 0.0, 0.0, 1.0) with 6 steps!\n",
      "we were successful with (1.0, 0.0, 0.0, 1.0) with 3 steps!\n",
      "we were successful with (1.0, 0.0, 0.0, 1.0) with 6 steps!\n",
      "we were successful with (1.0, 0.0, 0.0, 1.0) with 6 steps!\n",
      "we were successful with (1.0, 0.0, 0.0, 1.0) with 3 steps!\n",
      "we were successful with (1, 0, 0, 1) with 0 steps!\n",
      "we were successful with (1.0, 0.0, 0.0, 1.0) with 3 steps!\n",
      "we were successful with (1, 0, 0, 1) with 0 steps!\n",
      "we were successful with (1.0, 0.0, 0.0, 1.0) with 3 steps!\n",
      "we were successful with (1, 0, 0, 1) with 0 steps!\n",
      "we were successful with (1.0, 0.0, 0.0, 1.0) with 3 steps!\n",
      "we were successful with (1, 0, 0, 1) with 0 steps!\n",
      "we were successful with (1.0, 0.0, 0.0, 1.0) with 3 steps!\n",
      "we were successful with (1.0, 0.0, 0.0, 1.0) with 3 steps!\n",
      "we were successful with (1, 0, 0, 1) with 0 steps!\n",
      "we were successful with (1.0, 0.0, 0.0, 1.0) with 3 steps!\n",
      "we were successful with (1, 0, 0, 1) with 0 steps!\n",
      "we were successful with (1, 0, 0, 1) with 0 steps!\n",
      "we were successful with (1.0, 0.0, 0.0, 1.0) with 3 steps!\n",
      "we were successful with (1, 0, 0, 1) with 0 steps!\n",
      "we were successful with (1.0, 0.0, 0.0, 1.0) with 3 steps!\n",
      "we were successful with (1.0, 0.0, 0.0, 1.0) with 3 steps!\n",
      "we were successful with (1, 0, 0, 1) with 0 steps!\n",
      "we were successful with (1.0, 0.0, 0.0, 1.0) with 3 steps!\n",
      "we were successful with (1, 0, 0, 1) with 0 steps!\n",
      "we were successful with (1.0, 0.0, 0.0, 1.0) with 3 steps!\n",
      "we were successful with (1, 0, 0, 1) with 0 steps!\n",
      "we were successful with (1.0, 0.0, 0.0, 1.0) with 3 steps!\n",
      "we were successful with (1, 0, 0, 1) with 0 steps!\n",
      "we were successful with (1.0, 0.0, 0.0, 1.0) with 3 steps!\n",
      "we were successful with (1.0, 0.0, 0.0, 1.0) with 6 steps!\n",
      "we were successful with (1.0, 0.0, 0.0, 1.0) with 6 steps!\n",
      "we were successful with (1.0, 0.0, 0.0, 1.0) with 3 steps!\n",
      "we were successful with (1.0, 0.0, 0.0, 1.0) with 6 steps!\n",
      "we were successful with (1.0, 0.0, 0.0, 1.0) with 3 steps!\n",
      "we were successful with (1.0, 0.0, 0.0, 1.0) with 3 steps!\n",
      "we were successful with (1, 0, 0, 1) with 0 steps!\n",
      "we were successful with (1.0, 0.0, 0.0, 1.0) with 3 steps!\n",
      "we were successful with (1, 0, 0, 1) with 0 steps!\n",
      "we were successful with (1, 0, 0, 1) with 0 steps!\n",
      "we were successful with (1.0, 0.0, 0.0, 1.0) with 3 steps!\n",
      "we were successful with (1, 0, 0, 1) with 0 steps!\n",
      "we were successful with (1.0, 0.0, 0.0, 1.0) with 3 steps!\n",
      "we were successful with (1.0, 0.0, 0.0, 1.0) with 3 steps!\n",
      "we were successful with (1, 0, 0, 1) with 0 steps!\n",
      "we were successful with (1.0, 0.0, 0.0, 1.0) with 3 steps!\n",
      "we were successful with (1, 0, 0, 1) with 0 steps!\n",
      "we were successful with (1.0, 0.0, 0.0, 1.0) with 3 steps!\n",
      "we were successful with (1, 0, 0, 1) with 0 steps!\n",
      "we were successful with (1.0, 0.0, 0.0, 1.0) with 3 steps!\n",
      "we were successful with (1.0, 0.0, 0.0, 1.0) with 6 steps!\n",
      "we were successful with (1.0, 0.0, 0.0, 1.0) with 3 steps!\n",
      "we were successful with (1, 0, 0, 1) with 0 steps!\n",
      "we were successful with (1.0, 0.0, 0.0, 1.0) with 6 steps!\n",
      "we were successful with (1.0, 0.0, 0.0, 1.0) with 3 steps!\n",
      "we were successful with (1.0, 0.0, 0.0, 1.0) with 6 steps!\n",
      "we were successful with (1.0, 0.0, 0.0, 1.0) with 3 steps!\n",
      "we were successful with (1, 0, 0, 1) with 0 steps!\n",
      "we were successful with (1.0, 0.0, 0.0, 1.0) with 3 steps!\n",
      "we were successful with (1, 0, 0, 1) with 0 steps!\n",
      "we were successful with (1.0, 0.0, 0.0, 1.0) with 3 steps!\n",
      "we were successful with (1.0, 0.0, 0.0, 1.0) with 3 steps!\n",
      "we were successful with (1, 0, 0, 1) with 0 steps!\n",
      "we were successful with (1.0, 0.0, 0.0, 1.0) with 3 steps!\n",
      "we were successful with (1, 0, 0, 1) with 0 steps!\n",
      "Num successful:  88\n",
      "Number total:  2380\n"
     ]
    }
   ],
   "source": [
    "test_df['num_steps_to_origin'] = test_df.apply(lambda row: num_steps_to_origin_w_nn((df_row_to_mat(row))), axis=1)\n",
    "print(\"Num successful: \", int(sum(test_df['num_steps_to_origin'] != -1)))\n",
    "print(\"Number total: \", test_df.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
