{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code adapted from ChatGPT\n",
    "\n",
    "# Define your neural network model\n",
    "class SL2Z_NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SL2Z_NN, self).__init__()\n",
    "\n",
    "        self.step1 = nn.Linear(4, 128, bias=True)\n",
    "        self.step2 = nn.ReLU()\n",
    "        self.step3 = nn.Linear(128, 64, bias=True)\n",
    "        self.step4 = nn.ReLU()\n",
    "        self.step5 = nn.Linear(64, 16, bias=True)\n",
    "        self.step6 = nn.ReLU()\n",
    "        self.step7 = nn.Linear(16, 4, bias=True)\n",
    "\n",
    "        # multi-class classification adapted from ChatGPT\n",
    "        self.step8 = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # RUN IT ON A GPU if it exists\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.to(\"cuda\")\n",
    "\n",
    "        x = self.step1(x)\n",
    "        x = self.step2(x)\n",
    "        x = self.step3(x)\n",
    "        x = self.step4(x)\n",
    "        x = self.step5(x)\n",
    "        x = self.step6(x)\n",
    "        x = self.step7(x)\n",
    "        x = self.step8(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def train_model(inputs, desired_outputs, num_epochs=100, learning_rate=0.01, viz_accuracy=False):\n",
    "    # Convert inputs and desired_outputs to PyTorch tensors\n",
    "    inputs = torch.tensor(inputs, dtype=torch.float32)\n",
    "    desired_outputs = torch.tensor(desired_outputs, dtype=torch.float32)\n",
    "    \n",
    "    # Create a DataLoader to handle batching (if needed)\n",
    "    dataset = TensorDataset(inputs, desired_outputs)\n",
    "    dataloader = DataLoader(dataset, batch_size=1000, shuffle=True)  # Adjust batch_size as needed\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = SL2Z_NN()\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()  # Mean Squared Error loss\n",
    "    # optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "    # criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.995)\n",
    "    losses = []\n",
    "\n",
    "    inputs = inputs.float()\n",
    "    desired_outputs = desired_outputs.long()\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for batch_inputs, batch_desired_outputs in dataloader:\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            outputs = model(batch_inputs)  # Forward pass\n",
    "\n",
    "            batch_desired_outputs = batch_desired_outputs.long()\n",
    "\n",
    "            loss = criterion(outputs, batch_desired_outputs)  # Compute the loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update the model's parameters\n",
    "            cur_item = loss.item()\n",
    "            total_loss += cur_item\n",
    "        \n",
    "        # Print the average loss for this epoch\n",
    "        scheduler.step()\n",
    "        print(f\"total loss: {total_loss}\")\n",
    "        average_loss = total_loss / len(dataloader)\n",
    "        if viz_accuracy:\n",
    "            losses.append(average_loss)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.4f}')\n",
    "    \n",
    "    if viz_accuracy:\n",
    "        plt.scatter(x=range(1, len(losses)+1), y=losses)\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Average loss\")\n",
    "        plt.show()\n",
    "\n",
    "    # Return the trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Data_Generation/Data_files/subset_train_rows_SL2Z_Q_learn.csv\")\n",
    "test_df = pd.read_csv(\"../Data_Generation/Data_files/subset_test_rows_SL2Z_Q_learn.csv\")\n",
    "\n",
    "\n",
    "train = df.drop(\"num_moves_Q_learning_needs\", axis=1)\n",
    "# random.seed(42)\n",
    "# np.random.seed(42)\n",
    "# reordered = df.sample(frac=1).reset_index(drop=True)\n",
    "# bound = int(reordered.shape[0] * 0.6)\n",
    "# plus_one = bound+1\n",
    "\n",
    "# train = reordered.iloc[1:bound]\n",
    "# test_df = reordered.iloc[plus_one:reordered.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "just_input = train.drop('first_move_by_Q_learning', axis=1)\n",
    "\n",
    "# adapted from https://stackoverflow.com/questions/43898035/pandas-combine-column-values-into-a-list-in-a-new-column\n",
    "input_data = np.array(just_input.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_output = torch.tensor(train['first_move_by_Q_learning'].tolist(), dtype=torch.float32).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16089\\AppData\\Local\\Temp\\ipykernel_27204\\3137828461.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  desired_outputs = torch.tensor(desired_outputs, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total loss: 2.9214107990264893\n",
      "Epoch [1/500], Loss: 1.4607\n",
      "total loss: 2.8094093799591064\n",
      "Epoch [2/500], Loss: 1.4047\n",
      "total loss: 2.7312684059143066\n",
      "Epoch [3/500], Loss: 1.3656\n",
      "total loss: 2.674448251724243\n",
      "Epoch [4/500], Loss: 1.3372\n",
      "total loss: 2.6166884899139404\n",
      "Epoch [5/500], Loss: 1.3083\n",
      "total loss: 2.5615779161453247\n",
      "Epoch [6/500], Loss: 1.2808\n",
      "total loss: 2.4851627349853516\n",
      "Epoch [7/500], Loss: 1.2426\n",
      "total loss: 2.4275981187820435\n",
      "Epoch [8/500], Loss: 1.2138\n",
      "total loss: 2.38370144367218\n",
      "Epoch [9/500], Loss: 1.1919\n",
      "total loss: 2.362937808036804\n",
      "Epoch [10/500], Loss: 1.1815\n",
      "total loss: 2.3232932090759277\n",
      "Epoch [11/500], Loss: 1.1616\n",
      "total loss: 2.2898389101028442\n",
      "Epoch [12/500], Loss: 1.1449\n",
      "total loss: 2.2633273601531982\n",
      "Epoch [13/500], Loss: 1.1317\n",
      "total loss: 2.232224941253662\n",
      "Epoch [14/500], Loss: 1.1161\n",
      "total loss: 2.2110486030578613\n",
      "Epoch [15/500], Loss: 1.1055\n",
      "total loss: 2.189744234085083\n",
      "Epoch [16/500], Loss: 1.0949\n",
      "total loss: 2.170185089111328\n",
      "Epoch [17/500], Loss: 1.0851\n",
      "total loss: 2.1432429552078247\n",
      "Epoch [18/500], Loss: 1.0716\n",
      "total loss: 2.1123833656311035\n",
      "Epoch [19/500], Loss: 1.0562\n",
      "total loss: 2.0831748247146606\n",
      "Epoch [20/500], Loss: 1.0416\n",
      "total loss: 2.044888734817505\n",
      "Epoch [21/500], Loss: 1.0224\n",
      "total loss: 2.002476155757904\n",
      "Epoch [22/500], Loss: 1.0012\n",
      "total loss: 1.9894880056381226\n",
      "Epoch [23/500], Loss: 0.9947\n",
      "total loss: 1.9711206555366516\n",
      "Epoch [24/500], Loss: 0.9856\n",
      "total loss: 1.9361510872840881\n",
      "Epoch [25/500], Loss: 0.9681\n",
      "total loss: 1.9094091057777405\n",
      "Epoch [26/500], Loss: 0.9547\n",
      "total loss: 1.883850336074829\n",
      "Epoch [27/500], Loss: 0.9419\n",
      "total loss: 1.8551563620567322\n",
      "Epoch [28/500], Loss: 0.9276\n",
      "total loss: 1.8345946073532104\n",
      "Epoch [29/500], Loss: 0.9173\n",
      "total loss: 1.8221850991249084\n",
      "Epoch [30/500], Loss: 0.9111\n",
      "total loss: 1.8097172975540161\n",
      "Epoch [31/500], Loss: 0.9049\n",
      "total loss: 1.8020698428153992\n",
      "Epoch [32/500], Loss: 0.9010\n",
      "total loss: 1.7886147499084473\n",
      "Epoch [33/500], Loss: 0.8943\n",
      "total loss: 1.7793707847595215\n",
      "Epoch [34/500], Loss: 0.8897\n",
      "total loss: 1.7735435366630554\n",
      "Epoch [35/500], Loss: 0.8868\n",
      "total loss: 1.7652422785758972\n",
      "Epoch [36/500], Loss: 0.8826\n",
      "total loss: 1.7581011056900024\n",
      "Epoch [37/500], Loss: 0.8791\n",
      "total loss: 1.7549467086791992\n",
      "Epoch [38/500], Loss: 0.8775\n",
      "total loss: 1.747312307357788\n",
      "Epoch [39/500], Loss: 0.8737\n",
      "total loss: 1.7423295378684998\n",
      "Epoch [40/500], Loss: 0.8712\n",
      "total loss: 1.7382212281227112\n",
      "Epoch [41/500], Loss: 0.8691\n",
      "total loss: 1.7292993068695068\n",
      "Epoch [42/500], Loss: 0.8646\n",
      "total loss: 1.7231894135475159\n",
      "Epoch [43/500], Loss: 0.8616\n",
      "total loss: 1.7222055196762085\n",
      "Epoch [44/500], Loss: 0.8611\n",
      "total loss: 1.7165454626083374\n",
      "Epoch [45/500], Loss: 0.8583\n",
      "total loss: 1.7116042375564575\n",
      "Epoch [46/500], Loss: 0.8558\n",
      "total loss: 1.711637794971466\n",
      "Epoch [47/500], Loss: 0.8558\n",
      "total loss: 1.7049061059951782\n",
      "Epoch [48/500], Loss: 0.8525\n",
      "total loss: 1.7015563249588013\n",
      "Epoch [49/500], Loss: 0.8508\n",
      "total loss: 1.696923553943634\n",
      "Epoch [50/500], Loss: 0.8485\n",
      "total loss: 1.692310392856598\n",
      "Epoch [51/500], Loss: 0.8462\n",
      "total loss: 1.6927573084831238\n",
      "Epoch [52/500], Loss: 0.8464\n",
      "total loss: 1.6844030618667603\n",
      "Epoch [53/500], Loss: 0.8422\n",
      "total loss: 1.680234432220459\n",
      "Epoch [54/500], Loss: 0.8401\n",
      "total loss: 1.6827717423439026\n",
      "Epoch [55/500], Loss: 0.8414\n",
      "total loss: 1.6782746315002441\n",
      "Epoch [56/500], Loss: 0.8391\n",
      "total loss: 1.6710548400878906\n",
      "Epoch [57/500], Loss: 0.8355\n",
      "total loss: 1.6692394018173218\n",
      "Epoch [58/500], Loss: 0.8346\n",
      "total loss: 1.6710124015808105\n",
      "Epoch [59/500], Loss: 0.8355\n",
      "total loss: 1.6661472916603088\n",
      "Epoch [60/500], Loss: 0.8331\n",
      "total loss: 1.6654094457626343\n",
      "Epoch [61/500], Loss: 0.8327\n",
      "total loss: 1.6634016633033752\n",
      "Epoch [62/500], Loss: 0.8317\n",
      "total loss: 1.661020040512085\n",
      "Epoch [63/500], Loss: 0.8305\n",
      "total loss: 1.6570321321487427\n",
      "Epoch [64/500], Loss: 0.8285\n",
      "total loss: 1.6553292870521545\n",
      "Epoch [65/500], Loss: 0.8277\n",
      "total loss: 1.652513325214386\n",
      "Epoch [66/500], Loss: 0.8263\n",
      "total loss: 1.6500967741012573\n",
      "Epoch [67/500], Loss: 0.8250\n",
      "total loss: 1.6495709419250488\n",
      "Epoch [68/500], Loss: 0.8248\n",
      "total loss: 1.6451513767242432\n",
      "Epoch [69/500], Loss: 0.8226\n",
      "total loss: 1.6478356719017029\n",
      "Epoch [70/500], Loss: 0.8239\n",
      "total loss: 1.6421610116958618\n",
      "Epoch [71/500], Loss: 0.8211\n",
      "total loss: 1.6416460275650024\n",
      "Epoch [72/500], Loss: 0.8208\n",
      "total loss: 1.6405437588691711\n",
      "Epoch [73/500], Loss: 0.8203\n",
      "total loss: 1.636448323726654\n",
      "Epoch [74/500], Loss: 0.8182\n",
      "total loss: 1.6349445581436157\n",
      "Epoch [75/500], Loss: 0.8175\n",
      "total loss: 1.6354057788848877\n",
      "Epoch [76/500], Loss: 0.8177\n",
      "total loss: 1.6343831419944763\n",
      "Epoch [77/500], Loss: 0.8172\n",
      "total loss: 1.6293115615844727\n",
      "Epoch [78/500], Loss: 0.8147\n",
      "total loss: 1.6280646920204163\n",
      "Epoch [79/500], Loss: 0.8140\n",
      "total loss: 1.6297166347503662\n",
      "Epoch [80/500], Loss: 0.8149\n",
      "total loss: 1.6242639422416687\n",
      "Epoch [81/500], Loss: 0.8121\n",
      "total loss: 1.623335063457489\n",
      "Epoch [82/500], Loss: 0.8117\n",
      "total loss: 1.6243813633918762\n",
      "Epoch [83/500], Loss: 0.8122\n",
      "total loss: 1.6200572848320007\n",
      "Epoch [84/500], Loss: 0.8100\n",
      "total loss: 1.6197068095207214\n",
      "Epoch [85/500], Loss: 0.8099\n",
      "total loss: 1.6168377995491028\n",
      "Epoch [86/500], Loss: 0.8084\n",
      "total loss: 1.6175948977470398\n",
      "Epoch [87/500], Loss: 0.8088\n",
      "total loss: 1.6159139275550842\n",
      "Epoch [88/500], Loss: 0.8080\n",
      "total loss: 1.615227997303009\n",
      "Epoch [89/500], Loss: 0.8076\n",
      "total loss: 1.6146024465560913\n",
      "Epoch [90/500], Loss: 0.8073\n",
      "total loss: 1.6140720844268799\n",
      "Epoch [91/500], Loss: 0.8070\n",
      "total loss: 1.6085313558578491\n",
      "Epoch [92/500], Loss: 0.8043\n",
      "total loss: 1.6092979907989502\n",
      "Epoch [93/500], Loss: 0.8046\n",
      "total loss: 1.6077232956886292\n",
      "Epoch [94/500], Loss: 0.8039\n",
      "total loss: 1.6053366661071777\n",
      "Epoch [95/500], Loss: 0.8027\n",
      "total loss: 1.6036562323570251\n",
      "Epoch [96/500], Loss: 0.8018\n",
      "total loss: 1.6051445007324219\n",
      "Epoch [97/500], Loss: 0.8026\n",
      "total loss: 1.6033141016960144\n",
      "Epoch [98/500], Loss: 0.8017\n",
      "total loss: 1.6002100110054016\n",
      "Epoch [99/500], Loss: 0.8001\n",
      "total loss: 1.6014018058776855\n",
      "Epoch [100/500], Loss: 0.8007\n",
      "total loss: 1.599669337272644\n",
      "Epoch [101/500], Loss: 0.7998\n",
      "total loss: 1.5977609753608704\n",
      "Epoch [102/500], Loss: 0.7989\n",
      "total loss: 1.5982826352119446\n",
      "Epoch [103/500], Loss: 0.7991\n",
      "total loss: 1.5992515683174133\n",
      "Epoch [104/500], Loss: 0.7996\n",
      "total loss: 1.5952622294425964\n",
      "Epoch [105/500], Loss: 0.7976\n",
      "total loss: 1.5946317911148071\n",
      "Epoch [106/500], Loss: 0.7973\n",
      "total loss: 1.5928653478622437\n",
      "Epoch [107/500], Loss: 0.7964\n",
      "total loss: 1.5912355184555054\n",
      "Epoch [108/500], Loss: 0.7956\n",
      "total loss: 1.5917750597000122\n",
      "Epoch [109/500], Loss: 0.7959\n",
      "total loss: 1.5900678038597107\n",
      "Epoch [110/500], Loss: 0.7950\n",
      "total loss: 1.589256227016449\n",
      "Epoch [111/500], Loss: 0.7946\n",
      "total loss: 1.5858201384544373\n",
      "Epoch [112/500], Loss: 0.7929\n",
      "total loss: 1.586957335472107\n",
      "Epoch [113/500], Loss: 0.7935\n",
      "total loss: 1.5860318541526794\n",
      "Epoch [114/500], Loss: 0.7930\n",
      "total loss: 1.5849647521972656\n",
      "Epoch [115/500], Loss: 0.7925\n",
      "total loss: 1.5840697884559631\n",
      "Epoch [116/500], Loss: 0.7920\n",
      "total loss: 1.5837755799293518\n",
      "Epoch [117/500], Loss: 0.7919\n",
      "total loss: 1.5815114974975586\n",
      "Epoch [118/500], Loss: 0.7908\n",
      "total loss: 1.5837663412094116\n",
      "Epoch [119/500], Loss: 0.7919\n",
      "total loss: 1.5796324610710144\n",
      "Epoch [120/500], Loss: 0.7898\n",
      "total loss: 1.5810242891311646\n",
      "Epoch [121/500], Loss: 0.7905\n",
      "total loss: 1.5781336426734924\n",
      "Epoch [122/500], Loss: 0.7891\n",
      "total loss: 1.577839195728302\n",
      "Epoch [123/500], Loss: 0.7889\n",
      "total loss: 1.5774750113487244\n",
      "Epoch [124/500], Loss: 0.7887\n",
      "total loss: 1.5757092237472534\n",
      "Epoch [125/500], Loss: 0.7879\n",
      "total loss: 1.5758360028266907\n",
      "Epoch [126/500], Loss: 0.7879\n",
      "total loss: 1.5755863189697266\n",
      "Epoch [127/500], Loss: 0.7878\n",
      "total loss: 1.5737338662147522\n",
      "Epoch [128/500], Loss: 0.7869\n",
      "total loss: 1.5747498869895935\n",
      "Epoch [129/500], Loss: 0.7874\n",
      "total loss: 1.5744540095329285\n",
      "Epoch [130/500], Loss: 0.7872\n",
      "total loss: 1.5713207125663757\n",
      "Epoch [131/500], Loss: 0.7857\n",
      "total loss: 1.5720563530921936\n",
      "Epoch [132/500], Loss: 0.7860\n",
      "total loss: 1.5688717365264893\n",
      "Epoch [133/500], Loss: 0.7844\n",
      "total loss: 1.569202959537506\n",
      "Epoch [134/500], Loss: 0.7846\n",
      "total loss: 1.5695427656173706\n",
      "Epoch [135/500], Loss: 0.7848\n",
      "total loss: 1.5685975551605225\n",
      "Epoch [136/500], Loss: 0.7843\n",
      "total loss: 1.568093180656433\n",
      "Epoch [137/500], Loss: 0.7840\n",
      "total loss: 1.5666268467903137\n",
      "Epoch [138/500], Loss: 0.7833\n",
      "total loss: 1.56516695022583\n",
      "Epoch [139/500], Loss: 0.7826\n",
      "total loss: 1.568004071712494\n",
      "Epoch [140/500], Loss: 0.7840\n",
      "total loss: 1.5669689774513245\n",
      "Epoch [141/500], Loss: 0.7835\n",
      "total loss: 1.566592037677765\n",
      "Epoch [142/500], Loss: 0.7833\n",
      "total loss: 1.5674368739128113\n",
      "Epoch [143/500], Loss: 0.7837\n",
      "total loss: 1.5633835792541504\n",
      "Epoch [144/500], Loss: 0.7817\n",
      "total loss: 1.5636873841285706\n",
      "Epoch [145/500], Loss: 0.7818\n",
      "total loss: 1.5612006187438965\n",
      "Epoch [146/500], Loss: 0.7806\n",
      "total loss: 1.5621486902236938\n",
      "Epoch [147/500], Loss: 0.7811\n",
      "total loss: 1.5614341497421265\n",
      "Epoch [148/500], Loss: 0.7807\n",
      "total loss: 1.559449017047882\n",
      "Epoch [149/500], Loss: 0.7797\n",
      "total loss: 1.5608627200126648\n",
      "Epoch [150/500], Loss: 0.7804\n",
      "total loss: 1.5602203607559204\n",
      "Epoch [151/500], Loss: 0.7801\n",
      "total loss: 1.5579286813735962\n",
      "Epoch [152/500], Loss: 0.7790\n",
      "total loss: 1.5585126280784607\n",
      "Epoch [153/500], Loss: 0.7793\n",
      "total loss: 1.5575546026229858\n",
      "Epoch [154/500], Loss: 0.7788\n",
      "total loss: 1.558432936668396\n",
      "Epoch [155/500], Loss: 0.7792\n",
      "total loss: 1.5563527941703796\n",
      "Epoch [156/500], Loss: 0.7782\n",
      "total loss: 1.5563259720802307\n",
      "Epoch [157/500], Loss: 0.7782\n",
      "total loss: 1.5570434927940369\n",
      "Epoch [158/500], Loss: 0.7785\n",
      "total loss: 1.556994378566742\n",
      "Epoch [159/500], Loss: 0.7785\n",
      "total loss: 1.5538603067398071\n",
      "Epoch [160/500], Loss: 0.7769\n",
      "total loss: 1.5539870858192444\n",
      "Epoch [161/500], Loss: 0.7770\n",
      "total loss: 1.553049385547638\n",
      "Epoch [162/500], Loss: 0.7765\n",
      "total loss: 1.5528062582015991\n",
      "Epoch [163/500], Loss: 0.7764\n",
      "total loss: 1.5533640384674072\n",
      "Epoch [164/500], Loss: 0.7767\n",
      "total loss: 1.552076280117035\n",
      "Epoch [165/500], Loss: 0.7760\n",
      "total loss: 1.5512047410011292\n",
      "Epoch [166/500], Loss: 0.7756\n",
      "total loss: 1.5519056916236877\n",
      "Epoch [167/500], Loss: 0.7760\n",
      "total loss: 1.5510237216949463\n",
      "Epoch [168/500], Loss: 0.7755\n",
      "total loss: 1.5513333678245544\n",
      "Epoch [169/500], Loss: 0.7757\n",
      "total loss: 1.5508747696876526\n",
      "Epoch [170/500], Loss: 0.7754\n",
      "total loss: 1.550101637840271\n",
      "Epoch [171/500], Loss: 0.7751\n",
      "total loss: 1.5491458177566528\n",
      "Epoch [172/500], Loss: 0.7746\n",
      "total loss: 1.5493133664131165\n",
      "Epoch [173/500], Loss: 0.7747\n",
      "total loss: 1.5479453206062317\n",
      "Epoch [174/500], Loss: 0.7740\n",
      "total loss: 1.5491560101509094\n",
      "Epoch [175/500], Loss: 0.7746\n",
      "total loss: 1.5473191142082214\n",
      "Epoch [176/500], Loss: 0.7737\n",
      "total loss: 1.5478395223617554\n",
      "Epoch [177/500], Loss: 0.7739\n",
      "total loss: 1.5465930700302124\n",
      "Epoch [178/500], Loss: 0.7733\n",
      "total loss: 1.5467837452888489\n",
      "Epoch [179/500], Loss: 0.7734\n",
      "total loss: 1.5465298295021057\n",
      "Epoch [180/500], Loss: 0.7733\n",
      "total loss: 1.54519522190094\n",
      "Epoch [181/500], Loss: 0.7726\n",
      "total loss: 1.5448789596557617\n",
      "Epoch [182/500], Loss: 0.7724\n",
      "total loss: 1.5452431440353394\n",
      "Epoch [183/500], Loss: 0.7726\n",
      "total loss: 1.5446311235427856\n",
      "Epoch [184/500], Loss: 0.7723\n",
      "total loss: 1.5454232096672058\n",
      "Epoch [185/500], Loss: 0.7727\n",
      "total loss: 1.543975293636322\n",
      "Epoch [186/500], Loss: 0.7720\n",
      "total loss: 1.543041467666626\n",
      "Epoch [187/500], Loss: 0.7715\n",
      "total loss: 1.543177306652069\n",
      "Epoch [188/500], Loss: 0.7716\n",
      "total loss: 1.5420876741409302\n",
      "Epoch [189/500], Loss: 0.7710\n",
      "total loss: 1.5431726574897766\n",
      "Epoch [190/500], Loss: 0.7716\n",
      "total loss: 1.5418263673782349\n",
      "Epoch [191/500], Loss: 0.7709\n",
      "total loss: 1.5413421392440796\n",
      "Epoch [192/500], Loss: 0.7707\n",
      "total loss: 1.5404690504074097\n",
      "Epoch [193/500], Loss: 0.7702\n",
      "total loss: 1.5410204529762268\n",
      "Epoch [194/500], Loss: 0.7705\n",
      "total loss: 1.5400199890136719\n",
      "Epoch [195/500], Loss: 0.7700\n",
      "total loss: 1.5402963757514954\n",
      "Epoch [196/500], Loss: 0.7701\n",
      "total loss: 1.5390127301216125\n",
      "Epoch [197/500], Loss: 0.7695\n",
      "total loss: 1.5392888188362122\n",
      "Epoch [198/500], Loss: 0.7696\n",
      "total loss: 1.5397178530693054\n",
      "Epoch [199/500], Loss: 0.7699\n",
      "total loss: 1.5387287139892578\n",
      "Epoch [200/500], Loss: 0.7694\n",
      "total loss: 1.5394538044929504\n",
      "Epoch [201/500], Loss: 0.7697\n",
      "total loss: 1.5378921031951904\n",
      "Epoch [202/500], Loss: 0.7689\n",
      "total loss: 1.5385289192199707\n",
      "Epoch [203/500], Loss: 0.7693\n",
      "total loss: 1.5372376441955566\n",
      "Epoch [204/500], Loss: 0.7686\n",
      "total loss: 1.53731369972229\n",
      "Epoch [205/500], Loss: 0.7687\n",
      "total loss: 1.536944031715393\n",
      "Epoch [206/500], Loss: 0.7685\n",
      "total loss: 1.5356209874153137\n",
      "Epoch [207/500], Loss: 0.7678\n",
      "total loss: 1.5371907353401184\n",
      "Epoch [208/500], Loss: 0.7686\n",
      "total loss: 1.5361332893371582\n",
      "Epoch [209/500], Loss: 0.7681\n",
      "total loss: 1.5356629490852356\n",
      "Epoch [210/500], Loss: 0.7678\n",
      "total loss: 1.5358620882034302\n",
      "Epoch [211/500], Loss: 0.7679\n",
      "total loss: 1.5362996459007263\n",
      "Epoch [212/500], Loss: 0.7681\n",
      "total loss: 1.5349872708320618\n",
      "Epoch [213/500], Loss: 0.7675\n",
      "total loss: 1.5349642038345337\n",
      "Epoch [214/500], Loss: 0.7675\n",
      "total loss: 1.5354413986206055\n",
      "Epoch [215/500], Loss: 0.7677\n",
      "total loss: 1.5347696542739868\n",
      "Epoch [216/500], Loss: 0.7674\n",
      "total loss: 1.535275399684906\n",
      "Epoch [217/500], Loss: 0.7676\n",
      "total loss: 1.533999264240265\n",
      "Epoch [218/500], Loss: 0.7670\n",
      "total loss: 1.5326635837554932\n",
      "Epoch [219/500], Loss: 0.7663\n",
      "total loss: 1.5341618061065674\n",
      "Epoch [220/500], Loss: 0.7671\n",
      "total loss: 1.5338119268417358\n",
      "Epoch [221/500], Loss: 0.7669\n",
      "total loss: 1.534595251083374\n",
      "Epoch [222/500], Loss: 0.7673\n",
      "total loss: 1.5327558517456055\n",
      "Epoch [223/500], Loss: 0.7664\n",
      "total loss: 1.5324706435203552\n",
      "Epoch [224/500], Loss: 0.7662\n",
      "total loss: 1.5329320430755615\n",
      "Epoch [225/500], Loss: 0.7665\n",
      "total loss: 1.5325223207473755\n",
      "Epoch [226/500], Loss: 0.7663\n",
      "total loss: 1.5325111150741577\n",
      "Epoch [227/500], Loss: 0.7663\n",
      "total loss: 1.5316789746284485\n",
      "Epoch [228/500], Loss: 0.7658\n",
      "total loss: 1.531470000743866\n",
      "Epoch [229/500], Loss: 0.7657\n",
      "total loss: 1.5315820574760437\n",
      "Epoch [230/500], Loss: 0.7658\n",
      "total loss: 1.53146094083786\n",
      "Epoch [231/500], Loss: 0.7657\n",
      "total loss: 1.5310078263282776\n",
      "Epoch [232/500], Loss: 0.7655\n",
      "total loss: 1.531384289264679\n",
      "Epoch [233/500], Loss: 0.7657\n",
      "total loss: 1.5307794213294983\n",
      "Epoch [234/500], Loss: 0.7654\n",
      "total loss: 1.5297881364822388\n",
      "Epoch [235/500], Loss: 0.7649\n",
      "total loss: 1.5295331478118896\n",
      "Epoch [236/500], Loss: 0.7648\n",
      "total loss: 1.5296555757522583\n",
      "Epoch [237/500], Loss: 0.7648\n",
      "total loss: 1.529798984527588\n",
      "Epoch [238/500], Loss: 0.7649\n",
      "total loss: 1.5302230715751648\n",
      "Epoch [239/500], Loss: 0.7651\n",
      "total loss: 1.5284830927848816\n",
      "Epoch [240/500], Loss: 0.7642\n",
      "total loss: 1.5286693572998047\n",
      "Epoch [241/500], Loss: 0.7643\n",
      "total loss: 1.5279815793037415\n",
      "Epoch [242/500], Loss: 0.7640\n",
      "total loss: 1.5273500680923462\n",
      "Epoch [243/500], Loss: 0.7637\n",
      "total loss: 1.5289586782455444\n",
      "Epoch [244/500], Loss: 0.7645\n",
      "total loss: 1.5292429327964783\n",
      "Epoch [245/500], Loss: 0.7646\n",
      "total loss: 1.5278021097183228\n",
      "Epoch [246/500], Loss: 0.7639\n",
      "total loss: 1.5264936089515686\n",
      "Epoch [247/500], Loss: 0.7632\n",
      "total loss: 1.5272927284240723\n",
      "Epoch [248/500], Loss: 0.7636\n",
      "total loss: 1.526309847831726\n",
      "Epoch [249/500], Loss: 0.7632\n",
      "total loss: 1.526954174041748\n",
      "Epoch [250/500], Loss: 0.7635\n",
      "total loss: 1.5270353555679321\n",
      "Epoch [251/500], Loss: 0.7635\n",
      "total loss: 1.5260618925094604\n",
      "Epoch [252/500], Loss: 0.7630\n",
      "total loss: 1.5257023572921753\n",
      "Epoch [253/500], Loss: 0.7629\n",
      "total loss: 1.5266780853271484\n",
      "Epoch [254/500], Loss: 0.7633\n",
      "total loss: 1.5264983773231506\n",
      "Epoch [255/500], Loss: 0.7632\n",
      "total loss: 1.5265692472457886\n",
      "Epoch [256/500], Loss: 0.7633\n",
      "total loss: 1.5262606739997864\n",
      "Epoch [257/500], Loss: 0.7631\n",
      "total loss: 1.5271358489990234\n",
      "Epoch [258/500], Loss: 0.7636\n",
      "total loss: 1.5260475277900696\n",
      "Epoch [259/500], Loss: 0.7630\n",
      "total loss: 1.5257334113121033\n",
      "Epoch [260/500], Loss: 0.7629\n",
      "total loss: 1.5253877639770508\n",
      "Epoch [261/500], Loss: 0.7627\n",
      "total loss: 1.525427222251892\n",
      "Epoch [262/500], Loss: 0.7627\n",
      "total loss: 1.5253861546516418\n",
      "Epoch [263/500], Loss: 0.7627\n",
      "total loss: 1.523695707321167\n",
      "Epoch [264/500], Loss: 0.7618\n",
      "total loss: 1.5249766111373901\n",
      "Epoch [265/500], Loss: 0.7625\n",
      "total loss: 1.5235322713851929\n",
      "Epoch [266/500], Loss: 0.7618\n",
      "total loss: 1.5260544419288635\n",
      "Epoch [267/500], Loss: 0.7630\n",
      "total loss: 1.5248265862464905\n",
      "Epoch [268/500], Loss: 0.7624\n",
      "total loss: 1.5239731669425964\n",
      "Epoch [269/500], Loss: 0.7620\n",
      "total loss: 1.5256345868110657\n",
      "Epoch [270/500], Loss: 0.7628\n",
      "total loss: 1.5247048735618591\n",
      "Epoch [271/500], Loss: 0.7624\n",
      "total loss: 1.524151861667633\n",
      "Epoch [272/500], Loss: 0.7621\n",
      "total loss: 1.5230881571769714\n",
      "Epoch [273/500], Loss: 0.7615\n",
      "total loss: 1.523475468158722\n",
      "Epoch [274/500], Loss: 0.7617\n",
      "total loss: 1.524656593799591\n",
      "Epoch [275/500], Loss: 0.7623\n",
      "total loss: 1.522917091846466\n",
      "Epoch [276/500], Loss: 0.7615\n",
      "total loss: 1.522383213043213\n",
      "Epoch [277/500], Loss: 0.7612\n",
      "total loss: 1.5227360725402832\n",
      "Epoch [278/500], Loss: 0.7614\n",
      "total loss: 1.5236857533454895\n",
      "Epoch [279/500], Loss: 0.7618\n",
      "total loss: 1.5225797891616821\n",
      "Epoch [280/500], Loss: 0.7613\n",
      "total loss: 1.5226286053657532\n",
      "Epoch [281/500], Loss: 0.7613\n",
      "total loss: 1.5233616828918457\n",
      "Epoch [282/500], Loss: 0.7617\n",
      "total loss: 1.5221993923187256\n",
      "Epoch [283/500], Loss: 0.7611\n",
      "total loss: 1.5213878750801086\n",
      "Epoch [284/500], Loss: 0.7607\n",
      "total loss: 1.5217549204826355\n",
      "Epoch [285/500], Loss: 0.7609\n",
      "total loss: 1.5222119092941284\n",
      "Epoch [286/500], Loss: 0.7611\n",
      "total loss: 1.5222619771957397\n",
      "Epoch [287/500], Loss: 0.7611\n",
      "total loss: 1.522063434123993\n",
      "Epoch [288/500], Loss: 0.7610\n",
      "total loss: 1.5206038355827332\n",
      "Epoch [289/500], Loss: 0.7603\n",
      "total loss: 1.5206315517425537\n",
      "Epoch [290/500], Loss: 0.7603\n",
      "total loss: 1.5223681926727295\n",
      "Epoch [291/500], Loss: 0.7612\n",
      "total loss: 1.520971655845642\n",
      "Epoch [292/500], Loss: 0.7605\n",
      "total loss: 1.5212513208389282\n",
      "Epoch [293/500], Loss: 0.7606\n",
      "total loss: 1.5212809443473816\n",
      "Epoch [294/500], Loss: 0.7606\n",
      "total loss: 1.5206067562103271\n",
      "Epoch [295/500], Loss: 0.7603\n",
      "total loss: 1.5204246044158936\n",
      "Epoch [296/500], Loss: 0.7602\n",
      "total loss: 1.5198394656181335\n",
      "Epoch [297/500], Loss: 0.7599\n",
      "total loss: 1.520676612854004\n",
      "Epoch [298/500], Loss: 0.7603\n",
      "total loss: 1.520869791507721\n",
      "Epoch [299/500], Loss: 0.7604\n",
      "total loss: 1.5197277665138245\n",
      "Epoch [300/500], Loss: 0.7599\n",
      "total loss: 1.5197946429252625\n",
      "Epoch [301/500], Loss: 0.7599\n",
      "total loss: 1.5207762718200684\n",
      "Epoch [302/500], Loss: 0.7604\n",
      "total loss: 1.5194755792617798\n",
      "Epoch [303/500], Loss: 0.7597\n",
      "total loss: 1.5203951001167297\n",
      "Epoch [304/500], Loss: 0.7602\n",
      "total loss: 1.5199427604675293\n",
      "Epoch [305/500], Loss: 0.7600\n",
      "total loss: 1.5189558267593384\n",
      "Epoch [306/500], Loss: 0.7595\n",
      "total loss: 1.5195273160934448\n",
      "Epoch [307/500], Loss: 0.7598\n",
      "total loss: 1.5186898708343506\n",
      "Epoch [308/500], Loss: 0.7593\n",
      "total loss: 1.51856929063797\n",
      "Epoch [309/500], Loss: 0.7593\n",
      "total loss: 1.5198275446891785\n",
      "Epoch [310/500], Loss: 0.7599\n",
      "total loss: 1.5192815661430359\n",
      "Epoch [311/500], Loss: 0.7596\n",
      "total loss: 1.519662857055664\n",
      "Epoch [312/500], Loss: 0.7598\n",
      "total loss: 1.5193058848381042\n",
      "Epoch [313/500], Loss: 0.7597\n",
      "total loss: 1.5186394453048706\n",
      "Epoch [314/500], Loss: 0.7593\n",
      "total loss: 1.519030213356018\n",
      "Epoch [315/500], Loss: 0.7595\n",
      "total loss: 1.5179373621940613\n",
      "Epoch [316/500], Loss: 0.7590\n",
      "total loss: 1.5184406638145447\n",
      "Epoch [317/500], Loss: 0.7592\n",
      "total loss: 1.518505871295929\n",
      "Epoch [318/500], Loss: 0.7593\n",
      "total loss: 1.51796954870224\n",
      "Epoch [319/500], Loss: 0.7590\n",
      "total loss: 1.517972707748413\n",
      "Epoch [320/500], Loss: 0.7590\n",
      "total loss: 1.5179767608642578\n",
      "Epoch [321/500], Loss: 0.7590\n",
      "total loss: 1.5181187391281128\n",
      "Epoch [322/500], Loss: 0.7591\n",
      "total loss: 1.5173829793930054\n",
      "Epoch [323/500], Loss: 0.7587\n",
      "total loss: 1.5178046822547913\n",
      "Epoch [324/500], Loss: 0.7589\n",
      "total loss: 1.5171452760696411\n",
      "Epoch [325/500], Loss: 0.7586\n",
      "total loss: 1.517185926437378\n",
      "Epoch [326/500], Loss: 0.7586\n",
      "total loss: 1.5173728466033936\n",
      "Epoch [327/500], Loss: 0.7587\n",
      "total loss: 1.5171346068382263\n",
      "Epoch [328/500], Loss: 0.7586\n",
      "total loss: 1.5174882411956787\n",
      "Epoch [329/500], Loss: 0.7587\n",
      "total loss: 1.5168206095695496\n",
      "Epoch [330/500], Loss: 0.7584\n",
      "total loss: 1.5172637701034546\n",
      "Epoch [331/500], Loss: 0.7586\n",
      "total loss: 1.517274558544159\n",
      "Epoch [332/500], Loss: 0.7586\n",
      "total loss: 1.5168734192848206\n",
      "Epoch [333/500], Loss: 0.7584\n",
      "total loss: 1.5174202919006348\n",
      "Epoch [334/500], Loss: 0.7587\n",
      "total loss: 1.5162733793258667\n",
      "Epoch [335/500], Loss: 0.7581\n",
      "total loss: 1.516148865222931\n",
      "Epoch [336/500], Loss: 0.7581\n",
      "total loss: 1.515548825263977\n",
      "Epoch [337/500], Loss: 0.7578\n",
      "total loss: 1.5154173374176025\n",
      "Epoch [338/500], Loss: 0.7577\n",
      "total loss: 1.5168007016181946\n",
      "Epoch [339/500], Loss: 0.7584\n",
      "total loss: 1.5158249139785767\n",
      "Epoch [340/500], Loss: 0.7579\n",
      "total loss: 1.5157082676887512\n",
      "Epoch [341/500], Loss: 0.7579\n",
      "total loss: 1.515404760837555\n",
      "Epoch [342/500], Loss: 0.7577\n",
      "total loss: 1.515559196472168\n",
      "Epoch [343/500], Loss: 0.7578\n",
      "total loss: 1.5167085528373718\n",
      "Epoch [344/500], Loss: 0.7584\n",
      "total loss: 1.5155118703842163\n",
      "Epoch [345/500], Loss: 0.7578\n",
      "total loss: 1.5161534547805786\n",
      "Epoch [346/500], Loss: 0.7581\n",
      "total loss: 1.5164230465888977\n",
      "Epoch [347/500], Loss: 0.7582\n",
      "total loss: 1.5157537460327148\n",
      "Epoch [348/500], Loss: 0.7579\n",
      "total loss: 1.5153021216392517\n",
      "Epoch [349/500], Loss: 0.7577\n",
      "total loss: 1.5156489610671997\n",
      "Epoch [350/500], Loss: 0.7578\n",
      "total loss: 1.5150892734527588\n",
      "Epoch [351/500], Loss: 0.7575\n",
      "total loss: 1.5146856904029846\n",
      "Epoch [352/500], Loss: 0.7573\n",
      "total loss: 1.5153158903121948\n",
      "Epoch [353/500], Loss: 0.7577\n",
      "total loss: 1.5152859091758728\n",
      "Epoch [354/500], Loss: 0.7576\n",
      "total loss: 1.5155249238014221\n",
      "Epoch [355/500], Loss: 0.7578\n",
      "total loss: 1.5156226754188538\n",
      "Epoch [356/500], Loss: 0.7578\n",
      "total loss: 1.5148941278457642\n",
      "Epoch [357/500], Loss: 0.7574\n",
      "total loss: 1.5154677629470825\n",
      "Epoch [358/500], Loss: 0.7577\n",
      "total loss: 1.5148847103118896\n",
      "Epoch [359/500], Loss: 0.7574\n",
      "total loss: 1.5137980580329895\n",
      "Epoch [360/500], Loss: 0.7569\n",
      "total loss: 1.5150632858276367\n",
      "Epoch [361/500], Loss: 0.7575\n",
      "total loss: 1.5143581628799438\n",
      "Epoch [362/500], Loss: 0.7572\n",
      "total loss: 1.5143609046936035\n",
      "Epoch [363/500], Loss: 0.7572\n",
      "total loss: 1.5144129395484924\n",
      "Epoch [364/500], Loss: 0.7572\n",
      "total loss: 1.5133496522903442\n",
      "Epoch [365/500], Loss: 0.7567\n",
      "total loss: 1.5141181945800781\n",
      "Epoch [366/500], Loss: 0.7571\n",
      "total loss: 1.5142946243286133\n",
      "Epoch [367/500], Loss: 0.7571\n",
      "total loss: 1.5141600966453552\n",
      "Epoch [368/500], Loss: 0.7571\n",
      "total loss: 1.5149040818214417\n",
      "Epoch [369/500], Loss: 0.7575\n",
      "total loss: 1.5133818984031677\n",
      "Epoch [370/500], Loss: 0.7567\n",
      "total loss: 1.5144468545913696\n",
      "Epoch [371/500], Loss: 0.7572\n",
      "total loss: 1.5138282775878906\n",
      "Epoch [372/500], Loss: 0.7569\n",
      "total loss: 1.5137962102890015\n",
      "Epoch [373/500], Loss: 0.7569\n",
      "total loss: 1.5134254097938538\n",
      "Epoch [374/500], Loss: 0.7567\n",
      "total loss: 1.5144080519676208\n",
      "Epoch [375/500], Loss: 0.7572\n",
      "total loss: 1.513334035873413\n",
      "Epoch [376/500], Loss: 0.7567\n",
      "total loss: 1.5134297609329224\n",
      "Epoch [377/500], Loss: 0.7567\n",
      "total loss: 1.513660490512848\n",
      "Epoch [378/500], Loss: 0.7568\n",
      "total loss: 1.5128371119499207\n",
      "Epoch [379/500], Loss: 0.7564\n",
      "total loss: 1.5135104656219482\n",
      "Epoch [380/500], Loss: 0.7568\n",
      "total loss: 1.513237476348877\n",
      "Epoch [381/500], Loss: 0.7566\n",
      "total loss: 1.513329803943634\n",
      "Epoch [382/500], Loss: 0.7567\n",
      "total loss: 1.5133374333381653\n",
      "Epoch [383/500], Loss: 0.7567\n",
      "total loss: 1.5130284428596497\n",
      "Epoch [384/500], Loss: 0.7565\n",
      "total loss: 1.5127307176589966\n",
      "Epoch [385/500], Loss: 0.7564\n",
      "total loss: 1.5131380558013916\n",
      "Epoch [386/500], Loss: 0.7566\n",
      "total loss: 1.5126636624336243\n",
      "Epoch [387/500], Loss: 0.7563\n",
      "total loss: 1.5134042501449585\n",
      "Epoch [388/500], Loss: 0.7567\n",
      "total loss: 1.5132348537445068\n",
      "Epoch [389/500], Loss: 0.7566\n",
      "total loss: 1.5129445791244507\n",
      "Epoch [390/500], Loss: 0.7565\n",
      "total loss: 1.5129535794258118\n",
      "Epoch [391/500], Loss: 0.7565\n",
      "total loss: 1.5122202038764954\n",
      "Epoch [392/500], Loss: 0.7561\n",
      "total loss: 1.5120455622673035\n",
      "Epoch [393/500], Loss: 0.7560\n",
      "total loss: 1.5129703283309937\n",
      "Epoch [394/500], Loss: 0.7565\n",
      "total loss: 1.5111796259880066\n",
      "Epoch [395/500], Loss: 0.7556\n",
      "total loss: 1.5121252536773682\n",
      "Epoch [396/500], Loss: 0.7561\n",
      "total loss: 1.5118335485458374\n",
      "Epoch [397/500], Loss: 0.7559\n",
      "total loss: 1.512745976448059\n",
      "Epoch [398/500], Loss: 0.7564\n",
      "total loss: 1.5126311779022217\n",
      "Epoch [399/500], Loss: 0.7563\n",
      "total loss: 1.512171447277069\n",
      "Epoch [400/500], Loss: 0.7561\n",
      "total loss: 1.512281596660614\n",
      "Epoch [401/500], Loss: 0.7561\n",
      "total loss: 1.5121530890464783\n",
      "Epoch [402/500], Loss: 0.7561\n",
      "total loss: 1.5119643211364746\n",
      "Epoch [403/500], Loss: 0.7560\n",
      "total loss: 1.5123287439346313\n",
      "Epoch [404/500], Loss: 0.7562\n",
      "total loss: 1.5123006701469421\n",
      "Epoch [405/500], Loss: 0.7562\n",
      "total loss: 1.5117170810699463\n",
      "Epoch [406/500], Loss: 0.7559\n",
      "total loss: 1.512178122997284\n",
      "Epoch [407/500], Loss: 0.7561\n",
      "total loss: 1.5109384655952454\n",
      "Epoch [408/500], Loss: 0.7555\n",
      "total loss: 1.5122190117835999\n",
      "Epoch [409/500], Loss: 0.7561\n",
      "total loss: 1.5118741393089294\n",
      "Epoch [410/500], Loss: 0.7559\n",
      "total loss: 1.5118748545646667\n",
      "Epoch [411/500], Loss: 0.7559\n",
      "total loss: 1.5124266743659973\n",
      "Epoch [412/500], Loss: 0.7562\n",
      "total loss: 1.5118911266326904\n",
      "Epoch [413/500], Loss: 0.7559\n",
      "total loss: 1.5104150772094727\n",
      "Epoch [414/500], Loss: 0.7552\n",
      "total loss: 1.5117896795272827\n",
      "Epoch [415/500], Loss: 0.7559\n",
      "total loss: 1.5111385583877563\n",
      "Epoch [416/500], Loss: 0.7556\n",
      "total loss: 1.5110575556755066\n",
      "Epoch [417/500], Loss: 0.7555\n",
      "total loss: 1.511902093887329\n",
      "Epoch [418/500], Loss: 0.7560\n",
      "total loss: 1.5104064345359802\n",
      "Epoch [419/500], Loss: 0.7552\n",
      "total loss: 1.5109643936157227\n",
      "Epoch [420/500], Loss: 0.7555\n",
      "total loss: 1.510661005973816\n",
      "Epoch [421/500], Loss: 0.7553\n",
      "total loss: 1.5106483101844788\n",
      "Epoch [422/500], Loss: 0.7553\n",
      "total loss: 1.5107327699661255\n",
      "Epoch [423/500], Loss: 0.7554\n",
      "total loss: 1.510624885559082\n",
      "Epoch [424/500], Loss: 0.7553\n",
      "total loss: 1.5108989477157593\n",
      "Epoch [425/500], Loss: 0.7554\n",
      "total loss: 1.5107596516609192\n",
      "Epoch [426/500], Loss: 0.7554\n",
      "total loss: 1.5101982355117798\n",
      "Epoch [427/500], Loss: 0.7551\n",
      "total loss: 1.510197937488556\n",
      "Epoch [428/500], Loss: 0.7551\n",
      "total loss: 1.510316550731659\n",
      "Epoch [429/500], Loss: 0.7552\n",
      "total loss: 1.5109626650810242\n",
      "Epoch [430/500], Loss: 0.7555\n",
      "total loss: 1.51063734292984\n",
      "Epoch [431/500], Loss: 0.7553\n",
      "total loss: 1.509989857673645\n",
      "Epoch [432/500], Loss: 0.7550\n",
      "total loss: 1.5114912390708923\n",
      "Epoch [433/500], Loss: 0.7557\n",
      "total loss: 1.511155903339386\n",
      "Epoch [434/500], Loss: 0.7556\n",
      "total loss: 1.510832667350769\n",
      "Epoch [435/500], Loss: 0.7554\n",
      "total loss: 1.5105483531951904\n",
      "Epoch [436/500], Loss: 0.7553\n",
      "total loss: 1.5102080702781677\n",
      "Epoch [437/500], Loss: 0.7551\n",
      "total loss: 1.5098722577095032\n",
      "Epoch [438/500], Loss: 0.7549\n",
      "total loss: 1.5103102326393127\n",
      "Epoch [439/500], Loss: 0.7552\n",
      "total loss: 1.5105754733085632\n",
      "Epoch [440/500], Loss: 0.7553\n",
      "total loss: 1.51089745759964\n",
      "Epoch [441/500], Loss: 0.7554\n",
      "total loss: 1.510574996471405\n",
      "Epoch [442/500], Loss: 0.7553\n",
      "total loss: 1.5110211372375488\n",
      "Epoch [443/500], Loss: 0.7555\n",
      "total loss: 1.510194480419159\n",
      "Epoch [444/500], Loss: 0.7551\n",
      "total loss: 1.5095634460449219\n",
      "Epoch [445/500], Loss: 0.7548\n",
      "total loss: 1.510059893131256\n",
      "Epoch [446/500], Loss: 0.7550\n",
      "total loss: 1.5095999240875244\n",
      "Epoch [447/500], Loss: 0.7548\n",
      "total loss: 1.5099912285804749\n",
      "Epoch [448/500], Loss: 0.7550\n",
      "total loss: 1.5093086957931519\n",
      "Epoch [449/500], Loss: 0.7547\n",
      "total loss: 1.5101373791694641\n",
      "Epoch [450/500], Loss: 0.7551\n",
      "total loss: 1.510019063949585\n",
      "Epoch [451/500], Loss: 0.7550\n",
      "total loss: 1.5097113251686096\n",
      "Epoch [452/500], Loss: 0.7549\n",
      "total loss: 1.5094398856163025\n",
      "Epoch [453/500], Loss: 0.7547\n",
      "total loss: 1.5102676749229431\n",
      "Epoch [454/500], Loss: 0.7551\n",
      "total loss: 1.5100343823432922\n",
      "Epoch [455/500], Loss: 0.7550\n",
      "total loss: 1.5095731616020203\n",
      "Epoch [456/500], Loss: 0.7548\n",
      "total loss: 1.5106276869773865\n",
      "Epoch [457/500], Loss: 0.7553\n",
      "total loss: 1.5095091462135315\n",
      "Epoch [458/500], Loss: 0.7548\n",
      "total loss: 1.5101728439331055\n",
      "Epoch [459/500], Loss: 0.7551\n",
      "total loss: 1.5098005533218384\n",
      "Epoch [460/500], Loss: 0.7549\n",
      "total loss: 1.5094861388206482\n",
      "Epoch [461/500], Loss: 0.7547\n",
      "total loss: 1.5095030665397644\n",
      "Epoch [462/500], Loss: 0.7548\n",
      "total loss: 1.5093061923980713\n",
      "Epoch [463/500], Loss: 0.7547\n",
      "total loss: 1.5094147324562073\n",
      "Epoch [464/500], Loss: 0.7547\n",
      "total loss: 1.5096198320388794\n",
      "Epoch [465/500], Loss: 0.7548\n",
      "total loss: 1.5097576379776\n",
      "Epoch [466/500], Loss: 0.7549\n",
      "total loss: 1.509630799293518\n",
      "Epoch [467/500], Loss: 0.7548\n",
      "total loss: 1.5093077421188354\n",
      "Epoch [468/500], Loss: 0.7547\n",
      "total loss: 1.5095884203910828\n",
      "Epoch [469/500], Loss: 0.7548\n",
      "total loss: 1.50909423828125\n",
      "Epoch [470/500], Loss: 0.7545\n",
      "total loss: 1.5099664330482483\n",
      "Epoch [471/500], Loss: 0.7550\n",
      "total loss: 1.5096755623817444\n",
      "Epoch [472/500], Loss: 0.7548\n",
      "total loss: 1.5095939636230469\n",
      "Epoch [473/500], Loss: 0.7548\n",
      "total loss: 1.509420096874237\n",
      "Epoch [474/500], Loss: 0.7547\n",
      "total loss: 1.5091122388839722\n",
      "Epoch [475/500], Loss: 0.7546\n",
      "total loss: 1.5092819333076477\n",
      "Epoch [476/500], Loss: 0.7546\n",
      "total loss: 1.5095112919807434\n",
      "Epoch [477/500], Loss: 0.7548\n",
      "total loss: 1.5091660022735596\n",
      "Epoch [478/500], Loss: 0.7546\n",
      "total loss: 1.5083581805229187\n",
      "Epoch [479/500], Loss: 0.7542\n",
      "total loss: 1.5086157321929932\n",
      "Epoch [480/500], Loss: 0.7543\n",
      "total loss: 1.5088921189308167\n",
      "Epoch [481/500], Loss: 0.7544\n",
      "total loss: 1.5083729028701782\n",
      "Epoch [482/500], Loss: 0.7542\n",
      "total loss: 1.5088109374046326\n",
      "Epoch [483/500], Loss: 0.7544\n",
      "total loss: 1.5095800757408142\n",
      "Epoch [484/500], Loss: 0.7548\n",
      "total loss: 1.5088468194007874\n",
      "Epoch [485/500], Loss: 0.7544\n",
      "total loss: 1.5092253684997559\n",
      "Epoch [486/500], Loss: 0.7546\n",
      "total loss: 1.509419322013855\n",
      "Epoch [487/500], Loss: 0.7547\n",
      "total loss: 1.5081244707107544\n",
      "Epoch [488/500], Loss: 0.7541\n",
      "total loss: 1.5082314610481262\n",
      "Epoch [489/500], Loss: 0.7541\n",
      "total loss: 1.5084009170532227\n",
      "Epoch [490/500], Loss: 0.7542\n",
      "total loss: 1.5086528658866882\n",
      "Epoch [491/500], Loss: 0.7543\n",
      "total loss: 1.5090664625167847\n",
      "Epoch [492/500], Loss: 0.7545\n",
      "total loss: 1.5081813335418701\n",
      "Epoch [493/500], Loss: 0.7541\n",
      "total loss: 1.5084377527236938\n",
      "Epoch [494/500], Loss: 0.7542\n",
      "total loss: 1.5091550946235657\n",
      "Epoch [495/500], Loss: 0.7546\n",
      "total loss: 1.5081751942634583\n",
      "Epoch [496/500], Loss: 0.7541\n",
      "total loss: 1.5090122818946838\n",
      "Epoch [497/500], Loss: 0.7545\n",
      "total loss: 1.5080145001411438\n",
      "Epoch [498/500], Loss: 0.7540\n",
      "total loss: 1.5086328983306885\n",
      "Epoch [499/500], Loss: 0.7543\n",
      "total loss: 1.508802354335785\n",
      "Epoch [500/500], Loss: 0.7544\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGwCAYAAABVdURTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA+PElEQVR4nO3deXSU9d3//9c1CVlYMkmAkAQChEUwRCKghKjVoiBbo6j3zw0LtXdVEKyK3pZUENBauijiQrGu1KJSNyhWRBEXjAKRJQKyCBgJQkKAkBUSSOb6/pFfpgxZyCSz5/k4Z85hrvnM5D0XaF7nsxqmaZoCAAAIEBZvFwAAAOBKhBsAABBQCDcAACCgEG4AAEBAIdwAAICAQrgBAAABhXADAAACSrC3C/A0m82mQ4cOqUOHDjIMw9vlAACAJjBNU6WlpYqPj5fF0njfTKsLN4cOHVJCQoK3ywAAAM1w4MABdevWrdE2rS7cdOjQQVLNzYmIiPByNQAAoClKSkqUkJBg/z3emFYXbmqHoiIiIgg3AAD4maZMKWFCMQAACCiEGwAAEFAINwAAIKAQbgAAQEAh3AAAgIBCuAEAAAGFcAMAAAIK4QYAAAQUwg0AAAgorW6HYneptpnKyilUQWmFYjqEaWhitIIsHMwJAICnEW5cYNX2PM19f4fyiivs1+KsYZqdnqTRyXFerAwAgNaHYakWWrU9T1OWbHYINpKUX1yhKUs2a9X2PC9VBgBA60S4aYFqm6m57++QWc9rtdfmvr9D1bb6WgAAAHcg3LRAVk5hnR6bM5mS8oorlJVT6LmiAABo5Qg3LVBQ2nCwaU47AADQcoSbFojpEObSdgAAoOUINy0wNDFacdYwNbTg21DNqqmhidGeLAsAgFaNcNMCQRZDs9OTJKlOwKl9Pjs9if1uAADwIMJNC41OjtOi2wYr1uo49BRrDdOi2wazzw0AAB7GJn4uMDo5TiOTYtmhGAAAH0C4cZEgi6G03h29XQYAAK0ew1IAACCgEG4AAEBAIdwAAICAQrgBAAABhXADAAACCuEGAAAEFMINAAAIKIQbAAAQUAg3AAAgoBBuAABAQCHcAACAgEK4AQAAAYVwAwAAAgrhBgAABBTCDQAACCjB3i4gkFTbTGXlFKqgtEIxHcI0NDFaQRbD22UBANCqEG5cZNX2PM19f4fyiivs1+KsYZqdnqTRyXFerAwAgNaFYSkXWLU9T1OWbHYINpKUX1yhKUs2a9X2PC9VBgBA6+PVcLN27Vqlp6crPj5ehmFo+fLlTX7vV199peDgYF144YVuq68pqm2m5r6/Q2Y9r9Vem/v+DlXb6msBAABczavhpry8XCkpKVq4cKFT7ysqKtLEiRN11VVXuamypsvKKazTY3MmU1JecYWycgo9VxQAAK2YV+fcjBkzRmPGjHH6fZMnT9att96qoKAgp3p73KGgtOFg05x2AACgZfxuzs2rr76qH374QbNnz25S+8rKSpWUlDg8XCmmQ5hL2wEAgJbxq3CzZ88ezZgxQ0uWLFFwcNM6nebNmyer1Wp/JCQkuLSmoYnRirOGqaEF34ZqVk0NTYx26c8FAAD185twU11drVtvvVVz587Veeed1+T3ZWRkqLi42P44cOCAS+sKshianZ4kSXUCTu3z2elJ7HcDAICHGKZp+sQyHsMwtGzZMo0fP77e14uKihQVFaWgoCD7NZvNJtM0FRQUpI8//lhXXnnlOX9OSUmJrFariouLFRER4ary2ecGAAA3cub3t99s4hcREaFt27Y5XPvb3/6mTz/9VO+8844SExO9VFmN0clxGpkUyw7FAAB4mVfDTVlZmfbu3Wt/npOTo+zsbEVHR6t79+7KyMjQwYMH9dprr8lisSg5Odnh/TExMQoLC6tz3VuCLIbSenf0dhkAALRqXg03Gzdu1PDhw+3Pp0+fLkmaNGmSFi9erLy8POXm5nqrPAAA4Id8Zs6Np7hrzg0AAHAfZ35/+81qKQAAgKYg3AAAgIBCuAEAAAGFcAMAAAKK3+xz4y+qbSZ73QAA4EWEGxdil2IAALyPYSkXWbU9T1OWbHYINpKUX1yhKUs2a9X2PC9VBgBA60K4cYFqm6m57+9QfRsG1V6b+/4OVdta1ZZCAAB4BeHGBbJyCuv02JzJlJRXXKGsnELPFQUAQCtFuHGBgtKGg01z2gEAgOYj3LhATIcwl7YDAADNR7hxgaGJ0YqzhqmhBd+GalZNDU2M9mRZAAC0SoQbFwiyGJqdniRJdQJO7fPZ6UnsdwMAgAcQblxkdHKcFt02WLFWx6GnWGuYFt02mH1uAADwEDbxc6HRyXEamRTLDsUAAHgR4cbFgiyG0np39HYZAAC0WgxLAQCAgEK4AQAAAYVhKTfgZHAAALyHcONinAwOAIB3MSzlQpwMDgCA9xFuXISTwQEA8A2EGxfhZHAAAHwD4cZFOBkcAADfQLhxEU4GBwDANxBuXISTwQEA8A2EGxfhZHAAAHwD4caFOBkcAADvYxM/F+NkcAAAvItw4wacDA4AgPcwLAUAAAIKPTduwuGZAAB4B+HGDTg8EwAA72FYysU4PBMAAO8i3LgQh2cCAOB9hBsX4vBMAAC8j3DjQhyeCQCA93k13Kxdu1bp6emKj4+XYRhavnx5o+0zMzN16aWXqmPHjgoPD1f//v311FNPeabYJuDwTAAAvM+rq6XKy8uVkpKiX//617r++uvP2b5du3aaNm2aBg4cqHbt2ikzM1N33XWX2rVrpzvvvNMDFTeu9vDM/OKKeufdGKo5ioHDMwEAcB/DNE2fmN1qGIaWLVum8ePHO/W+66+/Xu3atdM///nPel+vrKxUZWWl/XlJSYkSEhJUXFysiIiIlpRcr9rVUpIcAk7tDjecMQUAgPNKSkpktVqb9Pvbr+fcbNmyRV9//bWuuOKKBtvMmzdPVqvV/khISHBrTRyeCQCAd/llz023bt105MgRVVVVac6cOZo1a1aDbT3dc1OLHYoBAHAdZ3pu/HKH4i+//FJlZWVav369ZsyYoT59+uiWW26pt21oaKhCQ0M9XCGHZwIA4C1+GW4SExMlSRdccIEOHz6sOXPmNBhuAABA6+KX4eZMNpvNYdjJVzAsBQCAd3g13JSVlWnv3r325zk5OcrOzlZ0dLS6d++ujIwMHTx4UK+99pokaeHCherevbv69+8vqWafnCeeeEK//e1vvVJ/Qzg4EwAA7/FquNm4caOGDx9ufz59+nRJ0qRJk7R48WLl5eUpNzfX/rrNZlNGRoZycnIUHBys3r17689//rPuuusuj9fekNql4GfP0q49OJMVUwAAuJfPrJbyFGdmWzur2mbqsj9/2uD5UrWb+GX+7kqGqAAAcEKr2efG13BwJgAA3ke4cSEOzgQAwPsINy7EwZkAAHgf4caFag/ObGg2jaGaVVMcnAkAgPsQblwoyGJodnqSJNUJOLXPZ6cnMZkYAAA3Ity4GAdnAgDgXX6/Q7EvGp0cp5FJsexQDACAFxBu3ISDMwEA8A6GpQAAQEAh3AAAgIBCuAEAAAGFcAMAAAIK4QYAAAQUwg0AAAgohBsAABBQ2OfGjaptJhv5AQDgYYQbN1m1PU9z39+hvOIK+7U4a5hmpydxBAMAAG7EsJQbrNqepylLNjsEG0nKL67QlCWbtWp7npcqAwAg8BFuXKzaZmru+ztk1vNa7bW57+9Qta2+FgAAoKUINy6WlVNYp8fmTKakvOIKZeUUeq4oAABaEcKNixWUNhxsmtMOAAA4h3DjYjEdwlzaDgAAOIdw42JDE6MVZw1TQwu+DdWsmhqaGO3JsgAAaDUINy4WZDE0Oz1JkuoEnNrns9OT2O8GAAA3Idy4wejkOC26bbBirY5DT7HWMC26bTD73AAA4EZs4ucmo5PjNDIplh2KAQDwMMKNGwVZDKX17ujtMgAAaFUYlgIAAAGFnhs34/BMAAA8i3DjRhyeCQCA5zEs5SYcngkAgHcQbtyAwzMBAPAewo0bcHgmAADeQ7hxAw7PBADAewg3bsDhmQAAeA/hxg04PBMAAO/xarhZu3at0tPTFR8fL8MwtHz58kbbv/feexo5cqQ6d+6siIgIpaWl6aOPPvJMsU7g8EwAALzHq+GmvLxcKSkpWrhwYZPar127ViNHjtTKlSu1adMmDR8+XOnp6dqyZYubK3Ueh2cCAOAdhmmaPrEe2TAMLVu2TOPHj3fqfQMGDNBNN92kRx55pEntS0pKZLVaVVxcrIiIiGZU6hx2KAYAoOWc+f3t13NubDabSktLFR3d8NyVyspKlZSUODw8KchiaGhitGI6hKmgtGb5N/vbAADgPn59/MITTzyhsrIy3XjjjQ22mTdvnubOnevBqhxxBAMAAJ7ltz03b7zxhubOnau33npLMTExDbbLyMhQcXGx/XHgwAGP1cgRDAAAeJ7T4ebAgQP66aef7M+zsrJ033336YUXXnBpYY1ZunSpfvOb3+itt97SiBEjGm0bGhqqiIgIh4cncAQDAADe4XS4ufXWW/XZZ59JkvLz8zVy5EhlZWXp4Ycf1qOPPuryAs/25ptv6vbbb9ebb76pcePGuf3nNRdHMAAA4B1Oh5vt27dr6NChkqS33npLycnJ+vrrr/X6669r8eLFTn1WWVmZsrOzlZ2dLUnKyclRdna2cnNzJdUMKU2cONHe/o033tDEiRP15JNPKjU1Vfn5+crPz1dxcbGzX8PtOIIBAADvcDrcnD59WqGhoZKkTz75RNdcc40kqX///srLc24OycaNGzVo0CANGjRIkjR9+nQNGjTIvqw7Ly/PHnQk6YUXXlBVVZWmTp2quLg4++Pee+919mu4HUcwAADgHU6vlhowYICef/55jRs3TqtXr9Zjjz0mSTp06JA6duzo1Gf9/Oc/V2Pb7JzdE/T55587W67X1B7B0NjQlCQdLz/loYoAAGgdnO65+fOf/6y///3v+vnPf65bbrlFKSkpkqQVK1bYh6tQs7/NrHHnn7PdYx8wqRgAAFdyuufm5z//uY4ePaqSkhJFRUXZr995551q27atS4vzd1HtQs/ZpnZScVpv53q9AABA/ZzuuTl58qQqKyvtwWb//v1asGCBdu/e3eh+M60Rk4oBAPA8p8PNtddeq9dee02SVFRUpNTUVD355JMaP368Fi1a5PIC/RmTigEA8Dynw83mzZv1s5/9TJL0zjvvqEuXLtq/f79ee+01PfPMMy4v0J/VTipu6JhMQzVHMQxNbPhsLAAA4Bynw82JEyfUoUMHSdLHH3+s66+/XhaLRcOGDdP+/ftdXqA/C7IYmp2eJEl1Ak7t89npSZwSDgCACzkdbvr06aPly5frwIED+uijj3T11VdLkgoKCjx2tIE/GZ0cp0W3DVas1XHoKapdGy28dRCHZwIA4GJOh5tHHnlEDz74oHr27KmhQ4cqLS1NUk0vTu1mfHA0OjlOs8YlKbpdiP1aYflpPfbBTg7PBADAxQyzsV30GpCfn6+8vDylpKTIYqnJR1lZWYqIiFD//v1dXqQrlZSUyGq1qri42GM9TbWng599o2sHoxbdNpgeHAAAGuHM72+n97mRpNjYWMXGxtpPB+/WrRsb+DXgXKeDG6o5HXxkUixzbwAAcAGnh6VsNpseffRRWa1W9ejRQz169FBkZKQee+wx2Ww2d9To1zgdHAAAz3K65+bhhx/Wyy+/rD/96U+69NJLJUmZmZmaM2eOKioq9Pjjj7u8SH/GRn4AAHiW0+HmH//4h1566SX7aeCSNHDgQHXt2lV333034eYsbOQHAIBnOT0sVVhYWO+k4f79+6uwkKGVs7GRHwAAnuV0uElJSdFzzz1X5/pzzz1nPyEc/9XYRn5SzZwbNvIDAMB1nB6W+stf/qJx48bpk08+se9xs27dOh04cEArV650eYGBoHYjvxnvbVPRidMOr0W2beOlqgAACExO99xcccUV+v7773XdddepqKhIRUVFuv7667V79277mVOoX/FZwab22pQlm9nMDwAAF2nWJn7+zBub+FXbTF32508bXBJuSIq1hinzd1cyPAUAQD1cvonf1q1bm/zDBw4c2OS2rYUze92k9e7oucIAAAhATQo3F154oQzD0Lk6eQzDUHV1tUsKCyTsdQMAgOc0Kdzk5OS4u46Axl43AAB4TpPCTY8ePdxdR0Cr3esmv7ii3jOmpJpVU+x1AwBAyzm9WgrOq93rprFBvaITp7V6R77HagIAIFARbjxkZFJso3va1J4OXm1rVYvXAABwOcKNh2TlFNbZwO9MnA4OAIBrEG48hBVTAAB4RrPCTVFRkV566SVlZGTYD8vcvHmzDh486NLiAgkrpgAA8Aynz5baunWrRowYIavVqh9//FF33HGHoqOj9d577yk3N1evvfaaO+r0e+daMVW7SzErpgAAaBmne26mT5+uX/3qV9qzZ4/Cwv7byzB27FitXbvWpcUFEk4HBwDAM5wON998843uuuuuOte7du2q/HyWMjem9nRwaz2rpjgdHAAA13A63ISGhqqkpKTO9e+//16dO3d2SVGBjtPBAQBwH6fDzTXXXKNHH31Up0/X/II2DEO5ubn63e9+pxtuuMHlBQaSapupue/vqHfOTe019roBAKBlnA43Tz75pMrKyhQTE6OTJ0/qiiuuUJ8+fdShQwc9/vjj7qgxYDhzOjgAAGgep1dLWa1WrV69WpmZmdq6davKyso0ePBgjRgxwh31BRT2ugEAwP2cDje1LrvsMl122WWurCXgsdcNAADu53S4eeaZZ+q9bhiGwsLC1KdPH11++eUKCgpqcXGBpnavm8aGpiTpePkpD1UEAEDgMUzTdGr2amJioo4cOaITJ04oKipKknT8+HG1bdtW7du3V0FBgXr16qXPPvtMCQkJbim6JUpKSmS1WlVcXKyIiAiP//yVWw/p7je2NNomzhqmzN9dyZ43AAD8/5z5/e30hOI//vGPuvjii7Vnzx4dO3ZMx44d0/fff6/U1FQ9/fTTys3NVWxsrO6///5zftbatWuVnp6u+Ph4GYah5cuXN9o+Ly9Pt956q8477zxZLBbdd999zpbvdVHtQs/ZhknFAAA0n9PhZubMmXrqqafUu3dv+7U+ffroiSeeUEZGhrp166a//OUv+uqrr875WeXl5UpJSdHChQub9LMrKyvVuXNnzZw5UykpKc6W7hOYVAwAgHs5PecmLy9PVVVVda5XVVXZdyiOj49XaWnpOT9rzJgxGjNmTJN/ds+ePfX0009Lkl555ZUmvaeyslKVlZX25/VtQOhJTCoGAMC9nO65GT58uO666y5t2fLfeSNbtmzRlClTdOWVV0qStm3bpsTERNdV2QLz5s2T1Wq1P7w9D6h2UvG5MKkYAIDmcTrcvPzyy4qOjtaQIUMUGhqq0NBQXXTRRYqOjtbLL78sSWrfvr2efPJJlxfbHBkZGSouLrY/Dhw44NV6giyGZo07/5ztHvuAnYoBAGgOp4elYmNjtXr1au3atUvff/+9JKlfv37q16+fvc3w4cNdV2EL1QYwX+LMpOK03h09UBEAAIGj2Zv49e/fX/3793dlLa0Gk4oBAHCfZoWbn376SStWrFBubq5OnXKcGzJ//nyXFBbImFQMAID7OB1u1qxZo2uuuUa9evXSrl27lJycrB9//FGmaWrw4MFOfVZZWZn27t1rf56Tk6Ps7GxFR0ere/fuysjI0MGDB/Xaa6/Z22RnZ9vfe+TIEWVnZyskJERJSUnOfhWvGZoYrci2bVR04nSDbSLbttHQxGgPVgUAQGBwOtxkZGTowQcf1Ny5c9WhQwe9++67iomJ0YQJEzR69GinPmvjxo0O83OmT58uSZo0aZIWL16svLw85ebmOrxn0KBB9j9v2rRJb7zxhnr06KEff/zR2a/i09ibGACA5nH6+IUOHTooOztbvXv3VlRUlDIzMzVgwAB9++23uvbaa30+ZHj7+AVJWrfvmG55cf052715xzAmFAMAIDcfv9CuXTv7PJu4uDjt27fP/trRo0ed/bhWiQnFAAC4j9PDUsOGDVNmZqbOP/98jR07Vg888IC2bdum9957T8OGDXNHjQGHCcUAALiP0+Fm/vz5KisrkyTNnTtXZWVl+te//qW+ffuyUqqJancpzi+uUENjgkwoBgCgeZwKN9XV1frpp580cOBASTVDVM8//7xbCgtkQRZDs9OTNHnJ5gbbFJ04rdU78jU6Oc6DlQEA4P+cmnMTFBSkq6++WsePH3dXPa3GyKRYRbZt0+DrhqS573MEAwAAznJ6QnFycrJ++OEHd9TSqmTlFDa6z42p/x7BAAAAms7pcPOHP/xBDz74oP7zn/8oLy9PJSUlDg80DSumAABwD6cnFI8dO1aSdM0118gw/rvVnGmaMgxD1dXVrqsugLFiCgAA93A63Hz22WfuqKPVacqKqThrGCumAABwktPh5oorrnBHHa1O7YqpKY2smLomJU5BFg5iAADAGU7PuZGkL7/8UrfddpsuueQSHTx4UJL0z3/+U5mZmS4tLtCNTo7TnZcnNvj6C2tztGp7ngcrAgDA/zkdbt59912NGjVK4eHh2rx5syorKyVJxcXF+uMf/+jyAgNZtc3Uim8bDy8sBwcAwDnNWi31/PPP68UXX1SbNv/dp+XSSy/V5s0ND7GgrqycQuUVN7waiuXgAAA4z+lws3v3bl1++eV1rlutVhUVFbmiplaD5eAAALie0+EmNjZWe/furXM9MzNTvXr1cklRrQXLwQEAcD2nw80dd9yhe++9Vxs2bJBhGDp06JBef/11Pfjgg5oyZYo7agxYtcvBG1sPxXJwAACc4/RS8BkzZshms+mqq67SiRMndPnllys0NFQPPvig7rnnHnfUGLBYDg4AgOsZpmk2aynOqVOntHfvXpWVlSkpKUnt27d3dW1uUVJSIqvVquLiYkVERHi7HEnSvJU79Pe1OfW+ZkhadNtgTgcHALRqzvz+dnpYasmSJTpx4oRCQkKUlJSkoUOH+k2w8UUsBwcAwLWcDjf333+/YmJidOutt2rlypWcJdVCLAcHAMC1nA43eXl5Wrp0qQzD0I033qi4uDhNnTpVX3/9tTvqC3gsBwcAwLWcDjfBwcH6xS9+oddff10FBQV66qmn9OOPP2r48OHq3bu3O2oMaCwHBwDAtZxeLXWmtm3batSoUTp+/Lj279+vnTt3uqquVuNcp4MbkmJZDg4AQJM16+DMEydO6PXXX9fYsWPVtWtXLViwQNddd52+++47V9cX8GqXg0uqs99N7fPZ6UksBwcAoImcDjc333yzYmJidP/996tXr176/PPPtXfvXj322GPq37+/O2oMeKOT47TotsGKtToOPUW1a6OFtw5iGTgAAE5welgqKChIb731lkaNGqWgoCCH17Zv367k5GSXFdeajE6Ok80mzfz3dhWWn5IkFZaf1mMf7JTFYhBwAABoomZv4lertLRUb775pl566SVt2rTJ55eG++ImfpK0anuepizZXGfeTe1gFBv5AQBaM7du4ldr7dq1mjRpkuLi4vTEE0/oyiuv1Pr165v7ca1atc3U3Pd31DuhuPYaG/kBANA0Tg1L5efna/HixXr55ZdVUlKiG2+8UZWVlVq+fLmSkpLcVWPAc2Yjv7TeHT1XGAAAfqjJPTfp6enq16+ftm7dqgULFujQoUN69tln3Vlbq8FGfgAAuE6Te24+/PBD/fa3v9WUKVPUt29fd9bU6rCRHwAArtPknpvMzEyVlpZqyJAhSk1N1XPPPaejR4+6s7ZWo3Yjv4Z2sjEkxbGRHwAATdLkcDNs2DC9+OKLysvL01133aWlS5cqPj5eNptNq1evVmlpqTvrDGhs5AcAgOu0aCn47t279fLLL+uf//ynioqKNHLkSK1YscKV9bmcry4Fl2qWg899f4fD5OI4a5hmpyexDBwA0Ko58/u7xfvcSFJ1dbXef/99vfLKK4SbFqq2mVq/75jW/XBUkqG03h01rFdHem0AAK2ax8ONP/H1cEPvDQAAdXlkEz9XWLt2rdLT0xUfHy/DMLR8+fJzvufzzz/X4MGDFRoaqj59+mjx4sVur9NTancpPnvPm/ziCk1Zslmrtud5qTIAAPyHV8NNeXm5UlJStHDhwia1z8nJ0bhx4zR8+HBlZ2frvvvu029+8xt99NFHbq7U/c61S7EpdikGAKApnD4405XGjBmjMWPGNLn9888/r8TERD355JOSpPPPP1+ZmZl66qmnNGrUKHeV6RHn2qVYYpdiAACawqs9N85at26dRowY4XBt1KhRWrduXYPvqaysVElJicPDFzV19+HVO/LdXAkAAP7Nr8JNfn6+unTp4nCtS5cuKikp0cmTJ+t9z7x582S1Wu2PhIQET5TqtKbuPvzv7EMMTQEA0Ai/CjfNkZGRoeLiYvvjwIED3i6pXkMToxXdrs052x0rP6WsnEIPVAQAgH/yq3ATGxurw4cPO1w7fPiwIiIiFB4eXu97QkNDFRER4fDwRUEWQ9dd2LVJbTlAEwCAhvlVuElLS9OaNWscrq1evVppaWleqsi1RiTFNqkdB2gCANAwr4absrIyZWdnKzs7W1LNUu/s7Gzl5uZKqhlSmjhxor395MmT9cMPP+ihhx7Srl279Le//U1vvfWW7r//fm+U73K1B2iey/HyUx6oBgAA/+TVcLNx40YNGjRIgwYNkiRNnz5dgwYN0iOPPCJJysvLswcdSUpMTNQHH3yg1atXKyUlRU8++aReeuklv18GXivIYmjWuPPP2e6xD9jvBgCAhnD8go9Zt++Ybnlx/TnbvXnHMPa7AQC0Gn5z/ALqaupkYSYVAwBQP8KNj2nqZGEmFQMAUD/CjY+pnVRsNPC6oZpTwocmRnuyLAAA/AbhxscEWQzNTk+SpHoDjilpdnqSgiwNxR8AAFo3wo0PGp0cp0W3DZa1bd0diyPruQYAAP6LcOPDik+crvfalCWbtWp7nhcqAgDA9xFufFC1zdTc93eovjX6tdfmvs9eNwAA1Idw44OycgqVV9zwUm9TUl5xBQdoAgBQD8KND2KvGwAAmo9w44PY6wYAgOYj3Pigc+11I7HXDQAADSHc+KAz97ppyDUpcex1AwBAPQg3Pmp0cpzuvDyxwddfWJvDcnAAAOpBuPFR1TZTK75tPLywHBwAgLoINz6K5eAAADQP4cZHsRwcAIDmIdz4KJaDAwDQPIQbH1W7HLwxLAcHAKAuwo2PCrIYuiYlrtE2LAcHAKAuwo2PaspqqRXf5rFaCgCAsxBufNS5VktJrJYCAKA+hBsfxWopAACah3Djo1gtBQBA8xBufFRTDs+MbNuG1VIAAJyFcOOjag/PbGy6cNGJ01q9I99jNQEA4A8INz5sZFKsItu2afB1Q5wvBQDA2Qg3Piwrp1BFJ043+DrnSwEAUBfhxoexYgoAAOcRbnwYK6YAAHAe4caHNeV8KUk6Xn7KA9UAAOAfCDc+LMhiaNa488/Z7rEPmFQMAEAtwo2Pi2oXes42TCoGAOC/CDc+rqmThdnvBgCAGoQbH9fUycL/zj7E0BQAACLc+LyhidGKbtfwRn61jpWfYmgKAAARbnxekMXQdRd2bVJbhqYAACDc+IURSbFNavfWxp8YmgIAtHo+EW4WLlyonj17KiwsTKmpqcrKymqw7enTp/Xoo4+qd+/eCgsLU0pKilatWuXBaj2vqUNTZZVVeu7TvR6oCAAA3+X1cPOvf/1L06dP1+zZs7V582alpKRo1KhRKigoqLf9zJkz9fe//13PPvusduzYocmTJ+u6667Tli1bPFy55zgzNPXq1zn03gAAWjXDNE2v/iZMTU3VxRdfrOeee06SZLPZlJCQoHvuuUczZsyo0z4+Pl4PP/ywpk6dar92ww03KDw8XEuWLDnnzyspKZHValVxcbEiIiJc90XcbN2+Y7rlxfVNavvmHcOU1rujmysCAMBznPn97dWem1OnTmnTpk0aMWKE/ZrFYtGIESO0bt26et9TWVmpsDDH5dHh4eHKzMxssH1JSYnDwx8NTYxWZPi5h6YkJhYDAFo3r4abo0ePqrq6Wl26dHG43qVLF+Xn1/8LetSoUZo/f7727Nkjm82m1atX67333lNeXl697efNmyer1Wp/JCQkuPx7eEKQxdDtl/ZsUlsmFgMAWjOvz7lx1tNPP62+ffuqf//+CgkJ0bRp03T77bfLYqn/q2RkZKi4uNj+OHDggIcrdp1pV/ZVu9Cgc7ZjYjEAoDXzarjp1KmTgoKCdPjwYYfrhw8fVmxs/cufO3furOXLl6u8vFz79+/Xrl271L59e/Xq1ave9qGhoYqIiHB4+Ksgi6GbL2paz9Pf1+6j9wYA0Cp5NdyEhIRoyJAhWrNmjf2azWbTmjVrlJaW1uh7w8LC1LVrV1VVVendd9/Vtdde6+5yfUJT97w5caqa3hsAQKvk9WGp6dOn68UXX9Q//vEP7dy5U1OmTFF5ebluv/12SdLEiROVkZFhb79hwwa99957+uGHH/Tll19q9OjRstlseuihh7z1FTzKmYnF9N4AAFojr4ebm266SU888YQeeeQRXXjhhcrOztaqVavsk4xzc3MdJgtXVFRo5syZSkpK0nXXXaeuXbsqMzNTkZGRXvoGnuXMxGJ6bwAArZHX97nxNH/d5+ZM1TZTF8z5SCdOVZ+zbUiwRTsfHa0gi+GBygAAcA+/2ecGzRNkMXTX5fVPoD7bqSqbnl2zx80VAQDgOwg3fmralX0VGty0v75Fn+9l7g0AoNUg3PipIIuhK/vHNKltZbWp37652c0VAQDgGwg3fuy2YT2a3PaDbfl6/IMdbqwGAADfQLjxY8N6dWzSjsW1XvwyRyu31n9MBQAAgYJw48eCLIb+esNAp97z0LtbmX8DAAhohBs/N3ZgvNIHNm3XYqnm3KlnWD0FAAhghJsAsODmwU4NTz29Zo/+k33IjRUBAOA9hJsA0JzhqWlLt2jeSiYYAwACD+EmQIwdGK87ftbTqff8fW0OPTgAgIBDuAkgD48boHEXNH3+jVTTg8MKKgBAICHcBJhnbhmstiFNn38jSXe/sVmrthNwAACBgXATYJw5d+pM9/8rmyXiAICAQLgJQNOu7CtreLBT7zl52qYJL6wj4AAA/B7hJgAFWQz92cnVU5K0/sfjGjjnI4aoAAB+jXAToEYnx+n52wY3+eTwWuWnqjV5yWY9/cn39OIAAPwS4SaAjU6O07Y5oxTmZMCRpKc+2aOL/rBaK7eyVBwA4F8INwEuJNii+TemNOu9x0+c1t1vsNkfAMC/EG5ageZs8HcmNvsDAPgTwk0r8fC4Afrfy3o2+/3Tlm4h4AAA/ALhphWZ9YuWB5ypr29kojEAwKcRblqZWb8YoDt+ltjs93+w7bD6z/xQC1bvJuQAAHwS4aYVenhckp67eVCz33/aZmrBmr0a8MgqVlMBAHwO4aaV+sWF8S0KOJJUUWXT3W9s0c1//1qnqmwuqgwAgJYh3LRiv7gwXndd3vwhqlrrc47rvJkfMh8HAOATDNM0W9Vvo5KSElmtVhUXFysiIsLb5fiElVvz9NC736qssrrFn2WRdPWALvplWk8N69VRQRaj5QUCAFo9Z35/E24gSTpVZdPgx1arrLLKZZ/ZLjRIf71hoMYOjHfZZwIAWidnfn8zLAVJNTsZP/H/OX/YZmPKK6t19xtbNH7hl/pq71GGrAAAHkHPDRys2p6nOSt2KL+kwuWf3SbI0JTLe+nekf0YrgIAOIVhqUYQbs6t2mYqK6dQH3+XpyUbcnW62rX/RIIMaWJaD109IE5DE6MJOgCAcyLcNIJw45xqm6ln1+zRc5/uUZUb/qW0Cw3SHZcl6p6rziPkAAAaRLhpBOGmeaptpu59c4v+sy3PLZ9vkZTcLULpA7tq0iU9FRLMdDAAwH8RbhpBuGmZlVvzdO/SLTrt5snB4y7oomduGUJvDgBAEuGmUYSblqu2mXp69fda+MVeVbtxY2L2zAEA1CLcNIJw4zq183Ge/2KfKtx8/EKwIQ3qEaWhidG6pHcnwg4AtDKEm0YQblyv2mZq/b5j+uvHu5R9oNgjPzPYIqVfEKf46LYyZCitd0cCDwAEML/bxG/hwoXq2bOnwsLClJqaqqysrEbbL1iwQP369VN4eLgSEhJ0//33q6LC9fuyoGmCLIYu7dtJy6depr/dOljtQ4Pd/jOrbNKyb/O08LN9eu6zvZrw0gYlPfKhFqzezWaBANDKeb3n5l//+pcmTpyo559/XqmpqVqwYIHefvtt7d69WzExMXXav/HGG/r1r3+tV155RZdccom+//57/epXv9LNN9+s+fPnn/Pn0XPjft7oyTkTK68AIPD41bBUamqqLr74Yj333HOSJJvNpoSEBN1zzz2aMWNGnfbTpk3Tzp07tWbNGvu1Bx54QBs2bFBmZuY5fx7hxrNOVdk08eUNWp9T6LUaenduq9HJcczVAQA/5szvb/ePHzTi1KlT2rRpkzIyMuzXLBaLRowYoXXr1tX7nksuuURLlixRVlaWhg4dqh9++EErV67UL3/5y3rbV1ZWqrKy0v68pKTEtV8CjQoJtmjpXWlauTVPM/+9XYXlpzxew74jJ7Tws31a+Nk+BRvShd0j1S2qrbpGhRN4ACAAeTXcHD16VNXV1erSpYvD9S5dumjXrl31vufWW2/V0aNHddlll8k0TVVVVWny5Mn6/e9/X2/7efPmae7cuS6vHc4ZOzBOo5JjlZVTqPzik8rce1T/2ZqnSjevsjpblSlt3F+kjfuLJMkeeFiJBQCBw6vDUocOHVLXrl319ddfKy0tzX79oYce0hdffKENGzbUec/nn3+um2++WX/4wx+UmpqqvXv36t5779Udd9yhWbNm1WlfX89NQkICw1I+oHZuzj/W5eiTnQXylXnAtb07XSPDZRgGPTwA4AP8ZliqU6dOCgoK0uHDhx2uHz58WLGxsfW+Z9asWfrlL3+p3/zmN5KkCy64QOXl5brzzjv18MMPy2JxnDwaGhqq0NBQ93wBtEjtKqtL+3by6J4553J2745EDw8A+BOvhpuQkBANGTJEa9as0fjx4yXVTChes2aNpk2bVu97Tpw4USfABAUFSZJa2ZY9ASXIYui+kefpnqv6av2+Y3pt/Y/6dFeBy08kb4kqU/rmx+P65sfj9rCTkmBVaHCQKqqqFd4mWCndInVpX4IPAHiTV8ONJE2fPl2TJk3SRRddpKFDh2rBggUqLy/X7bffLkmaOHGiunbtqnnz5kmS0tPTNX/+fA0aNMg+LDVr1iylp6fbQw7819m9Oev3HdNX+47oo+/yte/ICW+X56DKlDblOi51/2rfMf3tC8eJy3GRYYpuG6pOHUIVGxGmoYnRBB8AcCOvh5ubbrpJR44c0SOPPKL8/HxdeOGFWrVqlX2ScW5urkNPzcyZM2UYhmbOnKmDBw+qc+fOSk9P1+OPP+6trwA3OTPoPDT6fJ2qsinjva1emYjsrPqGtmqFBhm6oJtVXSPD7deY2wMAruP1fW48jX1u/F+1zVRWTqEKSivUqV2oNuQc0wtrf/D6XB1XMSSdF9NW/eMiFR9Frw8ASH62iZ+nEW4C05krrz7ddURVvrL0ysVCgqSeHdupfViwwoKD1LlDGAEIQKvgN6ulAFdpaK7ONzmFyv6p2KcmJrfEqWrp+4LyRtuEBhtKjo+wT3SuDUHdohn2AtA60HODgBfIYac5aic7x1vDdLTslEMAio8KU2R4iEoqTss0pai2IfQIAfAJ9NwAZzizV0dyDDsHj5/UoaKTrSrw1E52dlawRbqwm1Vdo9rWec0wDPuqsOh2ISo6cUrR7QlFALyDnhtABB53Cg02dEFXa52eok7tQ2XUk3lYOQagPkwobgThBk1F4PG+M4/COJNpmjpadkqV1TZ1iwzXgHirOrYPVWF5pYpOnpYhQ6mJ0bJYDB0tq1SndqGSIR0tq1RMB3qTAH9EuGkE4QYtcXbgqf0lm1dSoQOFJwN2lVagqe1N6hoZbv87bKxHyTRNHSs/rfAQi2I6hCkirI3yik9KoqcJ8BTCTSMIN3AXenpgkdSjY7jirOENDrs1NUw1ZQhPIlyh9SDcNIJwA086O/DU51DRSW0+UKTqwNiDEF5U3zCeK8NUS3q5aiedR4aHqPBEpbb/VKLwEItiI8I1uHuU4iLDNaRHlDbtP66C0gqGD1EH4aYRhBv4omqbqa/3HNU7mw/op+MnVVZZpf2FJ1RxmsSD1uvsTSubG8xcHd7c8VnhbYJ1QdeauWO1Kw4j24bY55GZpmQNb+OwTUN0uxCHeWZpvTs22Ht35s7u/hocCTeNINzAX9T+zyi/+KSOllWq8MQpHSw8Yf+fYXllNQEIgIPa41v6xVrtYepQ0UltO1Ti8P+KhrZ2cEV4c9dQKeGmEYQbBJL6AtChMyY614agH46WM/cHgMdFtm2jP11/gUYnx7X4swg3jSDcoDWqb+7P2QGIXiAA7vL8bYNbHHDYoRiAg7N3aa7P2aety5Dyi05q84HjOlxcqZOnqtSxni7q7YeKte/ICTd/AwD+bO77OzQyKdZj83wINwAk1QSgtN4d61y/4aKEc773VJVN/1z3o3KOlcuQlNItUsdPnLIPk0nnHstn6TwQuPKKK5SVU1jv/2PcgXADoMVCgi3635/1avHnNGXpvFQTlHYfLmMuEeBHCkorPPazCDcAfEZThs/OdOaE6sLymsM6Y9qHymaaWvfDUXtAOnOPlaKT/+1Nkpq2OuRg0Ult/alEp87YjCg02FC3qHDlFp4kYAFNENMhzGM/i3ADwG81NJQmST87r7NLf1ZD+4TU9jZ9ubdAWw8Ue2QvlUNFJ7X1YLEqqwhV8A9x1pr/ZjyFcAMATdBQkHK2t8lVGtoG4Gye3piuvl6uYEtN7xk9XK3X7PQkj24aSLgBAD/UWK+Vt9XXyyXJ4VrtUQtnhrO8ogqH4cMzN60M5B2K80oqdPB4hUMgDBRRbdtonov2uXEG4QYA4FINBa+zr/lqOPOGs+ePRbatewSDIUOpidGyWAwVlFQ4zDOrqrbpvS0/6afjJxUabKkTpgzDUBdrqEpPVqmgpOGtHXx5h2JnEG4AAPAyV/TEXdE/xkXV+D+LtwsAAABwJcINAAAIKIQbAAAQUAg3AAAgoBBuAABAQCHcAACAgEK4AQAAAYVwAwAAAgrhBgAABJRWt0OxadYc3FZSUuLlSgAAQFPV/t6u/T3emFYXbkpLSyVJCQkJXq4EAAA4q7S0VFartdE2htmUCBRAbDabDh06pA4dOsio7zSwZiopKVFCQoIOHDigiIgIl30uHHGfPYd77RncZ8/gPnuOu+61aZoqLS1VfHy8LJbGZ9W0up4bi8Wibt26ue3zIyIi+A/HA7jPnsO99gzus2dwnz3HHff6XD02tZhQDAAAAgrhBgAABBTCjYuEhoZq9uzZCg0N9XYpAY377Dnca8/gPnsG99lzfOFet7oJxQAAILDRcwMAAAIK4QYAAAQUwg0AAAgohBsAABBQCDcusnDhQvXs2VNhYWFKTU1VVlaWt0vyK2vXrlV6erri4+NlGIaWL1/u8LppmnrkkUcUFxen8PBwjRgxQnv27HFoU1hYqAkTJigiIkKRkZH63//9X5WVlXnwW/i+efPm6eKLL1aHDh0UExOj8ePHa/fu3Q5tKioqNHXqVHXs2FHt27fXDTfcoMOHDzu0yc3N1bhx49S2bVvFxMTo//7v/1RVVeXJr+LTFi1apIEDB9o3MUtLS9OHH35of5177B5/+tOfZBiG7rvvPvs17rVrzJkzR4ZhODz69+9vf93n7rOJFlu6dKkZEhJivvLKK+Z3331n3nHHHWZkZKR5+PBhb5fmN1auXGk+/PDD5nvvvWdKMpctW+bw+p/+9CfTarWay5cvN7/99lvzmmuuMRMTE82TJ0/a24wePdpMSUkx169fb3755Zdmnz59zFtuucXD38S3jRo1ynz11VfN7du3m9nZ2ebYsWPN7t27m2VlZfY2kydPNhMSEsw1a9aYGzduNIcNG2Zecskl9terqqrM5ORkc8SIEeaWLVvMlStXmp06dTIzMjK88ZV80ooVK8wPPvjA/P77783du3ebv//97802bdqY27dvN02Te+wOWVlZZs+ePc2BAwea9957r/0699o1Zs+ebQ4YMMDMy8uzP44cOWJ/3dfuM+HGBYYOHWpOnTrV/ry6utqMj483582b58Wq/NfZ4cZms5mxsbHmX//6V/u1oqIiMzQ01HzzzTdN0zTNHTt2mJLMb775xt7mww8/NA3DMA8ePOix2v1NQUGBKcn84osvTNOsua9t2rQx3377bXubnTt3mpLMdevWmaZZE0QtFouZn59vb7No0SIzIiLCrKys9OwX8CNRUVHmSy+9xD12g9LSUrNv377m6tWrzSuuuMIebrjXrjN79mwzJSWl3td88T4zLNVCp06d0qZNmzRixAj7NYvFohEjRmjdunVerCxw5OTkKD8/3+EeW61Wpaam2u/xunXrFBkZqYsuusjeZsSIEbJYLNqwYYPHa/YXxcXFkqTo6GhJ0qZNm3T69GmHe92/f391797d4V5fcMEF6tKli73NqFGjVFJSou+++86D1fuH6upqLV26VOXl5UpLS+Meu8HUqVM1btw4h3sq8e/Z1fbs2aP4+Hj16tVLEyZMUG5uriTfvM+t7uBMVzt69Kiqq6sd/sIkqUuXLtq1a5eXqgos+fn5klTvPa59LT8/XzExMQ6vBwcHKzo62t4Gjmw2m+677z5deumlSk5OllRzH0NCQhQZGenQ9ux7Xd/fRe1rqLFt2zalpaWpoqJC7du317Jly5SUlKTs7GzusQstXbpUmzdv1jfffFPnNf49u05qaqoWL16sfv36KS8vT3PnztXPfvYzbd++3SfvM+EGaKWmTp2q7du3KzMz09ulBKR+/fopOztbxcXFeueddzRp0iR98cUX3i4roBw4cED33nuvVq9erbCwMG+XE9DGjBlj//PAgQOVmpqqHj166K233lJ4eLgXK6sfw1It1KlTJwUFBdWZFX748GHFxsZ6qarAUnsfG7vHsbGxKigocHi9qqpKhYWF/D3UY9q0afrPf/6jzz77TN26dbNfj42N1alTp1RUVOTQ/ux7Xd/fRe1rqBESEqI+ffpoyJAhmjdvnlJSUvT0009zj11o06ZNKigo0ODBgxUcHKzg4GB98cUXeuaZZxQcHKwuXbpwr90kMjJS5513nvbu3euT/6YJNy0UEhKiIUOGaM2aNfZrNptNa9asUVpamhcrCxyJiYmKjY11uMclJSXasGGD/R6npaWpqKhImzZtsrf59NNPZbPZlJqa6vGafZVpmpo2bZqWLVumTz/9VImJiQ6vDxkyRG3atHG417t371Zubq7Dvd62bZtDmFy9erUiIiKUlJTkmS/ih2w2myorK7nHLnTVVVdp27Ztys7Otj8uuugiTZgwwf5n7rV7lJWVad++fYqLi/PNf9Mun6LcCi1dutQMDQ01Fy9ebO7YscO88847zcjISIdZ4WhcaWmpuWXLFnPLli2mJHP+/Pnmli1bzP3795umWbMUPDIy0vz3v/9tbt261bz22mvrXQo+aNAgc8OGDWZmZqbZt29floKfZcqUKabVajU///xzhyWdJ06csLeZPHmy2b17d/PTTz81N27caKalpZlpaWn212uXdF599dVmdna2uWrVKrNz584snT3DjBkzzC+++MLMyckxt27das6YMcM0DMP8+OOPTdPkHrvTmaulTJN77SoPPPCA+fnnn5s5OTnmV199ZY4YMcLs1KmTWVBQYJqm791nwo2LPPvss2b37t3NkJAQc+jQoeb69eu9XZJf+eyzz0xJdR6TJk0yTbNmOfisWbPMLl26mKGhoeZVV11l7t692+Ezjh07Zt5yyy1m+/btzYiICPP22283S0tLvfBtfFd991iS+eqrr9rbnDx50rz77rvNqKgos23btuZ1111n5uXlOXzOjz/+aI4ZM8YMDw83O3XqZD7wwAPm6dOnPfxtfNevf/1rs0ePHmZISIjZuXNn86qrrrIHG9PkHrvT2eGGe+0aN910kxkXF2eGhISYXbt2NW+66SZz79699td97T4bpmmaru8PAgAA8A7m3AAAgIBCuAEAAAGFcAMAAAIK4QYAAAQUwg0AAAgohBsAABBQCDcAACCgEG4AAEBAIdwAgCTDMLR8+XJvlwHABQg3ALzuV7/6lQzDqPMYPXq0t0sD4IeCvV0AAEjS6NGj9eqrrzpcCw0N9VI1APwZPTcAfEJoaKhiY2MdHlFRUZJqhowWLVqkMWPGKDw8XL169dI777zj8P5t27bpyiuvVHh4uDp27Kg777xTZWVlDm1eeeUVDRgwQKGhoYqLi9O0adMcXj969Kiuu+46tW3bVn379tWKFSvc+6UBuAXhBoBfmDVrlm644QZ9++23mjBhgm6++Wbt3LlTklReXq5Ro0YpKipK33zzjd5++2198sknDuFl0aJFmjp1qu68805t27ZNK1asUJ8+fRx+xty5c3XjjTdq69atGjt2rCZMmKDCwkKPfk8ALuCWs8YBwAmTJk0yg4KCzHbt2jk8Hn/8cdM0TVOSOXnyZIf3pKammlOmTDFN0zRfeOEFMyoqyiwrK7O//sEHH5gWi8XMz883TdM04+PjzYcffrjBGiSZM2fOtD8vKyszJZkffvihy74nAM9gzg0AnzB8+HAtWrTI4Vp0dLT9z2lpaQ6vpaWlKTs7W5K0c+dOpaSkqF27dvbXL730UtlsNu3evVuGYejQoUO66qqrGq1h4MCB9j+3a9dOERERKigoaO5XAuAlhBsAPqFdu3Z1holcJTw8vEnt2rRp4/DcMAzZbDZ3lATAjZhzA8AvrF+/vs7z888/X5J0/vnn69tvv1V5ebn99a+++koWi0X9+vVThw4d1LNnT61Zs8ajNQPwDnpuAPiEyspK5efnO1wLDg5Wp06dJElvv/22LrroIl122WV6/fXXlZWVpZdfflmSNGHCBM2ePVuTJk3SnDlzdOTIEd1zzz365S9/qS5dukiS5syZo8mTJysmJkZjxoxRaWmpvvrqK91zzz2e/aIA3I5wA8AnrFq1SnFxcQ7X+vXrp127dkmqWcm0dOlS3X333YqLi9Obb76ppKQkSVLbtm310Ucf6d5779XFF1+stm3b6oYbbtD8+fPtnzVp0iRVVFToqaee0oMPPqhOnTrpf/7nfzz3BQF4jGGapuntIgCgMYZhaNmyZRo/fry3SwHgB5hzAwAAAgrhBgAABBTm3ADweYyeA3AGPTcAACCgEG4AAEBAIdwAAICAQrgBAAABhXADAAACCuEGAAAEFMINAAAIKIQbAAAQUP4fqLLRdzNrnXUAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model\n",
    "trained_model = train_model(input_data, desired_output, 500, learning_rate=0.0005, viz_accuracy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(trained_model, new_data, expected_result):\n",
    "    test_input = torch.tensor(new_data, dtype=torch.float32)\n",
    "\n",
    "    test_output = trained_model(test_input)\n",
    "\n",
    "    predicted_classes = torch.argmax(test_output, dim=1)\n",
    "\n",
    "    test_output_compare = torch.tensor(expected_result, dtype=torch.int)\n",
    "\n",
    "    return float(sum(test_output_compare == predicted_classes)/len(test_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "just_input_test = test_df.drop('first_move_by_Q_learning', axis=1)\n",
    "just_input_test = just_input_test.drop('num_moves_Q_learning_needs', axis=1)\n",
    "\n",
    "input_data_test = np.array(just_input_test.values.tolist())\n",
    "desired_output = torch.tensor(test_df['first_move_by_Q_learning'].tolist(), dtype=torch.float32).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16089\\AppData\\Local\\Temp\\ipykernel_27204\\875939923.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_output_compare = torch.tensor(expected_result, dtype=torch.int)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9981895089149475"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(trained_model, input_data, torch.tensor(train['first_move_by_Q_learning'].tolist(), dtype=torch.float32).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16089\\AppData\\Local\\Temp\\ipykernel_27204\\875939923.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_output_compare = torch.tensor(expected_result, dtype=torch.int)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9981900453567505"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(trained_model, input_data_test, desired_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1, 2], [0, 1]])\n",
    "B = np.array([[1, 0], [2, 1]])\n",
    "\n",
    "# elements on the diagonal are 1 mod 4. \n",
    "# elements not on the diagonal are 0 mod 2. \n",
    "\n",
    "# C is the inverse of A\n",
    "# D is the inverse of B\n",
    "C = np.array([[1, -2], [0, 1]])\n",
    "D = np.array([[1, 0], [-2, 1]])\n",
    "\n",
    "\n",
    "identity = np.array([[1, 0], [0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how long does it take us to get back to the origin? \n",
    "def matrix_to_tuple(matrix):\n",
    "    return (matrix[0][0], matrix[0][1], matrix[0][1], matrix[1][1])\n",
    "\n",
    "def num_steps_to_origin_w_nn(test_mat):\n",
    "    for i in range(100):\n",
    "        if (test_mat==identity).all():\n",
    "            return i\n",
    "        demo_test_type = torch.tensor([matrix_to_tuple(test_mat)], dtype=torch.float32)\n",
    "        action = int(torch.argmax(trained_model(demo_test_type)))\n",
    "\n",
    "        if action==0:\n",
    "            test_mat = test_mat @ A\n",
    "        elif action==1:\n",
    "            test_mat = test_mat @ B\n",
    "        elif action==2:\n",
    "            test_mat = test_mat @ C\n",
    "        else:\n",
    "            test_mat = test_mat @ D\n",
    "\n",
    "    return 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_mat = np.array([[1, 1, 1], [0, 1, 1], [0, 0, 1]])\n",
    "# num_steps_to_origin_w_nn(A@A@A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_wrapper_for_num_steps(row):\n",
    "    arr = np.array([\n",
    "        [int(row['val1']), int(row['val2'])],\n",
    "        [int(row['val3']), int(row['val4'])]\n",
    "        ])\n",
    "    return num_steps_to_origin_w_nn(arr)\n",
    "\n",
    "test_df['num_steps_to_origin'] = test_df.apply(row_wrapper_for_num_steps, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val1</th>\n",
       "      <th>val2</th>\n",
       "      <th>val3</th>\n",
       "      <th>val4</th>\n",
       "      <th>num_moves_Q_learning_needs</th>\n",
       "      <th>first_move_by_Q_learning</th>\n",
       "      <th>num_steps_to_origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-55.0</td>\n",
       "      <td>-94.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-7.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-39.0</td>\n",
       "      <td>-92.0</td>\n",
       "      <td>-64.0</td>\n",
       "      <td>-151.0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>-27.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-23.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1101</th>\n",
       "      <td>25.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1102</th>\n",
       "      <td>65.0</td>\n",
       "      <td>-142.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>-83.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1103</th>\n",
       "      <td>29.0</td>\n",
       "      <td>-16.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>-75.0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1104</th>\n",
       "      <td>-223.0</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>504.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1105 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       val1   val2   val3   val4  num_moves_Q_learning_needs  \\\n",
       "0     -55.0  -94.0   24.0   41.0                           7   \n",
       "1      -7.0   -2.0   32.0    9.0                           5   \n",
       "2       1.0    4.0    6.0   25.0                           5   \n",
       "3     -39.0  -92.0  -64.0 -151.0                           8   \n",
       "4       5.0   -2.0    8.0   -3.0                           3   \n",
       "...     ...    ...    ...    ...                         ...   \n",
       "1100  -27.0   62.0   10.0  -23.0                           6   \n",
       "1101   25.0   -4.0   -6.0    1.0                           5   \n",
       "1102   65.0 -142.0   38.0  -83.0                           8   \n",
       "1103   29.0  -16.0  136.0  -75.0                           8   \n",
       "1104 -223.0  -50.0  504.0  113.0                           9   \n",
       "\n",
       "      first_move_by_Q_learning  num_steps_to_origin  \n",
       "0                            2                 1000  \n",
       "1                            3                 1000  \n",
       "2                            2                    5  \n",
       "3                            2                 1000  \n",
       "4                            1                 1000  \n",
       "...                        ...                  ...  \n",
       "1100                         0                 1000  \n",
       "1101                         1                    5  \n",
       "1102                         0                 1000  \n",
       "1103                         1                 1000  \n",
       "1104                         3                 1000  \n",
       "\n",
       "[1105 rows x 7 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(test_df['num_steps_to_origin'] != 1000))\n",
    "\n",
    "just_a_few = test_df[test_df['num_steps_to_origin'] != 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val1</th>\n",
       "      <th>val2</th>\n",
       "      <th>val3</th>\n",
       "      <th>val4</th>\n",
       "      <th>num_moves_Q_learning_needs</th>\n",
       "      <th>first_move_by_Q_learning</th>\n",
       "      <th>num_steps_to_origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>-7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>-23.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>-35.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>-23.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>9.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>9.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     val1  val2  val3  val4  num_moves_Q_learning_needs  \\\n",
       "11    1.0   2.0   0.0   1.0                           1   \n",
       "24    1.0   0.0   0.0   1.0                           0   \n",
       "25    1.0   0.0   0.0   1.0                           0   \n",
       "43    1.0   0.0   0.0   1.0                           0   \n",
       "49    9.0   2.0   4.0   1.0                           3   \n",
       "56    1.0   0.0   0.0   1.0                           0   \n",
       "62    1.0   4.0   0.0   1.0                           2   \n",
       "63    1.0   2.0   0.0   1.0                           1   \n",
       "64   -7.0   4.0  -2.0   1.0                           3   \n",
       "70    1.0   6.0   0.0   1.0                           3   \n",
       "72    1.0   2.0   0.0   1.0                           1   \n",
       "75    1.0   8.0   0.0   1.0                           4   \n",
       "83    1.0   0.0   0.0   1.0                           0   \n",
       "89    1.0   4.0   0.0   1.0                           2   \n",
       "95  -23.0   4.0  -6.0   1.0                           5   \n",
       "113   1.0   6.0   0.0   1.0                           3   \n",
       "118   1.0   2.0   0.0   1.0                           1   \n",
       "130 -35.0   6.0  -6.0   1.0                           6   \n",
       "139   5.0   2.0   2.0   1.0                           2   \n",
       "149   9.0   2.0   4.0   1.0                           3   \n",
       "151 -23.0   6.0  -4.0   1.0                           5   \n",
       "155   1.0   0.0   0.0   1.0                           0   \n",
       "171   9.0  20.0   4.0   9.0                           4   \n",
       "177   1.0   0.0   0.0   1.0                           0   \n",
       "182   9.0  22.0   2.0   5.0                           4   \n",
       "183   1.0   2.0   0.0   1.0                           1   \n",
       "197   5.0  12.0   2.0   5.0                           3   \n",
       "199   5.0   2.0   2.0   1.0                           2   \n",
       "219   5.0   2.0   2.0   1.0                           2   \n",
       "235   1.0   0.0   0.0   1.0                           0   \n",
       "236   1.0   0.0   0.0   1.0                           0   \n",
       "253   1.0   2.0   0.0   1.0                           1   \n",
       "261   1.0   0.0   0.0   1.0                           0   \n",
       "262   5.0   2.0   2.0   1.0                           2   \n",
       "269   5.0   2.0   2.0   1.0                           2   \n",
       "270   1.0   0.0   0.0   1.0                           0   \n",
       "279   1.0   0.0   0.0   1.0                           0   \n",
       "282   5.0   2.0   2.0   1.0                           2   \n",
       "292   5.0   2.0   2.0   1.0                           2   \n",
       "294   1.0   0.0   0.0   1.0                           0   \n",
       "297   1.0   0.0   0.0   1.0                           0   \n",
       "\n",
       "     first_move_by_Q_learning  num_steps_to_origin  \n",
       "11                          2                    1  \n",
       "24                          0                    0  \n",
       "25                          0                    0  \n",
       "43                          0                    0  \n",
       "49                          3                    3  \n",
       "56                          0                    0  \n",
       "62                          2                    2  \n",
       "63                          2                    1  \n",
       "64                          1                    3  \n",
       "70                          2                    3  \n",
       "72                          2                    1  \n",
       "75                          2                    4  \n",
       "83                          0                    0  \n",
       "89                          2                    2  \n",
       "95                          1                    5  \n",
       "113                         2                    3  \n",
       "118                         2                    1  \n",
       "130                         1                    6  \n",
       "139                         3                    2  \n",
       "149                         3                    3  \n",
       "151                         1                    5  \n",
       "155                         0                    0  \n",
       "171                         2                    4  \n",
       "177                         0                    0  \n",
       "182                         2                    4  \n",
       "183                         2                    1  \n",
       "197                         2                    3  \n",
       "199                         3                    2  \n",
       "219                         3                    2  \n",
       "235                         0                    0  \n",
       "236                         0                    0  \n",
       "253                         2                    1  \n",
       "261                         0                    0  \n",
       "262                         3                    2  \n",
       "269                         3                    2  \n",
       "270                         0                    0  \n",
       "279                         0                    0  \n",
       "282                         3                    2  \n",
       "292                         3                    2  \n",
       "294                         0                    0  \n",
       "297                         0                    0  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "just_a_few"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_action(cur_matrix, action):\n",
    "    if action==0:\n",
    "        return cur_matrix @ A\n",
    "    elif action==1:\n",
    "        return cur_matrix @ B\n",
    "    elif action==2:\n",
    "        return cur_matrix @ C\n",
    "    elif action==3:\n",
    "        return cur_matrix @ D\n",
    "    assert 1==2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# going to get to the origin and track where we've been\n",
    "already_seen = set()\n",
    "start_matrix = np.array([[1, 5, -8], [0, 1, 1], [0, 0, 1]])\n",
    "already_seen.add(matrix_to_tuple(start_matrix))\n",
    "\n",
    "cur_matrix = start_matrix\n",
    "i=0\n",
    "\n",
    "while not (cur_matrix==identity).all():\n",
    "\n",
    "    if i>10000:\n",
    "        break\n",
    "\n",
    "    demo_test_type = torch.tensor([matrix_to_tuple(start_matrix)], dtype=torch.float32)\n",
    "    action = int(torch.argmax(trained_model(demo_test_type)))\n",
    "\n",
    "    new_mat = apply_action(cur_matrix, action)\n",
    "\n",
    "    # if we're about to cycle\n",
    "    if matrix_to_tuple(new_mat) in already_seen:\n",
    "        action = int(trained_model(demo_test_type).topk(2, largest=True)[1][0][-1])\n",
    "        new_mat = apply_action(new_mat, action)\n",
    "\n",
    "        if matrix_to_tuple(new_mat) in already_seen:\n",
    "            action = int(trained_model(demo_test_type).topk(3, largest=True)[1][0][-1])\n",
    "            new_mat = apply_action(new_mat, action)\n",
    "\n",
    "            if matrix_to_tuple(new_mat) in already_seen:\n",
    "                action = int(torch.argmax(trained_model(demo_test_type)))\n",
    "                new_mat = apply_action(new_mat, action)\n",
    "\n",
    "                if matrix_to_tuple(new_mat) in already_seen:\n",
    "                    action = random.randint(1, 4)\n",
    "                    new_mat = apply_action(new_mat, action)\n",
    "\n",
    "    \n",
    "    cur_matrix = new_mat\n",
    "    already_seen.add(matrix_to_tuple(cur_matrix))\n",
    "    i+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(mat):\n",
    "    demo_test_type = torch.tensor([matrix_to_tuple(mat)], dtype=torch.float32)\n",
    "    return int(torch.argmax(trained_model(demo_test_type)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
