{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code adapted from ChatGPT\n",
    "\n",
    "# Define your neural network model\n",
    "class SL2Z_NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SL2Z_NN, self).__init__()\n",
    "\n",
    "        self.step1 = nn.Linear(4, 128, bias=True)\n",
    "        self.step2 = nn.ReLU()\n",
    "        self.step3 = nn.Linear(128, 64, bias=True)\n",
    "        self.step4 = nn.ReLU()\n",
    "        self.step5 = nn.Linear(64, 16, bias=True)\n",
    "        self.step6 = nn.ReLU()\n",
    "        self.step7 = nn.Linear(16, 4, bias=True)\n",
    "\n",
    "        # multi-class classification adapted from ChatGPT\n",
    "        self.step8 = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # RUN IT ON A GPU if it exists\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.to(\"cuda\")\n",
    "\n",
    "        x = self.step1(x)\n",
    "        x = self.step2(x)\n",
    "        x = self.step3(x)\n",
    "        x = self.step4(x)\n",
    "        x = self.step5(x)\n",
    "        x = self.step6(x)\n",
    "        x = self.step7(x)\n",
    "        x = self.step8(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def train_model(inputs, desired_outputs, num_epochs=100, learning_rate=0.01, viz_accuracy=False):\n",
    "    # Convert inputs and desired_outputs to PyTorch tensors\n",
    "    inputs = torch.tensor(inputs, dtype=torch.float32)\n",
    "    desired_outputs = torch.tensor(desired_outputs, dtype=torch.float32)\n",
    "    \n",
    "    # Create a DataLoader to handle batching (if needed)\n",
    "    dataset = TensorDataset(inputs, desired_outputs)\n",
    "    dataloader = DataLoader(dataset, batch_size=1000, shuffle=True)  # Adjust batch_size as needed\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = SL2Z_NN()\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()  # Mean Squared Error loss\n",
    "    # optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "    # criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.995)\n",
    "    losses = []\n",
    "\n",
    "    inputs = inputs.float()\n",
    "    desired_outputs = desired_outputs.long()\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for batch_inputs, batch_desired_outputs in dataloader:\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            outputs = model(batch_inputs)  # Forward pass\n",
    "\n",
    "            batch_desired_outputs = batch_desired_outputs.long()\n",
    "\n",
    "            loss = criterion(outputs, batch_desired_outputs)  # Compute the loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update the model's parameters\n",
    "            cur_item = loss.item()\n",
    "            total_loss += cur_item\n",
    "        \n",
    "        # Print the average loss for this epoch\n",
    "        scheduler.step()\n",
    "        print(f\"total loss: {total_loss}\")\n",
    "        average_loss = total_loss / len(dataloader)\n",
    "        if viz_accuracy:\n",
    "            losses.append(average_loss)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.4f}')\n",
    "    \n",
    "    if viz_accuracy:\n",
    "        plt.scatter(x=range(1, len(losses)+1), y=losses)\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Average loss\")\n",
    "        plt.show()\n",
    "\n",
    "    # Return the trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Data_Generation/Data_files/subset3_train_rows_SL2Z_Q_learn.csv\")\n",
    "test_df = pd.read_csv(\"../Data_Generation/Data_files/subset3_test_rows_SL2Z_Q_learn.csv\")\n",
    "\n",
    "\n",
    "train = df.drop(\"num_moves_Q_learning_needs\", axis=1)\n",
    "# random.seed(42)\n",
    "# np.random.seed(42)\n",
    "# reordered = df.sample(frac=1).reset_index(drop=True)\n",
    "# bound = int(reordered.shape[0] * 0.6)\n",
    "# plus_one = bound+1\n",
    "\n",
    "# train = reordered.iloc[1:bound]\n",
    "# test_df = reordered.iloc[plus_one:reordered.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "just_input = train.drop('first_move_by_Q_learning', axis=1)\n",
    "\n",
    "# adapted from https://stackoverflow.com/questions/43898035/pandas-combine-column-values-into-a-list-in-a-new-column\n",
    "input_data = np.array(just_input.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_output = torch.tensor(train['first_move_by_Q_learning'].tolist(), dtype=torch.float32).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t9/gxb6q8zj21dff090q066td740000gn/T/ipykernel_10569/3137828461.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  desired_outputs = torch.tensor(desired_outputs, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total loss: 2.8069804906845093\n",
      "Epoch [1/500], Loss: 1.4035\n",
      "total loss: 2.6596325635910034\n",
      "Epoch [2/500], Loss: 1.3298\n",
      "total loss: 2.5911247730255127\n",
      "Epoch [3/500], Loss: 1.2956\n",
      "total loss: 2.545790910720825\n",
      "Epoch [4/500], Loss: 1.2729\n",
      "total loss: 2.5209057331085205\n",
      "Epoch [5/500], Loss: 1.2605\n",
      "total loss: 2.498479127883911\n",
      "Epoch [6/500], Loss: 1.2492\n",
      "total loss: 2.4074506759643555\n",
      "Epoch [7/500], Loss: 1.2037\n",
      "total loss: 2.356298089027405\n",
      "Epoch [8/500], Loss: 1.1781\n",
      "total loss: 2.3306808471679688\n",
      "Epoch [9/500], Loss: 1.1653\n",
      "total loss: 2.3237205743789673\n",
      "Epoch [10/500], Loss: 1.1619\n",
      "total loss: 2.3179855346679688\n",
      "Epoch [11/500], Loss: 1.1590\n",
      "total loss: 2.31781804561615\n",
      "Epoch [12/500], Loss: 1.1589\n",
      "total loss: 2.3177011013031006\n",
      "Epoch [13/500], Loss: 1.1589\n",
      "total loss: 2.280306816101074\n",
      "Epoch [14/500], Loss: 1.1402\n",
      "total loss: 2.280889391899109\n",
      "Epoch [15/500], Loss: 1.1404\n",
      "total loss: 2.267929434776306\n",
      "Epoch [16/500], Loss: 1.1340\n",
      "total loss: 2.256562829017639\n",
      "Epoch [17/500], Loss: 1.1283\n",
      "total loss: 2.2103863954544067\n",
      "Epoch [18/500], Loss: 1.1052\n",
      "total loss: 2.186046600341797\n",
      "Epoch [19/500], Loss: 1.0930\n",
      "total loss: 2.169882655143738\n",
      "Epoch [20/500], Loss: 1.0849\n",
      "total loss: 2.138070225715637\n",
      "Epoch [21/500], Loss: 1.0690\n",
      "total loss: 2.112642526626587\n",
      "Epoch [22/500], Loss: 1.0563\n",
      "total loss: 2.104805827140808\n",
      "Epoch [23/500], Loss: 1.0524\n",
      "total loss: 2.0865447521209717\n",
      "Epoch [24/500], Loss: 1.0433\n",
      "total loss: 2.0888291597366333\n",
      "Epoch [25/500], Loss: 1.0444\n",
      "total loss: 2.0393773317337036\n",
      "Epoch [26/500], Loss: 1.0197\n",
      "total loss: 1.9884889721870422\n",
      "Epoch [27/500], Loss: 0.9942\n",
      "total loss: 1.9635223746299744\n",
      "Epoch [28/500], Loss: 0.9818\n",
      "total loss: 1.9625179767608643\n",
      "Epoch [29/500], Loss: 0.9813\n",
      "total loss: 1.9446902871131897\n",
      "Epoch [30/500], Loss: 0.9723\n",
      "total loss: 1.9464583992958069\n",
      "Epoch [31/500], Loss: 0.9732\n",
      "total loss: 1.9480423331260681\n",
      "Epoch [32/500], Loss: 0.9740\n",
      "total loss: 1.9437068104743958\n",
      "Epoch [33/500], Loss: 0.9719\n",
      "total loss: 1.9446921944618225\n",
      "Epoch [34/500], Loss: 0.9723\n",
      "total loss: 1.929413914680481\n",
      "Epoch [35/500], Loss: 0.9647\n",
      "total loss: 1.9323331117630005\n",
      "Epoch [36/500], Loss: 0.9662\n",
      "total loss: 1.9208185076713562\n",
      "Epoch [37/500], Loss: 0.9604\n",
      "total loss: 1.9297341704368591\n",
      "Epoch [38/500], Loss: 0.9649\n",
      "total loss: 1.926067292690277\n",
      "Epoch [39/500], Loss: 0.9630\n",
      "total loss: 1.9281785488128662\n",
      "Epoch [40/500], Loss: 0.9641\n",
      "total loss: 1.9246353507041931\n",
      "Epoch [41/500], Loss: 0.9623\n",
      "total loss: 1.9305074214935303\n",
      "Epoch [42/500], Loss: 0.9653\n",
      "total loss: 1.9214569926261902\n",
      "Epoch [43/500], Loss: 0.9607\n",
      "total loss: 1.9163627624511719\n",
      "Epoch [44/500], Loss: 0.9582\n",
      "total loss: 1.919963002204895\n",
      "Epoch [45/500], Loss: 0.9600\n",
      "total loss: 1.9200062155723572\n",
      "Epoch [46/500], Loss: 0.9600\n",
      "total loss: 1.9067938923835754\n",
      "Epoch [47/500], Loss: 0.9534\n",
      "total loss: 1.9166922569274902\n",
      "Epoch [48/500], Loss: 0.9583\n",
      "total loss: 1.91086745262146\n",
      "Epoch [49/500], Loss: 0.9554\n",
      "total loss: 1.9061055779457092\n",
      "Epoch [50/500], Loss: 0.9531\n",
      "total loss: 1.908556342124939\n",
      "Epoch [51/500], Loss: 0.9543\n",
      "total loss: 1.9077333211898804\n",
      "Epoch [52/500], Loss: 0.9539\n",
      "total loss: 1.8980188369750977\n",
      "Epoch [53/500], Loss: 0.9490\n",
      "total loss: 1.9022438526153564\n",
      "Epoch [54/500], Loss: 0.9511\n",
      "total loss: 1.9031655192375183\n",
      "Epoch [55/500], Loss: 0.9516\n",
      "total loss: 1.9008919596672058\n",
      "Epoch [56/500], Loss: 0.9504\n",
      "total loss: 1.903005063533783\n",
      "Epoch [57/500], Loss: 0.9515\n",
      "total loss: 1.8948566317558289\n",
      "Epoch [58/500], Loss: 0.9474\n",
      "total loss: 1.8911948800086975\n",
      "Epoch [59/500], Loss: 0.9456\n",
      "total loss: 1.8983659148216248\n",
      "Epoch [60/500], Loss: 0.9492\n",
      "total loss: 1.890748143196106\n",
      "Epoch [61/500], Loss: 0.9454\n",
      "total loss: 1.9042863845825195\n",
      "Epoch [62/500], Loss: 0.9521\n",
      "total loss: 1.8925181031227112\n",
      "Epoch [63/500], Loss: 0.9463\n",
      "total loss: 1.891742467880249\n",
      "Epoch [64/500], Loss: 0.9459\n",
      "total loss: 1.8915874361991882\n",
      "Epoch [65/500], Loss: 0.9458\n",
      "total loss: 1.8928582668304443\n",
      "Epoch [66/500], Loss: 0.9464\n",
      "total loss: 1.8884420990943909\n",
      "Epoch [67/500], Loss: 0.9442\n",
      "total loss: 1.8875699639320374\n",
      "Epoch [68/500], Loss: 0.9438\n",
      "total loss: 1.8882768154144287\n",
      "Epoch [69/500], Loss: 0.9441\n",
      "total loss: 1.888802409172058\n",
      "Epoch [70/500], Loss: 0.9444\n",
      "total loss: 1.8924522995948792\n",
      "Epoch [71/500], Loss: 0.9462\n",
      "total loss: 1.883114755153656\n",
      "Epoch [72/500], Loss: 0.9416\n",
      "total loss: 1.8835787177085876\n",
      "Epoch [73/500], Loss: 0.9418\n",
      "total loss: 1.8990527987480164\n",
      "Epoch [74/500], Loss: 0.9495\n",
      "total loss: 1.892137050628662\n",
      "Epoch [75/500], Loss: 0.9461\n",
      "total loss: 1.8855060338974\n",
      "Epoch [76/500], Loss: 0.9428\n",
      "total loss: 1.8838225603103638\n",
      "Epoch [77/500], Loss: 0.9419\n",
      "total loss: 1.8828396201133728\n",
      "Epoch [78/500], Loss: 0.9414\n",
      "total loss: 1.8818520903587341\n",
      "Epoch [79/500], Loss: 0.9409\n",
      "total loss: 1.892469346523285\n",
      "Epoch [80/500], Loss: 0.9462\n",
      "total loss: 1.8820419311523438\n",
      "Epoch [81/500], Loss: 0.9410\n",
      "total loss: 1.8843377828598022\n",
      "Epoch [82/500], Loss: 0.9422\n",
      "total loss: 1.8795986771583557\n",
      "Epoch [83/500], Loss: 0.9398\n",
      "total loss: 1.8815478682518005\n",
      "Epoch [84/500], Loss: 0.9408\n",
      "total loss: 1.8815192580223083\n",
      "Epoch [85/500], Loss: 0.9408\n",
      "total loss: 1.8768376111984253\n",
      "Epoch [86/500], Loss: 0.9384\n",
      "total loss: 1.8725560903549194\n",
      "Epoch [87/500], Loss: 0.9363\n",
      "total loss: 1.8781856894493103\n",
      "Epoch [88/500], Loss: 0.9391\n",
      "total loss: 1.8760285377502441\n",
      "Epoch [89/500], Loss: 0.9380\n",
      "total loss: 1.8763704299926758\n",
      "Epoch [90/500], Loss: 0.9382\n",
      "total loss: 1.8764649033546448\n",
      "Epoch [91/500], Loss: 0.9382\n",
      "total loss: 1.8730077147483826\n",
      "Epoch [92/500], Loss: 0.9365\n",
      "total loss: 1.8783984184265137\n",
      "Epoch [93/500], Loss: 0.9392\n",
      "total loss: 1.87110036611557\n",
      "Epoch [94/500], Loss: 0.9356\n",
      "total loss: 1.8716490864753723\n",
      "Epoch [95/500], Loss: 0.9358\n",
      "total loss: 1.876739740371704\n",
      "Epoch [96/500], Loss: 0.9384\n",
      "total loss: 1.8665347695350647\n",
      "Epoch [97/500], Loss: 0.9333\n",
      "total loss: 1.8718652725219727\n",
      "Epoch [98/500], Loss: 0.9359\n",
      "total loss: 1.8718881011009216\n",
      "Epoch [99/500], Loss: 0.9359\n",
      "total loss: 1.8764925003051758\n",
      "Epoch [100/500], Loss: 0.9382\n",
      "total loss: 1.870248019695282\n",
      "Epoch [101/500], Loss: 0.9351\n",
      "total loss: 1.8730847835540771\n",
      "Epoch [102/500], Loss: 0.9365\n",
      "total loss: 1.8798829317092896\n",
      "Epoch [103/500], Loss: 0.9399\n",
      "total loss: 1.8752174973487854\n",
      "Epoch [104/500], Loss: 0.9376\n",
      "total loss: 1.867816984653473\n",
      "Epoch [105/500], Loss: 0.9339\n",
      "total loss: 1.869431734085083\n",
      "Epoch [106/500], Loss: 0.9347\n",
      "total loss: 1.8584568500518799\n",
      "Epoch [107/500], Loss: 0.9292\n",
      "total loss: 1.862217128276825\n",
      "Epoch [108/500], Loss: 0.9311\n",
      "total loss: 1.8652687072753906\n",
      "Epoch [109/500], Loss: 0.9326\n",
      "total loss: 1.8615602254867554\n",
      "Epoch [110/500], Loss: 0.9308\n",
      "total loss: 1.859409511089325\n",
      "Epoch [111/500], Loss: 0.9297\n",
      "total loss: 1.865264356136322\n",
      "Epoch [112/500], Loss: 0.9326\n",
      "total loss: 1.8673462867736816\n",
      "Epoch [113/500], Loss: 0.9337\n",
      "total loss: 1.8620132803916931\n",
      "Epoch [114/500], Loss: 0.9310\n",
      "total loss: 1.863396942615509\n",
      "Epoch [115/500], Loss: 0.9317\n",
      "total loss: 1.857702910900116\n",
      "Epoch [116/500], Loss: 0.9289\n",
      "total loss: 1.857461154460907\n",
      "Epoch [117/500], Loss: 0.9287\n",
      "total loss: 1.8486858010292053\n",
      "Epoch [118/500], Loss: 0.9243\n",
      "total loss: 1.8365832567214966\n",
      "Epoch [119/500], Loss: 0.9183\n",
      "total loss: 1.8216400146484375\n",
      "Epoch [120/500], Loss: 0.9108\n",
      "total loss: 1.8150456547737122\n",
      "Epoch [121/500], Loss: 0.9075\n",
      "total loss: 1.815484344959259\n",
      "Epoch [122/500], Loss: 0.9077\n",
      "total loss: 1.8130132555961609\n",
      "Epoch [123/500], Loss: 0.9065\n",
      "total loss: 1.805696725845337\n",
      "Epoch [124/500], Loss: 0.9028\n",
      "total loss: 1.8001896739006042\n",
      "Epoch [125/500], Loss: 0.9001\n",
      "total loss: 1.7968426942825317\n",
      "Epoch [126/500], Loss: 0.8984\n",
      "total loss: 1.788767695426941\n",
      "Epoch [127/500], Loss: 0.8944\n",
      "total loss: 1.7805936336517334\n",
      "Epoch [128/500], Loss: 0.8903\n",
      "total loss: 1.7673493027687073\n",
      "Epoch [129/500], Loss: 0.8837\n",
      "total loss: 1.7701690196990967\n",
      "Epoch [130/500], Loss: 0.8851\n",
      "total loss: 1.7615512013435364\n",
      "Epoch [131/500], Loss: 0.8808\n",
      "total loss: 1.7648406028747559\n",
      "Epoch [132/500], Loss: 0.8824\n",
      "total loss: 1.7618880867958069\n",
      "Epoch [133/500], Loss: 0.8809\n",
      "total loss: 1.7555967569351196\n",
      "Epoch [134/500], Loss: 0.8778\n",
      "total loss: 1.7441978454589844\n",
      "Epoch [135/500], Loss: 0.8721\n",
      "total loss: 1.7388322353363037\n",
      "Epoch [136/500], Loss: 0.8694\n",
      "total loss: 1.727529227733612\n",
      "Epoch [137/500], Loss: 0.8638\n",
      "total loss: 1.715259313583374\n",
      "Epoch [138/500], Loss: 0.8576\n",
      "total loss: 1.7154122591018677\n",
      "Epoch [139/500], Loss: 0.8577\n",
      "total loss: 1.7212525010108948\n",
      "Epoch [140/500], Loss: 0.8606\n",
      "total loss: 1.7154731154441833\n",
      "Epoch [141/500], Loss: 0.8577\n",
      "total loss: 1.711893618106842\n",
      "Epoch [142/500], Loss: 0.8559\n",
      "total loss: 1.7101596593856812\n",
      "Epoch [143/500], Loss: 0.8551\n",
      "total loss: 1.7069044709205627\n",
      "Epoch [144/500], Loss: 0.8535\n",
      "total loss: 1.709964096546173\n",
      "Epoch [145/500], Loss: 0.8550\n",
      "total loss: 1.7063806056976318\n",
      "Epoch [146/500], Loss: 0.8532\n",
      "total loss: 1.6985998749732971\n",
      "Epoch [147/500], Loss: 0.8493\n",
      "total loss: 1.6917273998260498\n",
      "Epoch [148/500], Loss: 0.8459\n",
      "total loss: 1.6948888897895813\n",
      "Epoch [149/500], Loss: 0.8474\n",
      "total loss: 1.6971789598464966\n",
      "Epoch [150/500], Loss: 0.8486\n",
      "total loss: 1.6915422081947327\n",
      "Epoch [151/500], Loss: 0.8458\n",
      "total loss: 1.6966261267662048\n",
      "Epoch [152/500], Loss: 0.8483\n",
      "total loss: 1.6797741651535034\n",
      "Epoch [153/500], Loss: 0.8399\n",
      "total loss: 1.6783488392829895\n",
      "Epoch [154/500], Loss: 0.8392\n",
      "total loss: 1.6710342168807983\n",
      "Epoch [155/500], Loss: 0.8355\n",
      "total loss: 1.670462965965271\n",
      "Epoch [156/500], Loss: 0.8352\n",
      "total loss: 1.6717768907546997\n",
      "Epoch [157/500], Loss: 0.8359\n",
      "total loss: 1.6711861491203308\n",
      "Epoch [158/500], Loss: 0.8356\n",
      "total loss: 1.6721296310424805\n",
      "Epoch [159/500], Loss: 0.8361\n",
      "total loss: 1.6735557913780212\n",
      "Epoch [160/500], Loss: 0.8368\n",
      "total loss: 1.6675984859466553\n",
      "Epoch [161/500], Loss: 0.8338\n",
      "total loss: 1.6695841550827026\n",
      "Epoch [162/500], Loss: 0.8348\n",
      "total loss: 1.6714455485343933\n",
      "Epoch [163/500], Loss: 0.8357\n",
      "total loss: 1.662207543849945\n",
      "Epoch [164/500], Loss: 0.8311\n",
      "total loss: 1.6684128642082214\n",
      "Epoch [165/500], Loss: 0.8342\n",
      "total loss: 1.6614433526992798\n",
      "Epoch [166/500], Loss: 0.8307\n",
      "total loss: 1.6562381982803345\n",
      "Epoch [167/500], Loss: 0.8281\n",
      "total loss: 1.6646298170089722\n",
      "Epoch [168/500], Loss: 0.8323\n",
      "total loss: 1.6653429865837097\n",
      "Epoch [169/500], Loss: 0.8327\n",
      "total loss: 1.6657376885414124\n",
      "Epoch [170/500], Loss: 0.8329\n",
      "total loss: 1.663970410823822\n",
      "Epoch [171/500], Loss: 0.8320\n",
      "total loss: 1.6639965772628784\n",
      "Epoch [172/500], Loss: 0.8320\n",
      "total loss: 1.6621707081794739\n",
      "Epoch [173/500], Loss: 0.8311\n",
      "total loss: 1.6619510650634766\n",
      "Epoch [174/500], Loss: 0.8310\n",
      "total loss: 1.6561799049377441\n",
      "Epoch [175/500], Loss: 0.8281\n",
      "total loss: 1.654434323310852\n",
      "Epoch [176/500], Loss: 0.8272\n",
      "total loss: 1.6580236554145813\n",
      "Epoch [177/500], Loss: 0.8290\n",
      "total loss: 1.656545102596283\n",
      "Epoch [178/500], Loss: 0.8283\n",
      "total loss: 1.6613184809684753\n",
      "Epoch [179/500], Loss: 0.8307\n",
      "total loss: 1.6593027114868164\n",
      "Epoch [180/500], Loss: 0.8297\n",
      "total loss: 1.6570818424224854\n",
      "Epoch [181/500], Loss: 0.8285\n",
      "total loss: 1.6544013023376465\n",
      "Epoch [182/500], Loss: 0.8272\n",
      "total loss: 1.6578366160392761\n",
      "Epoch [183/500], Loss: 0.8289\n",
      "total loss: 1.6564833521842957\n",
      "Epoch [184/500], Loss: 0.8282\n",
      "total loss: 1.6549387574195862\n",
      "Epoch [185/500], Loss: 0.8275\n",
      "total loss: 1.6556864976882935\n",
      "Epoch [186/500], Loss: 0.8278\n",
      "total loss: 1.659646213054657\n",
      "Epoch [187/500], Loss: 0.8298\n",
      "total loss: 1.651579737663269\n",
      "Epoch [188/500], Loss: 0.8258\n",
      "total loss: 1.6583985686302185\n",
      "Epoch [189/500], Loss: 0.8292\n",
      "total loss: 1.6516878008842468\n",
      "Epoch [190/500], Loss: 0.8258\n",
      "total loss: 1.6545612812042236\n",
      "Epoch [191/500], Loss: 0.8273\n",
      "total loss: 1.6516373753547668\n",
      "Epoch [192/500], Loss: 0.8258\n",
      "total loss: 1.658420979976654\n",
      "Epoch [193/500], Loss: 0.8292\n",
      "total loss: 1.6560294032096863\n",
      "Epoch [194/500], Loss: 0.8280\n",
      "total loss: 1.6563499569892883\n",
      "Epoch [195/500], Loss: 0.8282\n",
      "total loss: 1.6564161777496338\n",
      "Epoch [196/500], Loss: 0.8282\n",
      "total loss: 1.6557289361953735\n",
      "Epoch [197/500], Loss: 0.8279\n",
      "total loss: 1.6559854745864868\n",
      "Epoch [198/500], Loss: 0.8280\n",
      "total loss: 1.6564159393310547\n",
      "Epoch [199/500], Loss: 0.8282\n",
      "total loss: 1.6519768238067627\n",
      "Epoch [200/500], Loss: 0.8260\n",
      "total loss: 1.6530727744102478\n",
      "Epoch [201/500], Loss: 0.8265\n",
      "total loss: 1.653511643409729\n",
      "Epoch [202/500], Loss: 0.8268\n",
      "total loss: 1.6506432890892029\n",
      "Epoch [203/500], Loss: 0.8253\n",
      "total loss: 1.6531313061714172\n",
      "Epoch [204/500], Loss: 0.8266\n",
      "total loss: 1.6417693495750427\n",
      "Epoch [205/500], Loss: 0.8209\n",
      "total loss: 1.6484336256980896\n",
      "Epoch [206/500], Loss: 0.8242\n",
      "total loss: 1.6457433104515076\n",
      "Epoch [207/500], Loss: 0.8229\n",
      "total loss: 1.6485894322395325\n",
      "Epoch [208/500], Loss: 0.8243\n",
      "total loss: 1.645040512084961\n",
      "Epoch [209/500], Loss: 0.8225\n",
      "total loss: 1.6528401970863342\n",
      "Epoch [210/500], Loss: 0.8264\n",
      "total loss: 1.6511300802230835\n",
      "Epoch [211/500], Loss: 0.8256\n",
      "total loss: 1.647383451461792\n",
      "Epoch [212/500], Loss: 0.8237\n",
      "total loss: 1.6527807712554932\n",
      "Epoch [213/500], Loss: 0.8264\n",
      "total loss: 1.6456497311592102\n",
      "Epoch [214/500], Loss: 0.8228\n",
      "total loss: 1.648375689983368\n",
      "Epoch [215/500], Loss: 0.8242\n",
      "total loss: 1.6465471386909485\n",
      "Epoch [216/500], Loss: 0.8233\n",
      "total loss: 1.6489002108573914\n",
      "Epoch [217/500], Loss: 0.8245\n",
      "total loss: 1.6465312242507935\n",
      "Epoch [218/500], Loss: 0.8233\n",
      "total loss: 1.6453147530555725\n",
      "Epoch [219/500], Loss: 0.8227\n",
      "total loss: 1.6492345929145813\n",
      "Epoch [220/500], Loss: 0.8246\n",
      "total loss: 1.6435548067092896\n",
      "Epoch [221/500], Loss: 0.8218\n",
      "total loss: 1.648362398147583\n",
      "Epoch [222/500], Loss: 0.8242\n",
      "total loss: 1.648083209991455\n",
      "Epoch [223/500], Loss: 0.8240\n",
      "total loss: 1.6431140899658203\n",
      "Epoch [224/500], Loss: 0.8216\n",
      "total loss: 1.6458709836006165\n",
      "Epoch [225/500], Loss: 0.8229\n",
      "total loss: 1.6464818716049194\n",
      "Epoch [226/500], Loss: 0.8232\n",
      "total loss: 1.6479192972183228\n",
      "Epoch [227/500], Loss: 0.8240\n",
      "total loss: 1.6380674242973328\n",
      "Epoch [228/500], Loss: 0.8190\n",
      "total loss: 1.6474925875663757\n",
      "Epoch [229/500], Loss: 0.8237\n",
      "total loss: 1.6450978517532349\n",
      "Epoch [230/500], Loss: 0.8225\n",
      "total loss: 1.6385313272476196\n",
      "Epoch [231/500], Loss: 0.8193\n",
      "total loss: 1.6404759883880615\n",
      "Epoch [232/500], Loss: 0.8202\n",
      "total loss: 1.6465007066726685\n",
      "Epoch [233/500], Loss: 0.8233\n",
      "total loss: 1.6433265805244446\n",
      "Epoch [234/500], Loss: 0.8217\n",
      "total loss: 1.6449959874153137\n",
      "Epoch [235/500], Loss: 0.8225\n",
      "total loss: 1.6419142484664917\n",
      "Epoch [236/500], Loss: 0.8210\n",
      "total loss: 1.6372923851013184\n",
      "Epoch [237/500], Loss: 0.8186\n",
      "total loss: 1.6441785097122192\n",
      "Epoch [238/500], Loss: 0.8221\n",
      "total loss: 1.648747742176056\n",
      "Epoch [239/500], Loss: 0.8244\n",
      "total loss: 1.642539381980896\n",
      "Epoch [240/500], Loss: 0.8213\n",
      "total loss: 1.6416740417480469\n",
      "Epoch [241/500], Loss: 0.8208\n",
      "total loss: 1.6411080956459045\n",
      "Epoch [242/500], Loss: 0.8206\n",
      "total loss: 1.6412045359611511\n",
      "Epoch [243/500], Loss: 0.8206\n",
      "total loss: 1.638388454914093\n",
      "Epoch [244/500], Loss: 0.8192\n",
      "total loss: 1.6433961391448975\n",
      "Epoch [245/500], Loss: 0.8217\n",
      "total loss: 1.6440213322639465\n",
      "Epoch [246/500], Loss: 0.8220\n",
      "total loss: 1.6434603929519653\n",
      "Epoch [247/500], Loss: 0.8217\n",
      "total loss: 1.6428290009498596\n",
      "Epoch [248/500], Loss: 0.8214\n",
      "total loss: 1.6432129740715027\n",
      "Epoch [249/500], Loss: 0.8216\n",
      "total loss: 1.642387568950653\n",
      "Epoch [250/500], Loss: 0.8212\n",
      "total loss: 1.6457934975624084\n",
      "Epoch [251/500], Loss: 0.8229\n",
      "total loss: 1.6361266374588013\n",
      "Epoch [252/500], Loss: 0.8181\n",
      "total loss: 1.6367971301078796\n",
      "Epoch [253/500], Loss: 0.8184\n",
      "total loss: 1.6373765468597412\n",
      "Epoch [254/500], Loss: 0.8187\n",
      "total loss: 1.6422066688537598\n",
      "Epoch [255/500], Loss: 0.8211\n",
      "total loss: 1.6417072415351868\n",
      "Epoch [256/500], Loss: 0.8209\n",
      "total loss: 1.6383690237998962\n",
      "Epoch [257/500], Loss: 0.8192\n",
      "total loss: 1.635657548904419\n",
      "Epoch [258/500], Loss: 0.8178\n",
      "total loss: 1.640120804309845\n",
      "Epoch [259/500], Loss: 0.8201\n",
      "total loss: 1.633804440498352\n",
      "Epoch [260/500], Loss: 0.8169\n",
      "total loss: 1.6408113837242126\n",
      "Epoch [261/500], Loss: 0.8204\n",
      "total loss: 1.6384454369544983\n",
      "Epoch [262/500], Loss: 0.8192\n",
      "total loss: 1.6310036778450012\n",
      "Epoch [263/500], Loss: 0.8155\n",
      "total loss: 1.638282597064972\n",
      "Epoch [264/500], Loss: 0.8191\n",
      "total loss: 1.634890615940094\n",
      "Epoch [265/500], Loss: 0.8174\n",
      "total loss: 1.6370201706886292\n",
      "Epoch [266/500], Loss: 0.8185\n",
      "total loss: 1.6404577493667603\n",
      "Epoch [267/500], Loss: 0.8202\n",
      "total loss: 1.6327665448188782\n",
      "Epoch [268/500], Loss: 0.8164\n",
      "total loss: 1.6378238201141357\n",
      "Epoch [269/500], Loss: 0.8189\n",
      "total loss: 1.6354367136955261\n",
      "Epoch [270/500], Loss: 0.8177\n",
      "total loss: 1.635499358177185\n",
      "Epoch [271/500], Loss: 0.8177\n",
      "total loss: 1.6374294757843018\n",
      "Epoch [272/500], Loss: 0.8187\n",
      "total loss: 1.63678640127182\n",
      "Epoch [273/500], Loss: 0.8184\n",
      "total loss: 1.6351509094238281\n",
      "Epoch [274/500], Loss: 0.8176\n",
      "total loss: 1.6317166090011597\n",
      "Epoch [275/500], Loss: 0.8159\n",
      "total loss: 1.634993314743042\n",
      "Epoch [276/500], Loss: 0.8175\n",
      "total loss: 1.6350245475769043\n",
      "Epoch [277/500], Loss: 0.8175\n",
      "total loss: 1.6383918523788452\n",
      "Epoch [278/500], Loss: 0.8192\n",
      "total loss: 1.6324902176856995\n",
      "Epoch [279/500], Loss: 0.8162\n",
      "total loss: 1.6377385258674622\n",
      "Epoch [280/500], Loss: 0.8189\n",
      "total loss: 1.6405012011528015\n",
      "Epoch [281/500], Loss: 0.8203\n",
      "total loss: 1.6344881653785706\n",
      "Epoch [282/500], Loss: 0.8172\n",
      "total loss: 1.6337340474128723\n",
      "Epoch [283/500], Loss: 0.8169\n",
      "total loss: 1.6359888315200806\n",
      "Epoch [284/500], Loss: 0.8180\n",
      "total loss: 1.6343103647232056\n",
      "Epoch [285/500], Loss: 0.8172\n",
      "total loss: 1.6317178010940552\n",
      "Epoch [286/500], Loss: 0.8159\n",
      "total loss: 1.6342233419418335\n",
      "Epoch [287/500], Loss: 0.8171\n",
      "total loss: 1.634444534778595\n",
      "Epoch [288/500], Loss: 0.8172\n",
      "total loss: 1.6272585988044739\n",
      "Epoch [289/500], Loss: 0.8136\n",
      "total loss: 1.6324217319488525\n",
      "Epoch [290/500], Loss: 0.8162\n",
      "total loss: 1.6368164420127869\n",
      "Epoch [291/500], Loss: 0.8184\n",
      "total loss: 1.6310869455337524\n",
      "Epoch [292/500], Loss: 0.8155\n",
      "total loss: 1.6313426494598389\n",
      "Epoch [293/500], Loss: 0.8157\n",
      "total loss: 1.635392963886261\n",
      "Epoch [294/500], Loss: 0.8177\n",
      "total loss: 1.6297280192375183\n",
      "Epoch [295/500], Loss: 0.8149\n",
      "total loss: 1.6349820494651794\n",
      "Epoch [296/500], Loss: 0.8175\n",
      "total loss: 1.626052439212799\n",
      "Epoch [297/500], Loss: 0.8130\n",
      "total loss: 1.6311431527137756\n",
      "Epoch [298/500], Loss: 0.8156\n",
      "total loss: 1.6284850239753723\n",
      "Epoch [299/500], Loss: 0.8142\n",
      "total loss: 1.6354131698608398\n",
      "Epoch [300/500], Loss: 0.8177\n",
      "total loss: 1.6347880959510803\n",
      "Epoch [301/500], Loss: 0.8174\n",
      "total loss: 1.6306821703910828\n",
      "Epoch [302/500], Loss: 0.8153\n",
      "total loss: 1.637073576450348\n",
      "Epoch [303/500], Loss: 0.8185\n",
      "total loss: 1.6301059126853943\n",
      "Epoch [304/500], Loss: 0.8151\n",
      "total loss: 1.6388052105903625\n",
      "Epoch [305/500], Loss: 0.8194\n",
      "total loss: 1.6351182460784912\n",
      "Epoch [306/500], Loss: 0.8176\n",
      "total loss: 1.6336647868156433\n",
      "Epoch [307/500], Loss: 0.8168\n",
      "total loss: 1.6319167613983154\n",
      "Epoch [308/500], Loss: 0.8160\n",
      "total loss: 1.6314235925674438\n",
      "Epoch [309/500], Loss: 0.8157\n",
      "total loss: 1.6283757090568542\n",
      "Epoch [310/500], Loss: 0.8142\n",
      "total loss: 1.633257508277893\n",
      "Epoch [311/500], Loss: 0.8166\n",
      "total loss: 1.6300960779190063\n",
      "Epoch [312/500], Loss: 0.8150\n",
      "total loss: 1.6320247054100037\n",
      "Epoch [313/500], Loss: 0.8160\n",
      "total loss: 1.6305637955665588\n",
      "Epoch [314/500], Loss: 0.8153\n",
      "total loss: 1.629946768283844\n",
      "Epoch [315/500], Loss: 0.8150\n",
      "total loss: 1.6293988227844238\n",
      "Epoch [316/500], Loss: 0.8147\n",
      "total loss: 1.6295825242996216\n",
      "Epoch [317/500], Loss: 0.8148\n",
      "total loss: 1.6278987526893616\n",
      "Epoch [318/500], Loss: 0.8139\n",
      "total loss: 1.6323544383049011\n",
      "Epoch [319/500], Loss: 0.8162\n",
      "total loss: 1.632571280002594\n",
      "Epoch [320/500], Loss: 0.8163\n",
      "total loss: 1.6284555196762085\n",
      "Epoch [321/500], Loss: 0.8142\n",
      "total loss: 1.6303699612617493\n",
      "Epoch [322/500], Loss: 0.8152\n",
      "total loss: 1.6318415999412537\n",
      "Epoch [323/500], Loss: 0.8159\n",
      "total loss: 1.631713628768921\n",
      "Epoch [324/500], Loss: 0.8159\n",
      "total loss: 1.6282119154930115\n",
      "Epoch [325/500], Loss: 0.8141\n",
      "total loss: 1.6290051341056824\n",
      "Epoch [326/500], Loss: 0.8145\n",
      "total loss: 1.6337506175041199\n",
      "Epoch [327/500], Loss: 0.8169\n",
      "total loss: 1.6275467276573181\n",
      "Epoch [328/500], Loss: 0.8138\n",
      "total loss: 1.6298633217811584\n",
      "Epoch [329/500], Loss: 0.8149\n",
      "total loss: 1.6317933201789856\n",
      "Epoch [330/500], Loss: 0.8159\n",
      "total loss: 1.6304437518119812\n",
      "Epoch [331/500], Loss: 0.8152\n",
      "total loss: 1.6317461729049683\n",
      "Epoch [332/500], Loss: 0.8159\n",
      "total loss: 1.630002498626709\n",
      "Epoch [333/500], Loss: 0.8150\n",
      "total loss: 1.6278995871543884\n",
      "Epoch [334/500], Loss: 0.8139\n",
      "total loss: 1.6293123960494995\n",
      "Epoch [335/500], Loss: 0.8147\n",
      "total loss: 1.6276229619979858\n",
      "Epoch [336/500], Loss: 0.8138\n",
      "total loss: 1.6280420422554016\n",
      "Epoch [337/500], Loss: 0.8140\n",
      "total loss: 1.6263207793235779\n",
      "Epoch [338/500], Loss: 0.8132\n",
      "total loss: 1.6307671070098877\n",
      "Epoch [339/500], Loss: 0.8154\n",
      "total loss: 1.627340853214264\n",
      "Epoch [340/500], Loss: 0.8137\n",
      "total loss: 1.6280436515808105\n",
      "Epoch [341/500], Loss: 0.8140\n",
      "total loss: 1.6239959597587585\n",
      "Epoch [342/500], Loss: 0.8120\n",
      "total loss: 1.6312878727912903\n",
      "Epoch [343/500], Loss: 0.8156\n",
      "total loss: 1.629922091960907\n",
      "Epoch [344/500], Loss: 0.8150\n",
      "total loss: 1.6300904154777527\n",
      "Epoch [345/500], Loss: 0.8150\n",
      "total loss: 1.6277978420257568\n",
      "Epoch [346/500], Loss: 0.8139\n",
      "total loss: 1.6257693767547607\n",
      "Epoch [347/500], Loss: 0.8129\n",
      "total loss: 1.6302664875984192\n",
      "Epoch [348/500], Loss: 0.8151\n",
      "total loss: 1.6318252086639404\n",
      "Epoch [349/500], Loss: 0.8159\n",
      "total loss: 1.6288946866989136\n",
      "Epoch [350/500], Loss: 0.8144\n",
      "total loss: 1.6239008903503418\n",
      "Epoch [351/500], Loss: 0.8120\n",
      "total loss: 1.6271114945411682\n",
      "Epoch [352/500], Loss: 0.8136\n",
      "total loss: 1.6245672702789307\n",
      "Epoch [353/500], Loss: 0.8123\n",
      "total loss: 1.6259833574295044\n",
      "Epoch [354/500], Loss: 0.8130\n",
      "total loss: 1.6298290491104126\n",
      "Epoch [355/500], Loss: 0.8149\n",
      "total loss: 1.622849702835083\n",
      "Epoch [356/500], Loss: 0.8114\n",
      "total loss: 1.632930040359497\n",
      "Epoch [357/500], Loss: 0.8165\n",
      "total loss: 1.6254025101661682\n",
      "Epoch [358/500], Loss: 0.8127\n",
      "total loss: 1.627111554145813\n",
      "Epoch [359/500], Loss: 0.8136\n",
      "total loss: 1.6266258358955383\n",
      "Epoch [360/500], Loss: 0.8133\n",
      "total loss: 1.6262821555137634\n",
      "Epoch [361/500], Loss: 0.8131\n",
      "total loss: 1.6260610222816467\n",
      "Epoch [362/500], Loss: 0.8130\n",
      "total loss: 1.624215006828308\n",
      "Epoch [363/500], Loss: 0.8121\n",
      "total loss: 1.6231459975242615\n",
      "Epoch [364/500], Loss: 0.8116\n",
      "total loss: 1.6237393617630005\n",
      "Epoch [365/500], Loss: 0.8119\n",
      "total loss: 1.6264644265174866\n",
      "Epoch [366/500], Loss: 0.8132\n",
      "total loss: 1.6259052157402039\n",
      "Epoch [367/500], Loss: 0.8130\n",
      "total loss: 1.6317495703697205\n",
      "Epoch [368/500], Loss: 0.8159\n",
      "total loss: 1.6225731372833252\n",
      "Epoch [369/500], Loss: 0.8113\n",
      "total loss: 1.6233580708503723\n",
      "Epoch [370/500], Loss: 0.8117\n",
      "total loss: 1.624373197555542\n",
      "Epoch [371/500], Loss: 0.8122\n",
      "total loss: 1.6313459277153015\n",
      "Epoch [372/500], Loss: 0.8157\n",
      "total loss: 1.6279505491256714\n",
      "Epoch [373/500], Loss: 0.8140\n",
      "total loss: 1.6244131922721863\n",
      "Epoch [374/500], Loss: 0.8122\n",
      "total loss: 1.6271219849586487\n",
      "Epoch [375/500], Loss: 0.8136\n",
      "total loss: 1.6234429478645325\n",
      "Epoch [376/500], Loss: 0.8117\n",
      "total loss: 1.6255854368209839\n",
      "Epoch [377/500], Loss: 0.8128\n",
      "total loss: 1.630502462387085\n",
      "Epoch [378/500], Loss: 0.8153\n",
      "total loss: 1.6264962553977966\n",
      "Epoch [379/500], Loss: 0.8132\n",
      "total loss: 1.6260507702827454\n",
      "Epoch [380/500], Loss: 0.8130\n",
      "total loss: 1.6215755343437195\n",
      "Epoch [381/500], Loss: 0.8108\n",
      "total loss: 1.6291757822036743\n",
      "Epoch [382/500], Loss: 0.8146\n",
      "total loss: 1.6248373985290527\n",
      "Epoch [383/500], Loss: 0.8124\n",
      "total loss: 1.6209295392036438\n",
      "Epoch [384/500], Loss: 0.8105\n",
      "total loss: 1.623271882534027\n",
      "Epoch [385/500], Loss: 0.8116\n",
      "total loss: 1.6203815937042236\n",
      "Epoch [386/500], Loss: 0.8102\n",
      "total loss: 1.6235464215278625\n",
      "Epoch [387/500], Loss: 0.8118\n",
      "total loss: 1.6215168237686157\n",
      "Epoch [388/500], Loss: 0.8108\n",
      "total loss: 1.6222489476203918\n",
      "Epoch [389/500], Loss: 0.8111\n",
      "total loss: 1.6294906735420227\n",
      "Epoch [390/500], Loss: 0.8147\n",
      "total loss: 1.6263002157211304\n",
      "Epoch [391/500], Loss: 0.8132\n",
      "total loss: 1.6288278698921204\n",
      "Epoch [392/500], Loss: 0.8144\n",
      "total loss: 1.6207393407821655\n",
      "Epoch [393/500], Loss: 0.8104\n",
      "total loss: 1.6286101341247559\n",
      "Epoch [394/500], Loss: 0.8143\n",
      "total loss: 1.6224359273910522\n",
      "Epoch [395/500], Loss: 0.8112\n",
      "total loss: 1.6240968108177185\n",
      "Epoch [396/500], Loss: 0.8120\n",
      "total loss: 1.6274796724319458\n",
      "Epoch [397/500], Loss: 0.8137\n",
      "total loss: 1.6209099292755127\n",
      "Epoch [398/500], Loss: 0.8105\n",
      "total loss: 1.6232719421386719\n",
      "Epoch [399/500], Loss: 0.8116\n",
      "total loss: 1.6205522418022156\n",
      "Epoch [400/500], Loss: 0.8103\n",
      "total loss: 1.6221029162406921\n",
      "Epoch [401/500], Loss: 0.8111\n",
      "total loss: 1.6226100325584412\n",
      "Epoch [402/500], Loss: 0.8113\n",
      "total loss: 1.6270132064819336\n",
      "Epoch [403/500], Loss: 0.8135\n",
      "total loss: 1.6287238001823425\n",
      "Epoch [404/500], Loss: 0.8144\n",
      "total loss: 1.6264864802360535\n",
      "Epoch [405/500], Loss: 0.8132\n",
      "total loss: 1.6271513104438782\n",
      "Epoch [406/500], Loss: 0.8136\n",
      "total loss: 1.61772882938385\n",
      "Epoch [407/500], Loss: 0.8089\n",
      "total loss: 1.6277421116828918\n",
      "Epoch [408/500], Loss: 0.8139\n",
      "total loss: 1.6276553273200989\n",
      "Epoch [409/500], Loss: 0.8138\n",
      "total loss: 1.6257016062736511\n",
      "Epoch [410/500], Loss: 0.8129\n",
      "total loss: 1.624218463897705\n",
      "Epoch [411/500], Loss: 0.8121\n",
      "total loss: 1.6230257153511047\n",
      "Epoch [412/500], Loss: 0.8115\n",
      "total loss: 1.6218241453170776\n",
      "Epoch [413/500], Loss: 0.8109\n",
      "total loss: 1.623019516468048\n",
      "Epoch [414/500], Loss: 0.8115\n",
      "total loss: 1.6231017112731934\n",
      "Epoch [415/500], Loss: 0.8116\n",
      "total loss: 1.6216801404953003\n",
      "Epoch [416/500], Loss: 0.8108\n",
      "total loss: 1.6254308819770813\n",
      "Epoch [417/500], Loss: 0.8127\n",
      "total loss: 1.6269106268882751\n",
      "Epoch [418/500], Loss: 0.8135\n",
      "total loss: 1.6154648661613464\n",
      "Epoch [419/500], Loss: 0.8077\n",
      "total loss: 1.6196730136871338\n",
      "Epoch [420/500], Loss: 0.8098\n",
      "total loss: 1.6253511309623718\n",
      "Epoch [421/500], Loss: 0.8127\n",
      "total loss: 1.620099127292633\n",
      "Epoch [422/500], Loss: 0.8100\n",
      "total loss: 1.6181915998458862\n",
      "Epoch [423/500], Loss: 0.8091\n",
      "total loss: 1.6173818111419678\n",
      "Epoch [424/500], Loss: 0.8087\n",
      "total loss: 1.6211289763450623\n",
      "Epoch [425/500], Loss: 0.8106\n",
      "total loss: 1.6233288645744324\n",
      "Epoch [426/500], Loss: 0.8117\n",
      "total loss: 1.623677909374237\n",
      "Epoch [427/500], Loss: 0.8118\n",
      "total loss: 1.6201680302619934\n",
      "Epoch [428/500], Loss: 0.8101\n",
      "total loss: 1.6180613040924072\n",
      "Epoch [429/500], Loss: 0.8090\n",
      "total loss: 1.6231774687767029\n",
      "Epoch [430/500], Loss: 0.8116\n",
      "total loss: 1.6252862215042114\n",
      "Epoch [431/500], Loss: 0.8126\n",
      "total loss: 1.623336374759674\n",
      "Epoch [432/500], Loss: 0.8117\n",
      "total loss: 1.6219383478164673\n",
      "Epoch [433/500], Loss: 0.8110\n",
      "total loss: 1.6203431487083435\n",
      "Epoch [434/500], Loss: 0.8102\n",
      "total loss: 1.6250187754631042\n",
      "Epoch [435/500], Loss: 0.8125\n",
      "total loss: 1.6178321838378906\n",
      "Epoch [436/500], Loss: 0.8089\n",
      "total loss: 1.6220941543579102\n",
      "Epoch [437/500], Loss: 0.8110\n",
      "total loss: 1.6182365417480469\n",
      "Epoch [438/500], Loss: 0.8091\n",
      "total loss: 1.6218645572662354\n",
      "Epoch [439/500], Loss: 0.8109\n",
      "total loss: 1.6252005100250244\n",
      "Epoch [440/500], Loss: 0.8126\n",
      "total loss: 1.6201368570327759\n",
      "Epoch [441/500], Loss: 0.8101\n",
      "total loss: 1.6215339303016663\n",
      "Epoch [442/500], Loss: 0.8108\n",
      "total loss: 1.6209588050842285\n",
      "Epoch [443/500], Loss: 0.8105\n",
      "total loss: 1.6201260089874268\n",
      "Epoch [444/500], Loss: 0.8101\n",
      "total loss: 1.6231980323791504\n",
      "Epoch [445/500], Loss: 0.8116\n",
      "total loss: 1.6209568977355957\n",
      "Epoch [446/500], Loss: 0.8105\n",
      "total loss: 1.6141111254692078\n",
      "Epoch [447/500], Loss: 0.8071\n",
      "total loss: 1.6215015649795532\n",
      "Epoch [448/500], Loss: 0.8108\n",
      "total loss: 1.620581567287445\n",
      "Epoch [449/500], Loss: 0.8103\n",
      "total loss: 1.6254779696464539\n",
      "Epoch [450/500], Loss: 0.8127\n",
      "total loss: 1.6216936111450195\n",
      "Epoch [451/500], Loss: 0.8108\n",
      "total loss: 1.6184210777282715\n",
      "Epoch [452/500], Loss: 0.8092\n",
      "total loss: 1.6215164065361023\n",
      "Epoch [453/500], Loss: 0.8108\n",
      "total loss: 1.613880693912506\n",
      "Epoch [454/500], Loss: 0.8069\n",
      "total loss: 1.6196109056472778\n",
      "Epoch [455/500], Loss: 0.8098\n",
      "total loss: 1.6154642701148987\n",
      "Epoch [456/500], Loss: 0.8077\n",
      "total loss: 1.6206257939338684\n",
      "Epoch [457/500], Loss: 0.8103\n",
      "total loss: 1.6214648485183716\n",
      "Epoch [458/500], Loss: 0.8107\n",
      "total loss: 1.6228472590446472\n",
      "Epoch [459/500], Loss: 0.8114\n",
      "total loss: 1.6244494915008545\n",
      "Epoch [460/500], Loss: 0.8122\n",
      "total loss: 1.6217959523200989\n",
      "Epoch [461/500], Loss: 0.8109\n",
      "total loss: 1.622582733631134\n",
      "Epoch [462/500], Loss: 0.8113\n",
      "total loss: 1.6194071173667908\n",
      "Epoch [463/500], Loss: 0.8097\n",
      "total loss: 1.620020866394043\n",
      "Epoch [464/500], Loss: 0.8100\n",
      "total loss: 1.6177661418914795\n",
      "Epoch [465/500], Loss: 0.8089\n",
      "total loss: 1.61958909034729\n",
      "Epoch [466/500], Loss: 0.8098\n",
      "total loss: 1.6274612545967102\n",
      "Epoch [467/500], Loss: 0.8137\n",
      "total loss: 1.6101515293121338\n",
      "Epoch [468/500], Loss: 0.8051\n",
      "total loss: 1.6193182468414307\n",
      "Epoch [469/500], Loss: 0.8097\n",
      "total loss: 1.6253097653388977\n",
      "Epoch [470/500], Loss: 0.8127\n",
      "total loss: 1.623391568660736\n",
      "Epoch [471/500], Loss: 0.8117\n",
      "total loss: 1.620555341243744\n",
      "Epoch [472/500], Loss: 0.8103\n",
      "total loss: 1.619678020477295\n",
      "Epoch [473/500], Loss: 0.8098\n",
      "total loss: 1.6210632920265198\n",
      "Epoch [474/500], Loss: 0.8105\n",
      "total loss: 1.6162428855895996\n",
      "Epoch [475/500], Loss: 0.8081\n",
      "total loss: 1.6210290789604187\n",
      "Epoch [476/500], Loss: 0.8105\n",
      "total loss: 1.623200535774231\n",
      "Epoch [477/500], Loss: 0.8116\n",
      "total loss: 1.6181462407112122\n",
      "Epoch [478/500], Loss: 0.8091\n",
      "total loss: 1.617214024066925\n",
      "Epoch [479/500], Loss: 0.8086\n",
      "total loss: 1.6238079071044922\n",
      "Epoch [480/500], Loss: 0.8119\n",
      "total loss: 1.6209254264831543\n",
      "Epoch [481/500], Loss: 0.8105\n",
      "total loss: 1.6193621158599854\n",
      "Epoch [482/500], Loss: 0.8097\n",
      "total loss: 1.6163451671600342\n",
      "Epoch [483/500], Loss: 0.8082\n",
      "total loss: 1.6210901737213135\n",
      "Epoch [484/500], Loss: 0.8105\n",
      "total loss: 1.616044282913208\n",
      "Epoch [485/500], Loss: 0.8080\n",
      "total loss: 1.620766282081604\n",
      "Epoch [486/500], Loss: 0.8104\n",
      "total loss: 1.623539686203003\n",
      "Epoch [487/500], Loss: 0.8118\n",
      "total loss: 1.6159308552742004\n",
      "Epoch [488/500], Loss: 0.8080\n",
      "total loss: 1.61762934923172\n",
      "Epoch [489/500], Loss: 0.8088\n",
      "total loss: 1.6220123171806335\n",
      "Epoch [490/500], Loss: 0.8110\n",
      "total loss: 1.6170336604118347\n",
      "Epoch [491/500], Loss: 0.8085\n",
      "total loss: 1.6151273846626282\n",
      "Epoch [492/500], Loss: 0.8076\n",
      "total loss: 1.622750163078308\n",
      "Epoch [493/500], Loss: 0.8114\n",
      "total loss: 1.6195461750030518\n",
      "Epoch [494/500], Loss: 0.8098\n",
      "total loss: 1.6174336075782776\n",
      "Epoch [495/500], Loss: 0.8087\n",
      "total loss: 1.6200594305992126\n",
      "Epoch [496/500], Loss: 0.8100\n",
      "total loss: 1.6150277853012085\n",
      "Epoch [497/500], Loss: 0.8075\n",
      "total loss: 1.6184066534042358\n",
      "Epoch [498/500], Loss: 0.8092\n",
      "total loss: 1.6211570501327515\n",
      "Epoch [499/500], Loss: 0.8106\n",
      "total loss: 1.615889549255371\n",
      "Epoch [500/500], Loss: 0.8079\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model\n",
    "trained_model = train_model(input_data, desired_output, 500, learning_rate=0.0005, viz_accuracy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(trained_model, new_data, expected_result):\n",
    "    test_input = torch.tensor(new_data, dtype=torch.float32)\n",
    "\n",
    "    test_output = trained_model(test_input)\n",
    "\n",
    "    predicted_classes = torch.argmax(test_output, dim=1)\n",
    "\n",
    "    test_output_compare = torch.tensor(expected_result, dtype=torch.int)\n",
    "\n",
    "    return float(sum(test_output_compare == predicted_classes)/len(test_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "just_input_test = test_df.drop('first_move_by_Q_learning', axis=1)\n",
    "just_input_test = just_input_test.drop('num_moves_Q_learning_needs', axis=1)\n",
    "\n",
    "input_data_test = np.array(just_input_test.values.tolist())\n",
    "desired_output = torch.tensor(test_df['first_move_by_Q_learning'].tolist(), dtype=torch.float32).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t9/gxb6q8zj21dff090q066td740000gn/T/ipykernel_10569/875939923.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_output_compare = torch.tensor(expected_result, dtype=torch.int)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9518449306488037"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(trained_model, input_data, torch.tensor(train['first_move_by_Q_learning'].tolist(), dtype=torch.float32).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t9/gxb6q8zj21dff090q066td740000gn/T/ipykernel_10569/875939923.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_output_compare = torch.tensor(expected_result, dtype=torch.int)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9400187730789185"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(trained_model, input_data_test, desired_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1, 3], [0, 1]])\n",
    "B = np.array([[1, 0], [3, 1]])\n",
    "\n",
    "# elements on the diagonal are 1 mod 4. \n",
    "# elements not on the diagonal are 0 mod 2. \n",
    "\n",
    "# C is the inverse of A\n",
    "# D is the inverse of B\n",
    "C = np.linalg.inv(A)\n",
    "D = np.linalg.inv(B)\n",
    "\n",
    "identity = np.array([[1, 0], [0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how long does it take us to get back to the origin? \n",
    "def matrix_to_tuple(matrix):\n",
    "    return (matrix[0][0], matrix[0][1], matrix[0][1], matrix[1][1])\n",
    "\n",
    "def num_steps_to_origin_w_nn(test_mat):\n",
    "    for i in range(100):\n",
    "        if (test_mat==identity).all():\n",
    "            return i\n",
    "        demo_test_type = torch.tensor([matrix_to_tuple(test_mat)], dtype=torch.float32)\n",
    "        action = int(torch.argmax(trained_model(demo_test_type)))\n",
    "\n",
    "        if action==0:\n",
    "            test_mat = test_mat @ A\n",
    "        elif action==1:\n",
    "            test_mat = test_mat @ B\n",
    "        elif action==2:\n",
    "            test_mat = test_mat @ C\n",
    "        else:\n",
    "            test_mat = test_mat @ D\n",
    "\n",
    "    return 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(trained_model.state_dict(), 'weights/subset3_sl2z_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_wrapper_for_num_steps(row):\n",
    "    arr = np.array([\n",
    "        [int(row['val1']), int(row['val2'])],\n",
    "        [int(row['val3']), int(row['val4'])]\n",
    "        ])\n",
    "    return num_steps_to_origin_w_nn(arr)\n",
    "\n",
    "test_df['num_steps_to_origin'] = test_df.apply(row_wrapper_for_num_steps, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(sum(test_df['num_steps_to_origin'] != 1000))\n",
    "\n",
    "just_a_few = test_df[test_df['num_steps_to_origin'] != 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_action(cur_matrix, action):\n",
    "    assert(action <= 3 or action >= 0)\n",
    "    if action==0:\n",
    "        return cur_matrix @ A\n",
    "    elif action==1:\n",
    "        return cur_matrix @ B\n",
    "    elif action==2:\n",
    "        return cur_matrix @ C\n",
    "    else:\n",
    "        return cur_matrix @ D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(mat):\n",
    "    demo_test_type = torch.tensor([matrix_to_tuple(mat)], dtype=torch.float32)\n",
    "    return int(torch.argmax(trained_model(demo_test_type)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_group_test = pd.read_csv(\"../Data_Generation/Data_files/test_rows_SL2Z_Q_learn.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "just_input = whole_group_test.drop('first_move_by_Q_learning', axis=1).drop('num_moves_Q_learning_needs', axis=1).to_numpy()\n",
    "# input_data = np.array(just_input.values.tolist())\n",
    "desired_output = torch.tensor(whole_group_test['first_move_by_Q_learning'].tolist(), dtype=torch.float32).long()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t9/gxb6q8zj21dff090q066td740000gn/T/ipykernel_10569/875939923.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_output_compare = torch.tensor(expected_result, dtype=torch.int)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9435216188430786"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test subgroup model on entire SL2Z dataset\n",
    "test_model(trained_model, just_input, desired_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "entire_gen = [np.array([[1, 2], [0, 1]]), np.array([[1, 0], [2, 1]]), np.linalg.inv(A), np.linalg.inv(B)]\n",
    "Q_table = None # todo fix :)\n",
    "def first_matrix_to_apply(cur_row):\n",
    "    outputs = [0, 0, 0, 0]\n",
    "    cur_matrix = np.array([\n",
    "        [int(cur_row['val1']), int(cur_row['val2'])], \n",
    "        [int(cur_row['val3']), int(cur_row['val4'])]\n",
    "        ])\n",
    "    for i in range(len(entire_gen)):\n",
    "        outputs[i] = Q_table[matrix_to_tuple(cur_matrix  @ entire_gen[i])]\n",
    "    return np.argmax(outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
