{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code adapted from ChatGPT\n",
    "\n",
    "# Define your neural network model\n",
    "class SL2Z_NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SL2Z_NN, self).__init__()\n",
    "\n",
    "        self.step1 = nn.Linear(4, 128, bias=True)\n",
    "        self.step2 = nn.ReLU()\n",
    "        self.step3 = nn.Linear(128, 64, bias=True)\n",
    "        self.step4 = nn.ReLU()\n",
    "        self.step5 = nn.Linear(64, 16, bias=True)\n",
    "        self.step6 = nn.ReLU()\n",
    "        self.step7 = nn.Linear(16, 4, bias=True)\n",
    "\n",
    "        # multi-class classification adapted from ChatGPT\n",
    "        self.step8 = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # RUN IT ON A GPU if it exists\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.to(\"cuda\")\n",
    "\n",
    "        x = self.step1(x)\n",
    "        x = self.step2(x)\n",
    "        x = self.step3(x)\n",
    "        x = self.step4(x)\n",
    "        x = self.step5(x)\n",
    "        x = self.step6(x)\n",
    "        x = self.step7(x)\n",
    "        x = self.step8(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def train_model(inputs, desired_outputs, num_epochs=100, learning_rate=0.01, viz_accuracy=False):\n",
    "    # Convert inputs and desired_outputs to PyTorch tensors\n",
    "    inputs = torch.tensor(inputs, dtype=torch.float32)\n",
    "    desired_outputs = torch.tensor(desired_outputs, dtype=torch.float32)\n",
    "    \n",
    "    # Create a DataLoader to handle batching (if needed)\n",
    "    dataset = TensorDataset(inputs, desired_outputs)\n",
    "    dataloader = DataLoader(dataset, batch_size=1000, shuffle=True)  # Adjust batch_size as needed\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = SL2Z_NN()\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()  # Mean Squared Error loss\n",
    "    # optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "    # criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.995)\n",
    "    losses = []\n",
    "\n",
    "    inputs = inputs.float()\n",
    "    desired_outputs = desired_outputs.long()\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for batch_inputs, batch_desired_outputs in dataloader:\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            outputs = model(batch_inputs)  # Forward pass\n",
    "\n",
    "            batch_desired_outputs = batch_desired_outputs.long()\n",
    "\n",
    "            loss = criterion(outputs, batch_desired_outputs)  # Compute the loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update the model's parameters\n",
    "            cur_item = loss.item()\n",
    "            total_loss += cur_item\n",
    "        \n",
    "        # Print the average loss for this epoch\n",
    "        scheduler.step()\n",
    "        print(f\"total loss: {total_loss}\")\n",
    "        average_loss = total_loss / len(dataloader)\n",
    "        if viz_accuracy:\n",
    "            losses.append(average_loss)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.4f}')\n",
    "    \n",
    "    if viz_accuracy:\n",
    "        plt.scatter(x=range(1, len(losses)+1), y=losses)\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Average loss\")\n",
    "        plt.show()\n",
    "\n",
    "    # Return the trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Data_Generation/Data_files/subset_train_rows_SL2Z_Q_learn.csv\")\n",
    "test_df = pd.read_csv(\"../Data_Generation/Data_files/subset_test_rows_SL2Z_Q_learn.csv\")\n",
    "\n",
    "\n",
    "train = df.drop(\"num_moves_Q_learning_needs\", axis=1)\n",
    "# random.seed(42)\n",
    "# np.random.seed(42)\n",
    "# reordered = df.sample(frac=1).reset_index(drop=True)\n",
    "# bound = int(reordered.shape[0] * 0.6)\n",
    "# plus_one = bound+1\n",
    "\n",
    "# train = reordered.iloc[1:bound]\n",
    "# test_df = reordered.iloc[plus_one:reordered.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "just_input = train.drop('first_move_by_Q_learning', axis=1)\n",
    "\n",
    "# adapted from https://stackoverflow.com/questions/43898035/pandas-combine-column-values-into-a-list-in-a-new-column\n",
    "input_data = np.array(just_input.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_output = torch.tensor(train['first_move_by_Q_learning'].tolist(), dtype=torch.float32).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t9/gxb6q8zj21dff090q066td740000gn/T/ipykernel_16434/3137828461.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  desired_outputs = torch.tensor(desired_outputs, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total loss: 2.795814633369446\n",
      "Epoch [1/500], Loss: 1.3979\n",
      "total loss: 2.6785625219345093\n",
      "Epoch [2/500], Loss: 1.3393\n",
      "total loss: 2.6268410682678223\n",
      "Epoch [3/500], Loss: 1.3134\n",
      "total loss: 2.579721450805664\n",
      "Epoch [4/500], Loss: 1.2899\n",
      "total loss: 2.539503574371338\n",
      "Epoch [5/500], Loss: 1.2698\n",
      "total loss: 2.4831490516662598\n",
      "Epoch [6/500], Loss: 1.2416\n",
      "total loss: 2.427849292755127\n",
      "Epoch [7/500], Loss: 1.2139\n",
      "total loss: 2.3863108158111572\n",
      "Epoch [8/500], Loss: 1.1932\n",
      "total loss: 2.348305106163025\n",
      "Epoch [9/500], Loss: 1.1742\n",
      "total loss: 2.311699151992798\n",
      "Epoch [10/500], Loss: 1.1558\n",
      "total loss: 2.2870677709579468\n",
      "Epoch [11/500], Loss: 1.1435\n",
      "total loss: 2.2694265842437744\n",
      "Epoch [12/500], Loss: 1.1347\n",
      "total loss: 2.217947483062744\n",
      "Epoch [13/500], Loss: 1.1090\n",
      "total loss: 2.1837432384490967\n",
      "Epoch [14/500], Loss: 1.0919\n",
      "total loss: 2.168055772781372\n",
      "Epoch [15/500], Loss: 1.0840\n",
      "total loss: 2.167367100715637\n",
      "Epoch [16/500], Loss: 1.0837\n",
      "total loss: 2.1484190225601196\n",
      "Epoch [17/500], Loss: 1.0742\n",
      "total loss: 2.138585090637207\n",
      "Epoch [18/500], Loss: 1.0693\n",
      "total loss: 2.1334407329559326\n",
      "Epoch [19/500], Loss: 1.0667\n",
      "total loss: 2.13134503364563\n",
      "Epoch [20/500], Loss: 1.0657\n",
      "total loss: 2.1188257932662964\n",
      "Epoch [21/500], Loss: 1.0594\n",
      "total loss: 2.114111542701721\n",
      "Epoch [22/500], Loss: 1.0571\n",
      "total loss: 2.1084107160568237\n",
      "Epoch [23/500], Loss: 1.0542\n",
      "total loss: 2.1088463068008423\n",
      "Epoch [24/500], Loss: 1.0544\n",
      "total loss: 2.0911827087402344\n",
      "Epoch [25/500], Loss: 1.0456\n",
      "total loss: 2.092442750930786\n",
      "Epoch [26/500], Loss: 1.0462\n",
      "total loss: 2.0931791067123413\n",
      "Epoch [27/500], Loss: 1.0466\n",
      "total loss: 2.0891445875167847\n",
      "Epoch [28/500], Loss: 1.0446\n",
      "total loss: 2.083103060722351\n",
      "Epoch [29/500], Loss: 1.0416\n",
      "total loss: 2.0805388689041138\n",
      "Epoch [30/500], Loss: 1.0403\n",
      "total loss: 2.073911428451538\n",
      "Epoch [31/500], Loss: 1.0370\n",
      "total loss: 2.067819356918335\n",
      "Epoch [32/500], Loss: 1.0339\n",
      "total loss: 2.0709868669509888\n",
      "Epoch [33/500], Loss: 1.0355\n",
      "total loss: 2.0636261701583862\n",
      "Epoch [34/500], Loss: 1.0318\n",
      "total loss: 2.0697535276412964\n",
      "Epoch [35/500], Loss: 1.0349\n",
      "total loss: 2.0563602447509766\n",
      "Epoch [36/500], Loss: 1.0282\n",
      "total loss: 2.0524877309799194\n",
      "Epoch [37/500], Loss: 1.0262\n",
      "total loss: 2.042846918106079\n",
      "Epoch [38/500], Loss: 1.0214\n",
      "total loss: 2.037458062171936\n",
      "Epoch [39/500], Loss: 1.0187\n",
      "total loss: 2.0250754356384277\n",
      "Epoch [40/500], Loss: 1.0125\n",
      "total loss: 2.0115763545036316\n",
      "Epoch [41/500], Loss: 1.0058\n",
      "total loss: 1.9951187372207642\n",
      "Epoch [42/500], Loss: 0.9976\n",
      "total loss: 1.9967352151870728\n",
      "Epoch [43/500], Loss: 0.9984\n",
      "total loss: 1.9821258783340454\n",
      "Epoch [44/500], Loss: 0.9911\n",
      "total loss: 1.9749935269355774\n",
      "Epoch [45/500], Loss: 0.9875\n",
      "total loss: 1.9617810249328613\n",
      "Epoch [46/500], Loss: 0.9809\n",
      "total loss: 1.9645874500274658\n",
      "Epoch [47/500], Loss: 0.9823\n",
      "total loss: 1.9462326765060425\n",
      "Epoch [48/500], Loss: 0.9731\n",
      "total loss: 1.9346713423728943\n",
      "Epoch [49/500], Loss: 0.9673\n",
      "total loss: 1.9214702248573303\n",
      "Epoch [50/500], Loss: 0.9607\n",
      "total loss: 1.903701663017273\n",
      "Epoch [51/500], Loss: 0.9519\n",
      "total loss: 1.9028688073158264\n",
      "Epoch [52/500], Loss: 0.9514\n",
      "total loss: 1.9053776264190674\n",
      "Epoch [53/500], Loss: 0.9527\n",
      "total loss: 1.8911994099617004\n",
      "Epoch [54/500], Loss: 0.9456\n",
      "total loss: 1.892282783985138\n",
      "Epoch [55/500], Loss: 0.9461\n",
      "total loss: 1.8807417154312134\n",
      "Epoch [56/500], Loss: 0.9404\n",
      "total loss: 1.8837255239486694\n",
      "Epoch [57/500], Loss: 0.9419\n",
      "total loss: 1.8709002137184143\n",
      "Epoch [58/500], Loss: 0.9355\n",
      "total loss: 1.868232786655426\n",
      "Epoch [59/500], Loss: 0.9341\n",
      "total loss: 1.8622354865074158\n",
      "Epoch [60/500], Loss: 0.9311\n",
      "total loss: 1.8682639598846436\n",
      "Epoch [61/500], Loss: 0.9341\n",
      "total loss: 1.8603898882865906\n",
      "Epoch [62/500], Loss: 0.9302\n",
      "total loss: 1.8431162238121033\n",
      "Epoch [63/500], Loss: 0.9216\n",
      "total loss: 1.8303865194320679\n",
      "Epoch [64/500], Loss: 0.9152\n",
      "total loss: 1.8253130316734314\n",
      "Epoch [65/500], Loss: 0.9127\n",
      "total loss: 1.8109607696533203\n",
      "Epoch [66/500], Loss: 0.9055\n",
      "total loss: 1.8072903156280518\n",
      "Epoch [67/500], Loss: 0.9036\n",
      "total loss: 1.8019327521324158\n",
      "Epoch [68/500], Loss: 0.9010\n",
      "total loss: 1.7918015122413635\n",
      "Epoch [69/500], Loss: 0.8959\n",
      "total loss: 1.782170295715332\n",
      "Epoch [70/500], Loss: 0.8911\n",
      "total loss: 1.7719730734825134\n",
      "Epoch [71/500], Loss: 0.8860\n",
      "total loss: 1.7572875618934631\n",
      "Epoch [72/500], Loss: 0.8786\n",
      "total loss: 1.7441914081573486\n",
      "Epoch [73/500], Loss: 0.8721\n",
      "total loss: 1.730067491531372\n",
      "Epoch [74/500], Loss: 0.8650\n",
      "total loss: 1.7015832662582397\n",
      "Epoch [75/500], Loss: 0.8508\n",
      "total loss: 1.67896968126297\n",
      "Epoch [76/500], Loss: 0.8395\n",
      "total loss: 1.666703999042511\n",
      "Epoch [77/500], Loss: 0.8334\n",
      "total loss: 1.665308952331543\n",
      "Epoch [78/500], Loss: 0.8327\n",
      "total loss: 1.6608468890190125\n",
      "Epoch [79/500], Loss: 0.8304\n",
      "total loss: 1.6565518975257874\n",
      "Epoch [80/500], Loss: 0.8283\n",
      "total loss: 1.6482051014900208\n",
      "Epoch [81/500], Loss: 0.8241\n",
      "total loss: 1.6464483737945557\n",
      "Epoch [82/500], Loss: 0.8232\n",
      "total loss: 1.6405993700027466\n",
      "Epoch [83/500], Loss: 0.8203\n",
      "total loss: 1.6379308700561523\n",
      "Epoch [84/500], Loss: 0.8190\n",
      "total loss: 1.6371516585350037\n",
      "Epoch [85/500], Loss: 0.8186\n",
      "total loss: 1.6338969469070435\n",
      "Epoch [86/500], Loss: 0.8169\n",
      "total loss: 1.6338546872138977\n",
      "Epoch [87/500], Loss: 0.8169\n",
      "total loss: 1.6298753023147583\n",
      "Epoch [88/500], Loss: 0.8149\n",
      "total loss: 1.6260467767715454\n",
      "Epoch [89/500], Loss: 0.8130\n",
      "total loss: 1.62553071975708\n",
      "Epoch [90/500], Loss: 0.8128\n",
      "total loss: 1.6241973042488098\n",
      "Epoch [91/500], Loss: 0.8121\n",
      "total loss: 1.6215901374816895\n",
      "Epoch [92/500], Loss: 0.8108\n",
      "total loss: 1.6197803616523743\n",
      "Epoch [93/500], Loss: 0.8099\n",
      "total loss: 1.6165665984153748\n",
      "Epoch [94/500], Loss: 0.8083\n",
      "total loss: 1.6137152910232544\n",
      "Epoch [95/500], Loss: 0.8069\n",
      "total loss: 1.6168374419212341\n",
      "Epoch [96/500], Loss: 0.8084\n",
      "total loss: 1.6171185374259949\n",
      "Epoch [97/500], Loss: 0.8086\n",
      "total loss: 1.6108484864234924\n",
      "Epoch [98/500], Loss: 0.8054\n",
      "total loss: 1.6083749532699585\n",
      "Epoch [99/500], Loss: 0.8042\n",
      "total loss: 1.6083120703697205\n",
      "Epoch [100/500], Loss: 0.8042\n",
      "total loss: 1.6066880822181702\n",
      "Epoch [101/500], Loss: 0.8033\n",
      "total loss: 1.6070263385772705\n",
      "Epoch [102/500], Loss: 0.8035\n",
      "total loss: 1.6051125526428223\n",
      "Epoch [103/500], Loss: 0.8026\n",
      "total loss: 1.6045327186584473\n",
      "Epoch [104/500], Loss: 0.8023\n",
      "total loss: 1.6019949913024902\n",
      "Epoch [105/500], Loss: 0.8010\n",
      "total loss: 1.6013768911361694\n",
      "Epoch [106/500], Loss: 0.8007\n",
      "total loss: 1.5990639925003052\n",
      "Epoch [107/500], Loss: 0.7995\n",
      "total loss: 1.59943026304245\n",
      "Epoch [108/500], Loss: 0.7997\n",
      "total loss: 1.5998783111572266\n",
      "Epoch [109/500], Loss: 0.7999\n",
      "total loss: 1.5989696979522705\n",
      "Epoch [110/500], Loss: 0.7995\n",
      "total loss: 1.5962844491004944\n",
      "Epoch [111/500], Loss: 0.7981\n",
      "total loss: 1.5944480895996094\n",
      "Epoch [112/500], Loss: 0.7972\n",
      "total loss: 1.5956898927688599\n",
      "Epoch [113/500], Loss: 0.7978\n",
      "total loss: 1.5936776399612427\n",
      "Epoch [114/500], Loss: 0.7968\n",
      "total loss: 1.5920242667198181\n",
      "Epoch [115/500], Loss: 0.7960\n",
      "total loss: 1.5915782451629639\n",
      "Epoch [116/500], Loss: 0.7958\n",
      "total loss: 1.589203417301178\n",
      "Epoch [117/500], Loss: 0.7946\n",
      "total loss: 1.5899824500083923\n",
      "Epoch [118/500], Loss: 0.7950\n",
      "total loss: 1.5895059704780579\n",
      "Epoch [119/500], Loss: 0.7948\n",
      "total loss: 1.590152621269226\n",
      "Epoch [120/500], Loss: 0.7951\n",
      "total loss: 1.589241623878479\n",
      "Epoch [121/500], Loss: 0.7946\n",
      "total loss: 1.587584376335144\n",
      "Epoch [122/500], Loss: 0.7938\n",
      "total loss: 1.5849183201789856\n",
      "Epoch [123/500], Loss: 0.7925\n",
      "total loss: 1.5840685963630676\n",
      "Epoch [124/500], Loss: 0.7920\n",
      "total loss: 1.583104133605957\n",
      "Epoch [125/500], Loss: 0.7916\n",
      "total loss: 1.5844042897224426\n",
      "Epoch [126/500], Loss: 0.7922\n",
      "total loss: 1.5835822820663452\n",
      "Epoch [127/500], Loss: 0.7918\n",
      "total loss: 1.582468330860138\n",
      "Epoch [128/500], Loss: 0.7912\n",
      "total loss: 1.582306146621704\n",
      "Epoch [129/500], Loss: 0.7912\n",
      "total loss: 1.579633355140686\n",
      "Epoch [130/500], Loss: 0.7898\n",
      "total loss: 1.5788086652755737\n",
      "Epoch [131/500], Loss: 0.7894\n",
      "total loss: 1.5785342454910278\n",
      "Epoch [132/500], Loss: 0.7893\n",
      "total loss: 1.5782460570335388\n",
      "Epoch [133/500], Loss: 0.7891\n",
      "total loss: 1.5779749751091003\n",
      "Epoch [134/500], Loss: 0.7890\n",
      "total loss: 1.5763846039772034\n",
      "Epoch [135/500], Loss: 0.7882\n",
      "total loss: 1.5765738487243652\n",
      "Epoch [136/500], Loss: 0.7883\n",
      "total loss: 1.575264811515808\n",
      "Epoch [137/500], Loss: 0.7876\n",
      "total loss: 1.5737956762313843\n",
      "Epoch [138/500], Loss: 0.7869\n",
      "total loss: 1.5735386610031128\n",
      "Epoch [139/500], Loss: 0.7868\n",
      "total loss: 1.5709532499313354\n",
      "Epoch [140/500], Loss: 0.7855\n",
      "total loss: 1.571835219860077\n",
      "Epoch [141/500], Loss: 0.7859\n",
      "total loss: 1.5717740058898926\n",
      "Epoch [142/500], Loss: 0.7859\n",
      "total loss: 1.5711300373077393\n",
      "Epoch [143/500], Loss: 0.7856\n",
      "total loss: 1.5695735812187195\n",
      "Epoch [144/500], Loss: 0.7848\n",
      "total loss: 1.5712595582008362\n",
      "Epoch [145/500], Loss: 0.7856\n",
      "total loss: 1.5685703754425049\n",
      "Epoch [146/500], Loss: 0.7843\n",
      "total loss: 1.569397747516632\n",
      "Epoch [147/500], Loss: 0.7847\n",
      "total loss: 1.567504644393921\n",
      "Epoch [148/500], Loss: 0.7838\n",
      "total loss: 1.5683818459510803\n",
      "Epoch [149/500], Loss: 0.7842\n",
      "total loss: 1.5671125054359436\n",
      "Epoch [150/500], Loss: 0.7836\n",
      "total loss: 1.567165195941925\n",
      "Epoch [151/500], Loss: 0.7836\n",
      "total loss: 1.5673208832740784\n",
      "Epoch [152/500], Loss: 0.7837\n",
      "total loss: 1.5662230253219604\n",
      "Epoch [153/500], Loss: 0.7831\n",
      "total loss: 1.5635415315628052\n",
      "Epoch [154/500], Loss: 0.7818\n",
      "total loss: 1.565036118030548\n",
      "Epoch [155/500], Loss: 0.7825\n",
      "total loss: 1.5638209581375122\n",
      "Epoch [156/500], Loss: 0.7819\n",
      "total loss: 1.5632205605506897\n",
      "Epoch [157/500], Loss: 0.7816\n",
      "total loss: 1.56295245885849\n",
      "Epoch [158/500], Loss: 0.7815\n",
      "total loss: 1.5628079175949097\n",
      "Epoch [159/500], Loss: 0.7814\n",
      "total loss: 1.5625123977661133\n",
      "Epoch [160/500], Loss: 0.7813\n",
      "total loss: 1.5637088418006897\n",
      "Epoch [161/500], Loss: 0.7819\n",
      "total loss: 1.5595558285713196\n",
      "Epoch [162/500], Loss: 0.7798\n",
      "total loss: 1.5608895421028137\n",
      "Epoch [163/500], Loss: 0.7804\n",
      "total loss: 1.5592146515846252\n",
      "Epoch [164/500], Loss: 0.7796\n",
      "total loss: 1.5615844130516052\n",
      "Epoch [165/500], Loss: 0.7808\n",
      "total loss: 1.5583224892616272\n",
      "Epoch [166/500], Loss: 0.7792\n",
      "total loss: 1.558496117591858\n",
      "Epoch [167/500], Loss: 0.7792\n",
      "total loss: 1.5578621625900269\n",
      "Epoch [168/500], Loss: 0.7789\n",
      "total loss: 1.5574836730957031\n",
      "Epoch [169/500], Loss: 0.7787\n",
      "total loss: 1.5579683184623718\n",
      "Epoch [170/500], Loss: 0.7790\n",
      "total loss: 1.5568078756332397\n",
      "Epoch [171/500], Loss: 0.7784\n",
      "total loss: 1.5567623972892761\n",
      "Epoch [172/500], Loss: 0.7784\n",
      "total loss: 1.5555892586708069\n",
      "Epoch [173/500], Loss: 0.7778\n",
      "total loss: 1.5577433705329895\n",
      "Epoch [174/500], Loss: 0.7789\n",
      "total loss: 1.553759217262268\n",
      "Epoch [175/500], Loss: 0.7769\n",
      "total loss: 1.5545719861984253\n",
      "Epoch [176/500], Loss: 0.7773\n",
      "total loss: 1.552808701992035\n",
      "Epoch [177/500], Loss: 0.7764\n",
      "total loss: 1.5535102486610413\n",
      "Epoch [178/500], Loss: 0.7768\n",
      "total loss: 1.5520912408828735\n",
      "Epoch [179/500], Loss: 0.7760\n",
      "total loss: 1.5519134402275085\n",
      "Epoch [180/500], Loss: 0.7760\n",
      "total loss: 1.552150547504425\n",
      "Epoch [181/500], Loss: 0.7761\n",
      "total loss: 1.5520363450050354\n",
      "Epoch [182/500], Loss: 0.7760\n",
      "total loss: 1.550448477268219\n",
      "Epoch [183/500], Loss: 0.7752\n",
      "total loss: 1.5499888062477112\n",
      "Epoch [184/500], Loss: 0.7750\n",
      "total loss: 1.5495336651802063\n",
      "Epoch [185/500], Loss: 0.7748\n",
      "total loss: 1.5501086115837097\n",
      "Epoch [186/500], Loss: 0.7751\n",
      "total loss: 1.5500783920288086\n",
      "Epoch [187/500], Loss: 0.7750\n",
      "total loss: 1.5501708388328552\n",
      "Epoch [188/500], Loss: 0.7751\n",
      "total loss: 1.5501046776771545\n",
      "Epoch [189/500], Loss: 0.7751\n",
      "total loss: 1.5493242144584656\n",
      "Epoch [190/500], Loss: 0.7747\n",
      "total loss: 1.5508118271827698\n",
      "Epoch [191/500], Loss: 0.7754\n",
      "total loss: 1.5483039617538452\n",
      "Epoch [192/500], Loss: 0.7742\n",
      "total loss: 1.5491986274719238\n",
      "Epoch [193/500], Loss: 0.7746\n",
      "total loss: 1.5483086705207825\n",
      "Epoch [194/500], Loss: 0.7742\n",
      "total loss: 1.546339213848114\n",
      "Epoch [195/500], Loss: 0.7732\n",
      "total loss: 1.5470502376556396\n",
      "Epoch [196/500], Loss: 0.7735\n",
      "total loss: 1.5473792552947998\n",
      "Epoch [197/500], Loss: 0.7737\n",
      "total loss: 1.5454846024513245\n",
      "Epoch [198/500], Loss: 0.7727\n",
      "total loss: 1.5464593172073364\n",
      "Epoch [199/500], Loss: 0.7732\n",
      "total loss: 1.5456234812736511\n",
      "Epoch [200/500], Loss: 0.7728\n",
      "total loss: 1.5449190735816956\n",
      "Epoch [201/500], Loss: 0.7725\n",
      "total loss: 1.54486882686615\n",
      "Epoch [202/500], Loss: 0.7724\n",
      "total loss: 1.545328140258789\n",
      "Epoch [203/500], Loss: 0.7727\n",
      "total loss: 1.544553279876709\n",
      "Epoch [204/500], Loss: 0.7723\n",
      "total loss: 1.5442304611206055\n",
      "Epoch [205/500], Loss: 0.7721\n",
      "total loss: 1.5445793271064758\n",
      "Epoch [206/500], Loss: 0.7723\n",
      "total loss: 1.5423139333724976\n",
      "Epoch [207/500], Loss: 0.7712\n",
      "total loss: 1.5427913665771484\n",
      "Epoch [208/500], Loss: 0.7714\n",
      "total loss: 1.5415382981300354\n",
      "Epoch [209/500], Loss: 0.7708\n",
      "total loss: 1.5416119694709778\n",
      "Epoch [210/500], Loss: 0.7708\n",
      "total loss: 1.5430250763893127\n",
      "Epoch [211/500], Loss: 0.7715\n",
      "total loss: 1.5421268343925476\n",
      "Epoch [212/500], Loss: 0.7711\n",
      "total loss: 1.5431623458862305\n",
      "Epoch [213/500], Loss: 0.7716\n",
      "total loss: 1.5436528325080872\n",
      "Epoch [214/500], Loss: 0.7718\n",
      "total loss: 1.5421708822250366\n",
      "Epoch [215/500], Loss: 0.7711\n",
      "total loss: 1.5399162769317627\n",
      "Epoch [216/500], Loss: 0.7700\n",
      "total loss: 1.5402582883834839\n",
      "Epoch [217/500], Loss: 0.7701\n",
      "total loss: 1.5398025512695312\n",
      "Epoch [218/500], Loss: 0.7699\n",
      "total loss: 1.5386250019073486\n",
      "Epoch [219/500], Loss: 0.7693\n",
      "total loss: 1.5402553081512451\n",
      "Epoch [220/500], Loss: 0.7701\n",
      "total loss: 1.5396243333816528\n",
      "Epoch [221/500], Loss: 0.7698\n",
      "total loss: 1.5399656891822815\n",
      "Epoch [222/500], Loss: 0.7700\n",
      "total loss: 1.5390141606330872\n",
      "Epoch [223/500], Loss: 0.7695\n",
      "total loss: 1.5393638610839844\n",
      "Epoch [224/500], Loss: 0.7697\n",
      "total loss: 1.537438452243805\n",
      "Epoch [225/500], Loss: 0.7687\n",
      "total loss: 1.539457619190216\n",
      "Epoch [226/500], Loss: 0.7697\n",
      "total loss: 1.5379745960235596\n",
      "Epoch [227/500], Loss: 0.7690\n",
      "total loss: 1.5370306968688965\n",
      "Epoch [228/500], Loss: 0.7685\n",
      "total loss: 1.5378153920173645\n",
      "Epoch [229/500], Loss: 0.7689\n",
      "total loss: 1.5380871295928955\n",
      "Epoch [230/500], Loss: 0.7690\n",
      "total loss: 1.5386421084403992\n",
      "Epoch [231/500], Loss: 0.7693\n",
      "total loss: 1.5348638892173767\n",
      "Epoch [232/500], Loss: 0.7674\n",
      "total loss: 1.5371001362800598\n",
      "Epoch [233/500], Loss: 0.7686\n",
      "total loss: 1.5375311374664307\n",
      "Epoch [234/500], Loss: 0.7688\n",
      "total loss: 1.5368878245353699\n",
      "Epoch [235/500], Loss: 0.7684\n",
      "total loss: 1.5343472361564636\n",
      "Epoch [236/500], Loss: 0.7672\n",
      "total loss: 1.5368155241012573\n",
      "Epoch [237/500], Loss: 0.7684\n",
      "total loss: 1.5354978442192078\n",
      "Epoch [238/500], Loss: 0.7677\n",
      "total loss: 1.5362030267715454\n",
      "Epoch [239/500], Loss: 0.7681\n",
      "total loss: 1.5352431535720825\n",
      "Epoch [240/500], Loss: 0.7676\n",
      "total loss: 1.5346606969833374\n",
      "Epoch [241/500], Loss: 0.7673\n",
      "total loss: 1.5357072353363037\n",
      "Epoch [242/500], Loss: 0.7679\n",
      "total loss: 1.535021185874939\n",
      "Epoch [243/500], Loss: 0.7675\n",
      "total loss: 1.5349926948547363\n",
      "Epoch [244/500], Loss: 0.7675\n",
      "total loss: 1.5342179536819458\n",
      "Epoch [245/500], Loss: 0.7671\n",
      "total loss: 1.5330981612205505\n",
      "Epoch [246/500], Loss: 0.7665\n",
      "total loss: 1.5333335995674133\n",
      "Epoch [247/500], Loss: 0.7667\n",
      "total loss: 1.5334513783454895\n",
      "Epoch [248/500], Loss: 0.7667\n",
      "total loss: 1.532805621623993\n",
      "Epoch [249/500], Loss: 0.7664\n",
      "total loss: 1.5333123207092285\n",
      "Epoch [250/500], Loss: 0.7667\n",
      "total loss: 1.5319433212280273\n",
      "Epoch [251/500], Loss: 0.7660\n",
      "total loss: 1.5330241918563843\n",
      "Epoch [252/500], Loss: 0.7665\n",
      "total loss: 1.5325764417648315\n",
      "Epoch [253/500], Loss: 0.7663\n",
      "total loss: 1.5327699780464172\n",
      "Epoch [254/500], Loss: 0.7664\n",
      "total loss: 1.532052218914032\n",
      "Epoch [255/500], Loss: 0.7660\n",
      "total loss: 1.5324114561080933\n",
      "Epoch [256/500], Loss: 0.7662\n",
      "total loss: 1.5306721329689026\n",
      "Epoch [257/500], Loss: 0.7653\n",
      "total loss: 1.5322002172470093\n",
      "Epoch [258/500], Loss: 0.7661\n",
      "total loss: 1.5306851863861084\n",
      "Epoch [259/500], Loss: 0.7653\n",
      "total loss: 1.530979037284851\n",
      "Epoch [260/500], Loss: 0.7655\n",
      "total loss: 1.5309264659881592\n",
      "Epoch [261/500], Loss: 0.7655\n",
      "total loss: 1.5305180549621582\n",
      "Epoch [262/500], Loss: 0.7653\n",
      "total loss: 1.5303964018821716\n",
      "Epoch [263/500], Loss: 0.7652\n",
      "total loss: 1.5303365588188171\n",
      "Epoch [264/500], Loss: 0.7652\n",
      "total loss: 1.5292776226997375\n",
      "Epoch [265/500], Loss: 0.7646\n",
      "total loss: 1.5309130549430847\n",
      "Epoch [266/500], Loss: 0.7655\n",
      "total loss: 1.5291510224342346\n",
      "Epoch [267/500], Loss: 0.7646\n",
      "total loss: 1.52977454662323\n",
      "Epoch [268/500], Loss: 0.7649\n",
      "total loss: 1.5301414132118225\n",
      "Epoch [269/500], Loss: 0.7651\n",
      "total loss: 1.5293754935264587\n",
      "Epoch [270/500], Loss: 0.7647\n",
      "total loss: 1.5291352272033691\n",
      "Epoch [271/500], Loss: 0.7646\n",
      "total loss: 1.5300860404968262\n",
      "Epoch [272/500], Loss: 0.7650\n",
      "total loss: 1.5292353630065918\n",
      "Epoch [273/500], Loss: 0.7646\n",
      "total loss: 1.5288984775543213\n",
      "Epoch [274/500], Loss: 0.7644\n",
      "total loss: 1.528315007686615\n",
      "Epoch [275/500], Loss: 0.7642\n",
      "total loss: 1.5284903645515442\n",
      "Epoch [276/500], Loss: 0.7642\n",
      "total loss: 1.528016984462738\n",
      "Epoch [277/500], Loss: 0.7640\n",
      "total loss: 1.5281904339790344\n",
      "Epoch [278/500], Loss: 0.7641\n",
      "total loss: 1.5294092297554016\n",
      "Epoch [279/500], Loss: 0.7647\n",
      "total loss: 1.5275052189826965\n",
      "Epoch [280/500], Loss: 0.7638\n",
      "total loss: 1.5270568132400513\n",
      "Epoch [281/500], Loss: 0.7635\n",
      "total loss: 1.5262196063995361\n",
      "Epoch [282/500], Loss: 0.7631\n",
      "total loss: 1.5284903049468994\n",
      "Epoch [283/500], Loss: 0.7642\n",
      "total loss: 1.5271306037902832\n",
      "Epoch [284/500], Loss: 0.7636\n",
      "total loss: 1.5281643271446228\n",
      "Epoch [285/500], Loss: 0.7641\n",
      "total loss: 1.5270258784294128\n",
      "Epoch [286/500], Loss: 0.7635\n",
      "total loss: 1.5273921489715576\n",
      "Epoch [287/500], Loss: 0.7637\n",
      "total loss: 1.5278578400611877\n",
      "Epoch [288/500], Loss: 0.7639\n",
      "total loss: 1.5269887447357178\n",
      "Epoch [289/500], Loss: 0.7635\n",
      "total loss: 1.5264105200767517\n",
      "Epoch [290/500], Loss: 0.7632\n",
      "total loss: 1.5263921022415161\n",
      "Epoch [291/500], Loss: 0.7632\n",
      "total loss: 1.5261152982711792\n",
      "Epoch [292/500], Loss: 0.7631\n",
      "total loss: 1.5266472697257996\n",
      "Epoch [293/500], Loss: 0.7633\n",
      "total loss: 1.5268794894218445\n",
      "Epoch [294/500], Loss: 0.7634\n",
      "total loss: 1.52610844373703\n",
      "Epoch [295/500], Loss: 0.7631\n",
      "total loss: 1.5268206596374512\n",
      "Epoch [296/500], Loss: 0.7634\n",
      "total loss: 1.5253217220306396\n",
      "Epoch [297/500], Loss: 0.7627\n",
      "total loss: 1.5269685983657837\n",
      "Epoch [298/500], Loss: 0.7635\n",
      "total loss: 1.5259913206100464\n",
      "Epoch [299/500], Loss: 0.7630\n",
      "total loss: 1.525510549545288\n",
      "Epoch [300/500], Loss: 0.7628\n",
      "total loss: 1.5241484642028809\n",
      "Epoch [301/500], Loss: 0.7621\n",
      "total loss: 1.5246331691741943\n",
      "Epoch [302/500], Loss: 0.7623\n",
      "total loss: 1.5251622200012207\n",
      "Epoch [303/500], Loss: 0.7626\n",
      "total loss: 1.5247923731803894\n",
      "Epoch [304/500], Loss: 0.7624\n",
      "total loss: 1.5243090391159058\n",
      "Epoch [305/500], Loss: 0.7622\n",
      "total loss: 1.52501380443573\n",
      "Epoch [306/500], Loss: 0.7625\n",
      "total loss: 1.5243314504623413\n",
      "Epoch [307/500], Loss: 0.7622\n",
      "total loss: 1.5239344239234924\n",
      "Epoch [308/500], Loss: 0.7620\n",
      "total loss: 1.5246326923370361\n",
      "Epoch [309/500], Loss: 0.7623\n",
      "total loss: 1.5235166549682617\n",
      "Epoch [310/500], Loss: 0.7618\n",
      "total loss: 1.5227920413017273\n",
      "Epoch [311/500], Loss: 0.7614\n",
      "total loss: 1.5236389636993408\n",
      "Epoch [312/500], Loss: 0.7618\n",
      "total loss: 1.5232062339782715\n",
      "Epoch [313/500], Loss: 0.7616\n",
      "total loss: 1.5226863622665405\n",
      "Epoch [314/500], Loss: 0.7613\n",
      "total loss: 1.5235050320625305\n",
      "Epoch [315/500], Loss: 0.7618\n",
      "total loss: 1.523304283618927\n",
      "Epoch [316/500], Loss: 0.7617\n",
      "total loss: 1.5236534476280212\n",
      "Epoch [317/500], Loss: 0.7618\n",
      "total loss: 1.5227289199829102\n",
      "Epoch [318/500], Loss: 0.7614\n",
      "total loss: 1.5235333442687988\n",
      "Epoch [319/500], Loss: 0.7618\n",
      "total loss: 1.5237579941749573\n",
      "Epoch [320/500], Loss: 0.7619\n",
      "total loss: 1.522382378578186\n",
      "Epoch [321/500], Loss: 0.7612\n",
      "total loss: 1.523018717765808\n",
      "Epoch [322/500], Loss: 0.7615\n",
      "total loss: 1.5221400260925293\n",
      "Epoch [323/500], Loss: 0.7611\n",
      "total loss: 1.5226908922195435\n",
      "Epoch [324/500], Loss: 0.7613\n",
      "total loss: 1.5231301188468933\n",
      "Epoch [325/500], Loss: 0.7616\n",
      "total loss: 1.5213305354118347\n",
      "Epoch [326/500], Loss: 0.7607\n",
      "total loss: 1.5226058959960938\n",
      "Epoch [327/500], Loss: 0.7613\n",
      "total loss: 1.5219305157661438\n",
      "Epoch [328/500], Loss: 0.7610\n",
      "total loss: 1.5216203927993774\n",
      "Epoch [329/500], Loss: 0.7608\n",
      "total loss: 1.5220435857772827\n",
      "Epoch [330/500], Loss: 0.7610\n",
      "total loss: 1.5224103927612305\n",
      "Epoch [331/500], Loss: 0.7612\n",
      "total loss: 1.5211710929870605\n",
      "Epoch [332/500], Loss: 0.7606\n",
      "total loss: 1.5213133692741394\n",
      "Epoch [333/500], Loss: 0.7607\n",
      "total loss: 1.5213754177093506\n",
      "Epoch [334/500], Loss: 0.7607\n",
      "total loss: 1.520577073097229\n",
      "Epoch [335/500], Loss: 0.7603\n",
      "total loss: 1.5214637517929077\n",
      "Epoch [336/500], Loss: 0.7607\n",
      "total loss: 1.5220153331756592\n",
      "Epoch [337/500], Loss: 0.7610\n",
      "total loss: 1.520868718624115\n",
      "Epoch [338/500], Loss: 0.7604\n",
      "total loss: 1.5201945900917053\n",
      "Epoch [339/500], Loss: 0.7601\n",
      "total loss: 1.5200879573822021\n",
      "Epoch [340/500], Loss: 0.7600\n",
      "total loss: 1.5197803378105164\n",
      "Epoch [341/500], Loss: 0.7599\n",
      "total loss: 1.5217587351799011\n",
      "Epoch [342/500], Loss: 0.7609\n",
      "total loss: 1.519673764705658\n",
      "Epoch [343/500], Loss: 0.7598\n",
      "total loss: 1.5210344195365906\n",
      "Epoch [344/500], Loss: 0.7605\n",
      "total loss: 1.5215805768966675\n",
      "Epoch [345/500], Loss: 0.7608\n",
      "total loss: 1.5211113095283508\n",
      "Epoch [346/500], Loss: 0.7606\n",
      "total loss: 1.520380437374115\n",
      "Epoch [347/500], Loss: 0.7602\n",
      "total loss: 1.5204191207885742\n",
      "Epoch [348/500], Loss: 0.7602\n",
      "total loss: 1.5206792950630188\n",
      "Epoch [349/500], Loss: 0.7603\n",
      "total loss: 1.5195204615592957\n",
      "Epoch [350/500], Loss: 0.7598\n",
      "total loss: 1.520039141178131\n",
      "Epoch [351/500], Loss: 0.7600\n",
      "total loss: 1.5193114280700684\n",
      "Epoch [352/500], Loss: 0.7597\n",
      "total loss: 1.519318163394928\n",
      "Epoch [353/500], Loss: 0.7597\n",
      "total loss: 1.5207734107971191\n",
      "Epoch [354/500], Loss: 0.7604\n",
      "total loss: 1.5191490054130554\n",
      "Epoch [355/500], Loss: 0.7596\n",
      "total loss: 1.519090175628662\n",
      "Epoch [356/500], Loss: 0.7595\n",
      "total loss: 1.5190290808677673\n",
      "Epoch [357/500], Loss: 0.7595\n",
      "total loss: 1.519609272480011\n",
      "Epoch [358/500], Loss: 0.7598\n",
      "total loss: 1.5187088251113892\n",
      "Epoch [359/500], Loss: 0.7594\n",
      "total loss: 1.5204420685768127\n",
      "Epoch [360/500], Loss: 0.7602\n",
      "total loss: 1.5192583203315735\n",
      "Epoch [361/500], Loss: 0.7596\n",
      "total loss: 1.5197799801826477\n",
      "Epoch [362/500], Loss: 0.7599\n",
      "total loss: 1.5199626088142395\n",
      "Epoch [363/500], Loss: 0.7600\n",
      "total loss: 1.5200077891349792\n",
      "Epoch [364/500], Loss: 0.7600\n",
      "total loss: 1.5180968046188354\n",
      "Epoch [365/500], Loss: 0.7590\n",
      "total loss: 1.518408477306366\n",
      "Epoch [366/500], Loss: 0.7592\n",
      "total loss: 1.5183536410331726\n",
      "Epoch [367/500], Loss: 0.7592\n",
      "total loss: 1.518809199333191\n",
      "Epoch [368/500], Loss: 0.7594\n",
      "total loss: 1.518736720085144\n",
      "Epoch [369/500], Loss: 0.7594\n",
      "total loss: 1.5190900564193726\n",
      "Epoch [370/500], Loss: 0.7595\n",
      "total loss: 1.5188321471214294\n",
      "Epoch [371/500], Loss: 0.7594\n",
      "total loss: 1.517637848854065\n",
      "Epoch [372/500], Loss: 0.7588\n",
      "total loss: 1.5182417035102844\n",
      "Epoch [373/500], Loss: 0.7591\n",
      "total loss: 1.5177777409553528\n",
      "Epoch [374/500], Loss: 0.7589\n",
      "total loss: 1.5183732509613037\n",
      "Epoch [375/500], Loss: 0.7592\n",
      "total loss: 1.5178325772285461\n",
      "Epoch [376/500], Loss: 0.7589\n",
      "total loss: 1.5172641277313232\n",
      "Epoch [377/500], Loss: 0.7586\n",
      "total loss: 1.5175127387046814\n",
      "Epoch [378/500], Loss: 0.7588\n",
      "total loss: 1.5173930525779724\n",
      "Epoch [379/500], Loss: 0.7587\n",
      "total loss: 1.5181527137756348\n",
      "Epoch [380/500], Loss: 0.7591\n",
      "total loss: 1.5180333256721497\n",
      "Epoch [381/500], Loss: 0.7590\n",
      "total loss: 1.5181524157524109\n",
      "Epoch [382/500], Loss: 0.7591\n",
      "total loss: 1.5181882977485657\n",
      "Epoch [383/500], Loss: 0.7591\n",
      "total loss: 1.5173922181129456\n",
      "Epoch [384/500], Loss: 0.7587\n",
      "total loss: 1.516688048839569\n",
      "Epoch [385/500], Loss: 0.7583\n",
      "total loss: 1.5172406435012817\n",
      "Epoch [386/500], Loss: 0.7586\n",
      "total loss: 1.5171472430229187\n",
      "Epoch [387/500], Loss: 0.7586\n",
      "total loss: 1.5173529386520386\n",
      "Epoch [388/500], Loss: 0.7587\n",
      "total loss: 1.5171997547149658\n",
      "Epoch [389/500], Loss: 0.7586\n",
      "total loss: 1.5170626640319824\n",
      "Epoch [390/500], Loss: 0.7585\n",
      "total loss: 1.5162853598594666\n",
      "Epoch [391/500], Loss: 0.7581\n",
      "total loss: 1.516586720943451\n",
      "Epoch [392/500], Loss: 0.7583\n",
      "total loss: 1.5175974369049072\n",
      "Epoch [393/500], Loss: 0.7588\n",
      "total loss: 1.5168131589889526\n",
      "Epoch [394/500], Loss: 0.7584\n",
      "total loss: 1.5165446400642395\n",
      "Epoch [395/500], Loss: 0.7583\n",
      "total loss: 1.5169687867164612\n",
      "Epoch [396/500], Loss: 0.7585\n",
      "total loss: 1.5161604285240173\n",
      "Epoch [397/500], Loss: 0.7581\n",
      "total loss: 1.516126811504364\n",
      "Epoch [398/500], Loss: 0.7581\n",
      "total loss: 1.5155382752418518\n",
      "Epoch [399/500], Loss: 0.7578\n",
      "total loss: 1.517394483089447\n",
      "Epoch [400/500], Loss: 0.7587\n",
      "total loss: 1.5165512561798096\n",
      "Epoch [401/500], Loss: 0.7583\n",
      "total loss: 1.5162292122840881\n",
      "Epoch [402/500], Loss: 0.7581\n",
      "total loss: 1.5162786841392517\n",
      "Epoch [403/500], Loss: 0.7581\n",
      "total loss: 1.5165026783943176\n",
      "Epoch [404/500], Loss: 0.7583\n",
      "total loss: 1.5155794620513916\n",
      "Epoch [405/500], Loss: 0.7578\n",
      "total loss: 1.5155683159828186\n",
      "Epoch [406/500], Loss: 0.7578\n",
      "total loss: 1.515821933746338\n",
      "Epoch [407/500], Loss: 0.7579\n",
      "total loss: 1.5153473019599915\n",
      "Epoch [408/500], Loss: 0.7577\n",
      "total loss: 1.5160841941833496\n",
      "Epoch [409/500], Loss: 0.7580\n",
      "total loss: 1.5150968432426453\n",
      "Epoch [410/500], Loss: 0.7575\n",
      "total loss: 1.5170502066612244\n",
      "Epoch [411/500], Loss: 0.7585\n",
      "total loss: 1.515762984752655\n",
      "Epoch [412/500], Loss: 0.7579\n",
      "total loss: 1.5156446695327759\n",
      "Epoch [413/500], Loss: 0.7578\n",
      "total loss: 1.515785038471222\n",
      "Epoch [414/500], Loss: 0.7579\n",
      "total loss: 1.5152339339256287\n",
      "Epoch [415/500], Loss: 0.7576\n",
      "total loss: 1.5162349343299866\n",
      "Epoch [416/500], Loss: 0.7581\n",
      "total loss: 1.5150812864303589\n",
      "Epoch [417/500], Loss: 0.7575\n",
      "total loss: 1.5153248310089111\n",
      "Epoch [418/500], Loss: 0.7577\n",
      "total loss: 1.5148253440856934\n",
      "Epoch [419/500], Loss: 0.7574\n",
      "total loss: 1.5148311853408813\n",
      "Epoch [420/500], Loss: 0.7574\n",
      "total loss: 1.5156632661819458\n",
      "Epoch [421/500], Loss: 0.7578\n",
      "total loss: 1.5158297419548035\n",
      "Epoch [422/500], Loss: 0.7579\n",
      "total loss: 1.5155529379844666\n",
      "Epoch [423/500], Loss: 0.7578\n",
      "total loss: 1.514793336391449\n",
      "Epoch [424/500], Loss: 0.7574\n",
      "total loss: 1.5147778987884521\n",
      "Epoch [425/500], Loss: 0.7574\n",
      "total loss: 1.5156593322753906\n",
      "Epoch [426/500], Loss: 0.7578\n",
      "total loss: 1.5154967308044434\n",
      "Epoch [427/500], Loss: 0.7577\n",
      "total loss: 1.5147117376327515\n",
      "Epoch [428/500], Loss: 0.7574\n",
      "total loss: 1.514845371246338\n",
      "Epoch [429/500], Loss: 0.7574\n",
      "total loss: 1.5148680806159973\n",
      "Epoch [430/500], Loss: 0.7574\n",
      "total loss: 1.5135722160339355\n",
      "Epoch [431/500], Loss: 0.7568\n",
      "total loss: 1.515956461429596\n",
      "Epoch [432/500], Loss: 0.7580\n",
      "total loss: 1.513613760471344\n",
      "Epoch [433/500], Loss: 0.7568\n",
      "total loss: 1.513642966747284\n",
      "Epoch [434/500], Loss: 0.7568\n",
      "total loss: 1.5142117738723755\n",
      "Epoch [435/500], Loss: 0.7571\n",
      "total loss: 1.5146336555480957\n",
      "Epoch [436/500], Loss: 0.7573\n",
      "total loss: 1.5145790576934814\n",
      "Epoch [437/500], Loss: 0.7573\n",
      "total loss: 1.513703465461731\n",
      "Epoch [438/500], Loss: 0.7569\n",
      "total loss: 1.513212263584137\n",
      "Epoch [439/500], Loss: 0.7566\n",
      "total loss: 1.5142608880996704\n",
      "Epoch [440/500], Loss: 0.7571\n",
      "total loss: 1.5144484639167786\n",
      "Epoch [441/500], Loss: 0.7572\n",
      "total loss: 1.5138864517211914\n",
      "Epoch [442/500], Loss: 0.7569\n",
      "total loss: 1.5137168169021606\n",
      "Epoch [443/500], Loss: 0.7569\n",
      "total loss: 1.5138712525367737\n",
      "Epoch [444/500], Loss: 0.7569\n",
      "total loss: 1.5134412050247192\n",
      "Epoch [445/500], Loss: 0.7567\n",
      "total loss: 1.5136698484420776\n",
      "Epoch [446/500], Loss: 0.7568\n",
      "total loss: 1.5137439370155334\n",
      "Epoch [447/500], Loss: 0.7569\n",
      "total loss: 1.5135964751243591\n",
      "Epoch [448/500], Loss: 0.7568\n",
      "total loss: 1.514233410358429\n",
      "Epoch [449/500], Loss: 0.7571\n",
      "total loss: 1.513484537601471\n",
      "Epoch [450/500], Loss: 0.7567\n",
      "total loss: 1.5147383213043213\n",
      "Epoch [451/500], Loss: 0.7574\n",
      "total loss: 1.5133861899375916\n",
      "Epoch [452/500], Loss: 0.7567\n",
      "total loss: 1.5144774913787842\n",
      "Epoch [453/500], Loss: 0.7572\n",
      "total loss: 1.5137039422988892\n",
      "Epoch [454/500], Loss: 0.7569\n",
      "total loss: 1.513439416885376\n",
      "Epoch [455/500], Loss: 0.7567\n",
      "total loss: 1.5138342380523682\n",
      "Epoch [456/500], Loss: 0.7569\n",
      "total loss: 1.5128796696662903\n",
      "Epoch [457/500], Loss: 0.7564\n",
      "total loss: 1.513649821281433\n",
      "Epoch [458/500], Loss: 0.7568\n",
      "total loss: 1.513868510723114\n",
      "Epoch [459/500], Loss: 0.7569\n",
      "total loss: 1.5128180980682373\n",
      "Epoch [460/500], Loss: 0.7564\n",
      "total loss: 1.5135614275932312\n",
      "Epoch [461/500], Loss: 0.7568\n",
      "total loss: 1.5148466229438782\n",
      "Epoch [462/500], Loss: 0.7574\n",
      "total loss: 1.511953592300415\n",
      "Epoch [463/500], Loss: 0.7560\n",
      "total loss: 1.51377671957016\n",
      "Epoch [464/500], Loss: 0.7569\n",
      "total loss: 1.5127553939819336\n",
      "Epoch [465/500], Loss: 0.7564\n",
      "total loss: 1.5127370953559875\n",
      "Epoch [466/500], Loss: 0.7564\n",
      "total loss: 1.5127411484718323\n",
      "Epoch [467/500], Loss: 0.7564\n",
      "total loss: 1.5141147375106812\n",
      "Epoch [468/500], Loss: 0.7571\n",
      "total loss: 1.512569010257721\n",
      "Epoch [469/500], Loss: 0.7563\n",
      "total loss: 1.5131434202194214\n",
      "Epoch [470/500], Loss: 0.7566\n",
      "total loss: 1.5131980776786804\n",
      "Epoch [471/500], Loss: 0.7566\n",
      "total loss: 1.5121907591819763\n",
      "Epoch [472/500], Loss: 0.7561\n",
      "total loss: 1.5132718086242676\n",
      "Epoch [473/500], Loss: 0.7566\n",
      "total loss: 1.5135124325752258\n",
      "Epoch [474/500], Loss: 0.7568\n",
      "total loss: 1.513080656528473\n",
      "Epoch [475/500], Loss: 0.7565\n",
      "total loss: 1.512741208076477\n",
      "Epoch [476/500], Loss: 0.7564\n",
      "total loss: 1.5127593874931335\n",
      "Epoch [477/500], Loss: 0.7564\n",
      "total loss: 1.512105107307434\n",
      "Epoch [478/500], Loss: 0.7561\n",
      "total loss: 1.5126670598983765\n",
      "Epoch [479/500], Loss: 0.7563\n",
      "total loss: 1.5126458406448364\n",
      "Epoch [480/500], Loss: 0.7563\n",
      "total loss: 1.5123416781425476\n",
      "Epoch [481/500], Loss: 0.7562\n",
      "total loss: 1.5119981169700623\n",
      "Epoch [482/500], Loss: 0.7560\n",
      "total loss: 1.5127863883972168\n",
      "Epoch [483/500], Loss: 0.7564\n",
      "total loss: 1.5126193165779114\n",
      "Epoch [484/500], Loss: 0.7563\n",
      "total loss: 1.5123661756515503\n",
      "Epoch [485/500], Loss: 0.7562\n",
      "total loss: 1.5128855109214783\n",
      "Epoch [486/500], Loss: 0.7564\n",
      "total loss: 1.5116766095161438\n",
      "Epoch [487/500], Loss: 0.7558\n",
      "total loss: 1.5129024982452393\n",
      "Epoch [488/500], Loss: 0.7565\n",
      "total loss: 1.5120272040367126\n",
      "Epoch [489/500], Loss: 0.7560\n",
      "total loss: 1.5126414895057678\n",
      "Epoch [490/500], Loss: 0.7563\n",
      "total loss: 1.5121313333511353\n",
      "Epoch [491/500], Loss: 0.7561\n",
      "total loss: 1.5116564631462097\n",
      "Epoch [492/500], Loss: 0.7558\n",
      "total loss: 1.5122246742248535\n",
      "Epoch [493/500], Loss: 0.7561\n",
      "total loss: 1.5125004649162292\n",
      "Epoch [494/500], Loss: 0.7563\n",
      "total loss: 1.511926531791687\n",
      "Epoch [495/500], Loss: 0.7560\n",
      "total loss: 1.5119552612304688\n",
      "Epoch [496/500], Loss: 0.7560\n",
      "total loss: 1.5121585726737976\n",
      "Epoch [497/500], Loss: 0.7561\n",
      "total loss: 1.5124242305755615\n",
      "Epoch [498/500], Loss: 0.7562\n",
      "total loss: 1.5122753977775574\n",
      "Epoch [499/500], Loss: 0.7561\n",
      "total loss: 1.5119855403900146\n",
      "Epoch [500/500], Loss: 0.7560\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model\n",
    "trained_model = train_model(input_data, desired_output, 500, learning_rate=0.0005, viz_accuracy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(trained_model, new_data, expected_result):\n",
    "    test_input = torch.tensor(new_data, dtype=torch.float32)\n",
    "\n",
    "    test_output = trained_model(test_input)\n",
    "\n",
    "    predicted_classes = torch.argmax(test_output, dim=1)\n",
    "\n",
    "    test_output_compare = torch.tensor(expected_result, dtype=torch.int)\n",
    "\n",
    "    return float(sum(test_output_compare == predicted_classes)/len(test_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "just_input_test = test_df.drop('first_move_by_Q_learning', axis=1)\n",
    "just_input_test = just_input_test.drop('num_moves_Q_learning_needs', axis=1)\n",
    "\n",
    "input_data_test = np.array(just_input_test.values.tolist())\n",
    "desired_output = torch.tensor(test_df['first_move_by_Q_learning'].tolist(), dtype=torch.float32).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t9/gxb6q8zj21dff090q066td740000gn/T/ipykernel_16434/875939923.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_output_compare = torch.tensor(expected_result, dtype=torch.int)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(trained_model, input_data, torch.tensor(train['first_move_by_Q_learning'].tolist(), dtype=torch.float32).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t9/gxb6q8zj21dff090q066td740000gn/T/ipykernel_16434/875939923.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_output_compare = torch.tensor(expected_result, dtype=torch.int)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9990950226783752"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(trained_model, input_data_test, desired_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1, 2], [0, 1]])\n",
    "B = np.array([[1, 0], [2, 1]])\n",
    "\n",
    "# elements on the diagonal are 1 mod 4. \n",
    "# elements not on the diagonal are 0 mod 2. \n",
    "\n",
    "# C is the inverse of A\n",
    "# D is the inverse of B\n",
    "C = np.array([[1, -2], [0, 1]])\n",
    "D = np.array([[1, 0], [-2, 1]])\n",
    "\n",
    "\n",
    "identity = np.array([[1, 0], [0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how long does it take us to get back to the origin? \n",
    "def matrix_to_tuple(matrix):\n",
    "    return (matrix[0][0], matrix[0][1], matrix[0][1], matrix[1][1])\n",
    "\n",
    "def num_steps_to_origin_w_nn(test_mat):\n",
    "    for i in range(100):\n",
    "        if (test_mat==identity).all():\n",
    "            return i\n",
    "        demo_test_type = torch.tensor([matrix_to_tuple(test_mat)], dtype=torch.float32)\n",
    "        action = int(torch.argmax(trained_model(demo_test_type)))\n",
    "\n",
    "        if action==0:\n",
    "            test_mat = test_mat @ A\n",
    "        elif action==1:\n",
    "            test_mat = test_mat @ B\n",
    "        elif action==2:\n",
    "            test_mat = test_mat @ C\n",
    "        else:\n",
    "            test_mat = test_mat @ D\n",
    "\n",
    "    return 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(trained_model.state_dict(), 'weights/subset_sl2z_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_wrapper_for_num_steps(row):\n",
    "    arr = np.array([\n",
    "        [int(row['val1']), int(row['val2'])],\n",
    "        [int(row['val3']), int(row['val4'])]\n",
    "        ])\n",
    "    return num_steps_to_origin_w_nn(arr)\n",
    "\n",
    "test_df['num_steps_to_origin'] = test_df.apply(row_wrapper_for_num_steps, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "print(sum(test_df['num_steps_to_origin'] != 1000))\n",
    "\n",
    "just_a_few = test_df[test_df['num_steps_to_origin'] != 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_action(cur_matrix, action):\n",
    "    assert(action <= 3 or action >= 0)\n",
    "    if action==0:\n",
    "        return cur_matrix @ A\n",
    "    elif action==1:\n",
    "        return cur_matrix @ B\n",
    "    elif action==2:\n",
    "        return cur_matrix @ C\n",
    "    else:\n",
    "        return cur_matrix @ D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(mat):\n",
    "    demo_test_type = torch.tensor([matrix_to_tuple(mat)], dtype=torch.float32)\n",
    "    return int(torch.argmax(trained_model(demo_test_type)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_group_test = pd.read_csv(\"../Data_Generation/Data_files/test_rows_SL2Z_Q_learn.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "just_input = whole_group_test.drop('first_move_by_Q_learning', axis=1).drop('num_moves_Q_learning_needs', axis=1).to_numpy()\n",
    "# input_data = np.array(just_input.values.tolist())\n",
    "desired_output = torch.tensor(whole_group_test['first_move_by_Q_learning'].tolist(), dtype=torch.float32).long()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t9/gxb6q8zj21dff090q066td740000gn/T/ipykernel_16434/875939923.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_output_compare = torch.tensor(expected_result, dtype=torch.int)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test subgroup model on entire SL2Z dataset\n",
    "test_model(trained_model, just_input, desired_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entire_gen = [np.array([[1, 2], [0, 1]]), np.array([[1, 0], [2, 1]]), np.linalg.inv(A), np.linalg.inv(B)]\n",
    "Q_table = None # todo fix :)\n",
    "def first_matrix_to_apply(cur_row):\n",
    "    outputs = [0, 0, 0, 0]\n",
    "    cur_matrix = np.array([\n",
    "        [int(cur_row['val1']), int(cur_row['val2'])], \n",
    "        [int(cur_row['val3']), int(cur_row['val4'])]\n",
    "        ])\n",
    "    for i in range(len(entire_gen)):\n",
    "        outputs[i] = Q_table[matrix_to_tuple(cur_matrix  @ entire_gen[i])]\n",
    "    return np.argmax(outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
