{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-15 08:21:24.106612: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "#\n",
    "model = Sequential()\n",
    "model.add(Dense(30, input_dim=784, activation = 'sigmoid'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='mse',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_T = torch.tensor([[0, 2], [1, 0]], dtype=torch.float64)\n",
    "matrix_U = torch.tensor([[1, -1], [0, 1]], dtype=torch.float64)\n",
    "\n",
    "inverse_T = torch.inverse(matrix_T)\n",
    "inverse_U = torch.inverse(matrix_U)\n",
    "\n",
    "max_moves = 30  #how many inverse operations on the final destination\n",
    "goal_position = torch.tensor([[1], [0]], dtype=torch.float64)\n",
    "\n",
    "def data_generator(num_of_datapoints):\n",
    "    data = torch.empty((num_of_datapoints, 3), dtype=torch.float64)\n",
    "    for i in range(num_of_datapoints):\n",
    "        start = goal_position\n",
    "        moves = random.randint(1, max_moves)\n",
    "        coin = 3\n",
    "        for q in range(moves):\n",
    "            coin = random.randint(0, 1)\n",
    "            if coin == 1:\n",
    "                #  print(start, inverse_U, \"=\")\n",
    "                start = torch.matmul(inverse_U, start)\n",
    "            #  print(start)\n",
    "            else:\n",
    "                #  print(start, inverse_T, \"=\")\n",
    "                start = torch.matmul(inverse_T, start)\n",
    "                if q != moves - 1:\n",
    "                    start = torch.matmul(inverse_U, start)\n",
    "            # print(start)\n",
    "        # print(\"done\")\n",
    "        data[i, 0] = start[0]\n",
    "        data[i, 1] = start[1]\n",
    "        data[i, 2] = coin\n",
    "    return data\n",
    "\n",
    "#https://stackoverflow.com/questions/36158058/torch-save-tensor-to-csv-file#:~:text=For%20simple%20tables%2C%20you%20can,then%20to%20a%20Pandas%20dataframe.&text=You%20can%20first%20convert%20the,table%20as%20a%20csv%20file.\n",
    "data = data_generator(20000)\n",
    "data_table = data.numpy()\n",
    "\n",
    "x_train = data_table[:, :-1]\n",
    "y_train = data_table[:, -1]\n",
    "\n",
    "data = data_generator(20000)\n",
    "data_table = data.numpy()\n",
    "\n",
    "x_test = data_table[:, :-1]\n",
    "y_test = data_table[:, -1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "#\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=2, activation = 'relu'))\n",
    "model.add(Dense(64,  activation = 'relu'))\n",
    "model.add(Dense(16, activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "200/200 [==============================] - 1s 2ms/step - loss: 0.0630 - accuracy: 0.9624\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.0064 - accuracy: 0.9981\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.0027 - accuracy: 0.9998\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 9.8577e-04 - accuracy: 0.9999\n",
      "Epoch 5/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 4.5680e-04 - accuracy: 0.9999\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 2.1780e-04 - accuracy: 1.0000\n",
      "Epoch 7/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.4562e-04 - accuracy: 1.0000\n",
      "Epoch 8/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.0681e-04 - accuracy: 1.0000\n",
      "Epoch 9/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 7.7964e-05 - accuracy: 1.0000\n",
      "Epoch 10/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 6.0742e-05 - accuracy: 1.0000\n",
      "Epoch 11/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 5.5921e-05 - accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 4.5329e-05 - accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 4.6822e-05 - accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 3.3980e-05 - accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 3.5787e-05 - accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 2.4427e-05 - accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.7473e-05 - accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.6145e-05 - accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.5878e-05 - accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.6276e-05 - accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.0062e-05 - accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.3951e-05 - accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.8600e-05 - accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.0622e-05 - accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.5079e-05 - accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 7.3557e-06 - accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 4.9043e-06 - accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 3.7201e-06 - accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 5.2212e-06 - accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 3.8736e-06 - accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 2.6465e-06 - accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 2.5714e-06 - accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 2.9452e-06 - accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.8536e-06 - accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.3488e-06 - accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.0552e-06 - accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 9.6157e-07 - accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 5.9619e-04 - accuracy: 0.9994\n",
      "Epoch 39/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.3104e-06 - accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 1.1717e-06 - accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.0704e-06 - accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 9.7486e-07 - accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 9.3172e-07 - accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 8.2375e-07 - accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 8.2769e-07 - accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 8.1098e-07 - accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 7.6544e-07 - accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 7.0894e-07 - accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 7.2882e-07 - accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 6.0342e-07 - accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 5.4147e-07 - accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 5.2685e-07 - accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 5.3821e-07 - accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 4.8136e-07 - accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 4.3865e-07 - accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 4.5249e-07 - accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 3.9804e-07 - accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 3.8099e-07 - accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 3.4801e-07 - accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 3.5356e-07 - accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 2.7285e-07 - accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 2.7372e-07 - accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 2.9418e-07 - accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 2.5214e-07 - accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 2.3311e-07 - accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 2.0803e-07 - accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.7440e-07 - accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 2.0990e-07 - accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.8330e-07 - accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.4178e-07 - accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.2627e-07 - accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.2976e-07 - accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.2424e-07 - accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 7.9539e-08 - accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.2175e-07 - accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 9.5544e-08 - accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 8.0691e-08 - accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 6.8941e-08 - accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 6.5077e-08 - accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 6.2991e-08 - accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 3.0317e-08 - accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 4.3793e-08 - accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 4.4639e-08 - accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 2.9807e-08 - accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 2.6632e-08 - accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 3.8543e-08 - accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 3.1609e-08 - accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 2.2300e-08 - accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 2.4073e-08 - accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 1.5917e-08 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 1.7777e-08 - accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 1.6305e-08 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 1.3974e-08 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.0188e-08 - accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 9.9413e-09 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 9.4204e-09 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.1171e-08 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 7.5289e-09 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 8.2421e-09 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 7.2153e-09 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x141f93190>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=100, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8392e-10 - accuracy: 1.0000\n",
      "1.8391899114789112e-10\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, batch_size=100)\n",
    "print(score[0])\n",
    "print(score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
