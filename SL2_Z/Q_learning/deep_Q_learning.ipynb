{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '../Data_Generation/Data_files/'\n",
    "base_fp = base_dir + 'subset_sl2_Z.csv'\n",
    "test_fp = base_dir + 'subset_test_rows_SL2Z_Q_learn.csv'\n",
    "train_fp = base_dir + 'subset_train_rows_SL2Z_Q_learn.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Architecture\n",
    "$4\\times1\\to$ hidden layers $\\to 4\\times 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DeepQModel, self).__init__()\n",
    "        self.stack = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, output_size),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQAgent:\n",
    "    def __init__(self, learning_rate: float, gamma: float, epsilon: float, discount_factor: float,\n",
    "                 batch_size: int, memory_capacity: int):\n",
    "        # Initialize main an target models and set weights to be equivalent\n",
    "        self.mainModel = DeepQModel(4, 4)\n",
    "        self.targetModel = DeepQModel(4, 4)\n",
    "        \n",
    "        # TODO: check if this works\n",
    "        self.copy_main_to_target()\n",
    "\n",
    "        # Initialize Hyperparameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.discount_factor = discount_factor\n",
    "        self.batch_size = batch_size\n",
    "        self.memory_capacity = memory_capacity\n",
    "\n",
    "        # Experience Replay Buffers\n",
    "        self.memory = deque()\n",
    "       \n",
    "    def copy_main_to_target(self):\n",
    "        self.targetModel.load_state_dict(self.mainModel.state_dict())\n",
    "\n",
    "    def replay(self):\n",
    "        # Ensure there is enough memory for a full batch\n",
    "        if len(self.memory) <  self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # Random sample a batch_size's worth of memory\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "\n",
    "        # Vectorizing data\n",
    "        states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "        states = torch.tensor(states, dtype=torch.float32)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.int64)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32)\n",
    "        \n",
    "        # Bellman Equation: (Reward + discount_factor * state_prediction)\n",
    "        # len(states) x 1\n",
    "        target_predictions = self.targetModel.forward(states).max(dim=1)\n",
    "        bellmans = rewards + self.discount_factor * target_predictions\n",
    "        next_q = self.targetModel.forward(next_states).max(dim=1)\n",
    "\n",
    "        F.mse_loss(next_q, bellmans)\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done: bool):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if len(self.memory) > self.memory_capacity:\n",
    "            self.memory.popleft()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQAgentTrainer: \n",
    "    def __init__(self, agent: DeepQAgent, batch_size) -> None:\n",
    "        self.batch_size = batch_size\n",
    "        self.agent = agent\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
