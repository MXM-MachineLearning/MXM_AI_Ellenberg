{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from collections import deque\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from util import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Definition:\n",
    "Let $A = \\begin{bmatrix}1 & 2 \\\\ 0 & 1\\end{bmatrix}$, $B = \\begin{bmatrix}1 & 0 \\\\ 2 & 1\\end{bmatrix}$.\n",
    "The group $\\Gamma \\coloneqq \\langle A,B\\rangle \\subseteq SL_2(\\mathbb{Z})$ is an index $12$ subgroup. The diagonal entries are congruent to $1\\pmod{4}$ and the non-diagonal entries and divisible by $2$. \n",
    "\n",
    "We define $C = A^{-1}$ and $D = B^{-1}$.\n",
    "\n",
    "We can generate with any coset by starting at a representative from each coset and see if we get our way back to it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_REWARD = 100\n",
    "STEP_PENALTY = -1\n",
    "\n",
    "\n",
    "def getReward(matrix: torch.tensor) -> int:\n",
    "    if (matrix == torch.eye(2)).all():\n",
    "        return MAX_REWARD\n",
    "    else:\n",
    "        return STEP_PENALTY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data from ../Data_Generation/Data_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "generators = k_sl2z_2s_gen\n",
    "subset = '_2s'\n",
    "\n",
    "base_dir = '../Data_Generation/Data_files/'\n",
    "base_fp = base_dir + 'points/sl2_Z' + subset + '.csv'\n",
    "test_fp = base_dir + 'labeled_points/sl2_Z' + subset + '_test.csv'\n",
    "train_fp = base_dir + 'labeled_points/sl2_Z' + subset + '_train.csv'\n",
    "\n",
    "df = pd.read_csv(base_fp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the diagonal entries are congruent to $1\\pmod{4}$ and the non-diagonal entries and divisible by $2$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val1</th>\n",
       "      <th>val2</th>\n",
       "      <th>val3</th>\n",
       "      <th>val4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5.0</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9949</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9971</th>\n",
       "      <td>-27.0</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9974</th>\n",
       "      <td>-7.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9988</th>\n",
       "      <td>-7.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9989</th>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>847 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      val1  val2  val3  val4\n",
       "9      1.0   0.0  -2.0   1.0\n",
       "22     5.0  -2.0   8.0  -3.0\n",
       "28     5.0 -12.0  -2.0   5.0\n",
       "34     1.0   2.0   2.0   5.0\n",
       "37     1.0  -2.0   2.0  -3.0\n",
       "...    ...   ...   ...   ...\n",
       "9949   1.0  -8.0   0.0   1.0\n",
       "9971 -27.0 -20.0  -4.0  -3.0\n",
       "9974  -7.0  30.0  -4.0  17.0\n",
       "9988  -7.0  24.0   2.0  -7.0\n",
       "9989   9.0   4.0   2.0   1.0\n",
       "\n",
       "[847 rows x 4 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_df = df[df['val1'] % 4 == 1]\n",
    "filter_df = filter_df[filter_df['val2'] % 2 == 0]\n",
    "filter_df = filter_df[filter_df['val3'] % 2 == 0]\n",
    "filter_df = filter_df[filter_df['val4'] % 4 == 1]\n",
    "filter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nc/4qjwq93d3_58jrsktchrftsr0000gn/T/ipykernel_54776/3450575276.py:21: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
      "  [int(cur_row['val1']), int(cur_row['val2'])],\n",
      "/var/folders/nc/4qjwq93d3_58jrsktchrftsr0000gn/T/ipykernel_54776/3450575276.py:22: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
      "  [int(cur_row['val3']), int(cur_row['val4'])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 500\teps: 0.951\tAvg rwd: -20.000\n",
      "Episode 1000\teps: 0.905\tAvg rwd: -19.274\n",
      "Episode 1500\teps: 0.861\tAvg rwd: -20.000\n",
      "Episode 2000\teps: 0.819\tAvg rwd: -18.872\n",
      "Episode 2500\teps: 0.779\tAvg rwd: -18.448\n",
      "Episode 3000\teps: 0.741\tAvg rwd: -18.872\n",
      "Episode 3500\teps: 0.705\tAvg rwd: -18.544\n",
      "Episode 4000\teps: 0.670\tAvg rwd: -19.636\n",
      "Episode 4500\teps: 0.638\tAvg rwd: -17.052\n",
      "Episode 5000\teps: 0.607\tAvg rwd: -18.294\n",
      "Episode 5500\teps: 0.577\tAvg rwd: -16.148\n",
      "Episode 6000\teps: 0.549\tAvg rwd: -17.178\n",
      "Episode 6500\teps: 0.522\tAvg rwd: -18.168\n",
      "Episode 7000\teps: 0.497\tAvg rwd: -15.420\n",
      "Episode 7500\teps: 0.472\tAvg rwd: -18.410\n",
      "Episode 8000\teps: 0.449\tAvg rwd: -18.188\n",
      "Episode 8500\teps: 0.427\tAvg rwd: -16.526\n",
      "Episode 9000\teps: 0.407\tAvg rwd: -17.724\n",
      "Episode 9500\teps: 0.387\tAvg rwd: -16.908\n",
      "Episode 10000\teps: 0.368\tAvg rwd: -14.976\n",
      "Episode 10500\teps: 0.350\tAvg rwd: -17.316\n",
      "Episode 11000\teps: 0.333\tAvg rwd: -17.718\n",
      "Episode 11500\teps: 0.317\tAvg rwd: -17.302\n",
      "Episode 12000\teps: 0.301\tAvg rwd: -16.528\n",
      "Episode 12500\teps: 0.286\tAvg rwd: -16.558\n",
      "Episode 13000\teps: 0.273\tAvg rwd: -16.054\n",
      "Episode 13500\teps: 0.259\tAvg rwd: -16.950\n",
      "Episode 14000\teps: 0.247\tAvg rwd: -17.656\n",
      "Episode 14500\teps: 0.235\tAvg rwd: -17.266\n",
      "Episode 15000\teps: 0.223\tAvg rwd: -16.482\n",
      "Episode 15500\teps: 0.212\tAvg rwd: -18.058\n",
      "Episode 16000\teps: 0.202\tAvg rwd: -17.252\n",
      "Episode 16500\teps: 0.192\tAvg rwd: -16.460\n",
      "Episode 17000\teps: 0.183\tAvg rwd: -15.696\n",
      "Episode 17500\teps: 0.174\tAvg rwd: -18.052\n",
      "Episode 18000\teps: 0.165\tAvg rwd: -16.086\n",
      "Episode 18500\teps: 0.157\tAvg rwd: -16.888\n",
      "Episode 19000\teps: 0.150\tAvg rwd: -18.428\n",
      "Episode 19500\teps: 0.142\tAvg rwd: -16.084\n",
      "Episode 20000\teps: 0.135\tAvg rwd: -16.486\n",
      "Episode 20500\teps: 0.129\tAvg rwd: -17.626\n",
      "Episode 21000\teps: 0.122\tAvg rwd: -16.468\n",
      "Episode 21500\teps: 0.116\tAvg rwd: -17.658\n",
      "Episode 22000\teps: 0.111\tAvg rwd: -18.046\n",
      "Episode 22500\teps: 0.105\tAvg rwd: -17.276\n",
      "Episode 23000\teps: 0.100\tAvg rwd: -16.846\n",
      "Episode 23500\teps: 0.095\tAvg rwd: -17.628\n",
      "Episode 24000\teps: 0.091\tAvg rwd: -15.278\n",
      "Episode 24500\teps: 0.086\tAvg rwd: -18.044\n",
      "Episode 25000\teps: 0.082\tAvg rwd: -17.640\n",
      "Episode 25500\teps: 0.078\tAvg rwd: -17.248\n",
      "Episode 26000\teps: 0.074\tAvg rwd: -16.850\n",
      "Episode 26500\teps: 0.071\tAvg rwd: -17.642\n",
      "Episode 27000\teps: 0.067\tAvg rwd: -18.422\n",
      "Episode 27500\teps: 0.064\tAvg rwd: -16.850\n",
      "Episode 28000\teps: 0.061\tAvg rwd: -16.468\n",
      "Episode 28500\teps: 0.058\tAvg rwd: -16.076\n",
      "Episode 29000\teps: 0.055\tAvg rwd: -18.416\n",
      "Episode 29500\teps: 0.052\tAvg rwd: -14.880\n",
      "Episode 30000\teps: 0.050\tAvg rwd: -17.246\n"
     ]
    }
   ],
   "source": [
    "env = TabularQEnv(generators, defaultdict(lambda: 0), getReward, MAX_REWARD)\n",
    "\n",
    "EPISODES = 30000\n",
    "LEARNING_RATE = .9\n",
    "DISCOUNT_FACTOR = .99\n",
    "EPSILON = 1\n",
    "EPSILON_DECAY = .9999\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# adapted from ChatGPT\n",
    "episode_reward_record = deque(maxlen=100)\n",
    "\n",
    "for i in range(EPISODES):\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    # choose a random starting row\n",
    "    # adapted from https://stackoverflow.com/questions/15923826/random-row-selection-in-pandas-dataframe\n",
    "    cur_row = df.sample(1)\n",
    "    obs = torch.tensor([\n",
    "        [int(cur_row['val1']), int(cur_row['val2'])], \n",
    "        [int(cur_row['val3']), int(cur_row['val4'])]\n",
    "        ], dtype=torch.long)\n",
    "\n",
    "    index = 1\n",
    "\n",
    "    while (not done):\n",
    "        # perform an epsilon greedy action \n",
    "        # Q(s, a) = (1-LEARNING_RATE)Q(s, a) + (LEARNING_RATE)(r + DISCOUNT_FACTOR(max a'(Q(s', a'))))\n",
    "        obs, reward, done = env.step(LEARNING_RATE, DISCOUNT_FACTOR, EPSILON, obs)\n",
    "\n",
    "        episode_reward += reward # update episode reward\n",
    "\n",
    "        index += 1\n",
    "        # if we take more than 100 steps, end this iteration early (we are probably not making progress)\n",
    "        if index > 100:\n",
    "            done=True\n",
    "\n",
    "    # decay the epsilon\n",
    "    EPSILON *= EPSILON_DECAY\n",
    "\n",
    "    # record the reward for this episode\n",
    "    episode_reward_record.append(episode_reward) \n",
    "\n",
    "    if (i+1)%500 ==0 and i>0:\n",
    "        print(\"Episode {i}\\teps: {eps:.3f}\\tAvg rwd: {rwd:.3f}\".format(i=i+1, eps=EPSILON, rwd=sum(list(episode_reward_record))/500))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def access_Q_table(mat):\n",
    "    return env.Q_table[matrix_to_tuple(mat)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with the other dataframe. \n",
    "test_df = pd.read_csv(base_fp)\n",
    "test_df['num_moves_Q_learning_needs'] = test_df.apply(lambda row: env.play(df_row_to_mat(row)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The proportion of starting positions in the test dataset that we can find a route to the origin that's <50 steps: \n",
      "1.0\n",
      "Of these, the proportion of times where we learned a path that was < 20 moves: \n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"The proportion of starting positions in the test dataset that we can find a route to the origin that's <50 steps: \")\n",
    "print(sum(test_df['num_moves_Q_learning_needs']<=50)/test_df.shape[0])\n",
    "\n",
    "print(\"Of these, the proportion of times where we learned a path that was < 20 moves: \")\n",
    "# encouraging because all of these were generated as sequences of 30 moves\n",
    "# so we've found significantly faster paths back to the origin for almost all moves that we find a path to the origin \n",
    "print(sum(test_df['num_moves_Q_learning_needs']<20)/sum(test_df['num_moves_Q_learning_needs']<=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_info_states_csv(fname_i, of_train, of_test, Q_env, prop_train=0.7):\n",
    "    \"\"\"\n",
    "    Given CSV with various states and tabular-Q environment trained on a set containing those states, \n",
    "    estimate next best move + number of moves to identity, and append them to the state information.\n",
    "    Then, split that dataset into train/test and write to corresponding CSVs\n",
    "    Args:\n",
    "        fname_i: csv to append to\n",
    "        of_train: where to write final train csv\n",
    "        of_test: where to write final test csv\n",
    "        Q_env: TabularQEnv used to make predictons\n",
    "        prop_train: proportion of data to be used for training\n",
    "    \"\"\"\n",
    "    test_df = pd.read_csv(fname_i)\n",
    "    test_df['num_moves_Q_learning_needs'] = test_df.apply(\n",
    "        lambda row: Q_env.play(df_row_to_mat(row)), axis=1)\n",
    "    filtered_df = test_df[test_df['num_moves_Q_learning_needs'] != 100]\n",
    "    filtered_df['first_move_by_Q_learning'] = filtered_df.apply(\n",
    "        lambda row: Q_env.best_move(df_row_to_mat(row)), axis=1)\n",
    "\n",
    "    print(filtered_df.shape)\n",
    "\n",
    "    bound = int(filtered_df.shape[0] * prop_train)\n",
    "    train = filtered_df.iloc[1:bound]\n",
    "    test = filtered_df.iloc[bound:filtered_df.shape[0]]\n",
    "\n",
    "    train.to_csv(of_train, index=False)\n",
    "    test.to_csv(of_test, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_info_states_csv(base_fp,\n",
    "                       train_fp,\n",
    "                       test_fp,\n",
    "                       env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
