{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from collections import deque\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from util import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Definition:\n",
    "Let $A = \\begin{bmatrix}1 & 2 \\\\ 0 & 1\\end{bmatrix}$, $B = \\begin{bmatrix}1 & 0 \\\\ 2 & 1\\end{bmatrix}$.\n",
    "The group $\\Gamma \\coloneqq \\langle A,B\\rangle \\subseteq SL_2(\\mathbb{Z})$ is an index $12$ subgroup. The diagonal entries are congruent to $1\\pmod{4}$ and the non-diagonal entries and divisible by $2$. \n",
    "\n",
    "We define $C = A^{-1}$ and $D = B^{-1}$.\n",
    "\n",
    "We can generate with any coset by starting at a representative from each coset and see if we get our way back to it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_REWARD = 100\n",
    "STEP_PENALTY = -1\n",
    "\n",
    "\n",
    "def getReward(matrix: torch.tensor) -> int:\n",
    "    if (matrix == torch.eye(2)).all():\n",
    "        return MAX_REWARD\n",
    "    else:\n",
    "        return STEP_PENALTY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data from ../Data_Generation/Data_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "generators = k_sl2z_2s_gen\n",
    "subset = '_2s'\n",
    "\n",
    "base_dir = '../Data_Generation/Data_files/'\n",
    "base_fp = base_dir + 'points/sl2_Z' + subset + '.csv'\n",
    "test_fp = base_dir + 'labeled_points/sl2_Z' + subset + '_test.csv'\n",
    "train_fp = base_dir + 'labeled_points/sl2_Z' + subset + '_train.csv'\n",
    "\n",
    "df = pd.read_csv(base_fp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the diagonal entries are congruent to $1\\pmod{4}$ and the non-diagonal entries and divisible by $2$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val1</th>\n",
       "      <th>val2</th>\n",
       "      <th>val3</th>\n",
       "      <th>val4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-227.0</td>\n",
       "      <td>-520.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>197.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1657.0</td>\n",
       "      <td>298.0</td>\n",
       "      <td>-506.0</td>\n",
       "      <td>-91.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-43.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>65.0</td>\n",
       "      <td>-112.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>-255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>-1171.0</td>\n",
       "      <td>492.0</td>\n",
       "      <td>-2868.0</td>\n",
       "      <td>1205.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>9.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>-19.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>25.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>601.0</td>\n",
       "      <td>-282.0</td>\n",
       "      <td>-130.0</td>\n",
       "      <td>61.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         val1   val2    val3    val4\n",
       "0      -227.0 -520.0    86.0   197.0\n",
       "1        29.0   12.0    70.0    29.0\n",
       "2      1657.0  298.0  -506.0   -91.0\n",
       "3       -43.0   16.0     8.0    -3.0\n",
       "4        65.0 -112.0   148.0  -255.0\n",
       "...       ...    ...     ...     ...\n",
       "99995 -1171.0  492.0 -2868.0  1205.0\n",
       "99996     9.0   -2.0    -4.0     1.0\n",
       "99997   -19.0   50.0    -8.0    21.0\n",
       "99998    25.0   66.0    14.0    37.0\n",
       "99999   601.0 -282.0  -130.0    61.0\n",
       "\n",
       "[100000 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_df = df[df['val1'] % 4 == 1]\n",
    "filter_df = filter_df[filter_df['val2'] % 2 == 0]\n",
    "filter_df = filter_df[filter_df['val3'] % 2 == 0]\n",
    "filter_df = filter_df[filter_df['val4'] % 4 == 1]\n",
    "filter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nc/4qjwq93d3_58jrsktchrftsr0000gn/T/ipykernel_54776/3450575276.py:21: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
      "  [int(cur_row['val1']), int(cur_row['val2'])],\n",
      "/var/folders/nc/4qjwq93d3_58jrsktchrftsr0000gn/T/ipykernel_54776/3450575276.py:22: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
      "  [int(cur_row['val3']), int(cur_row['val4'])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 500\teps: 0.951\tAvg rwd: -20.000\n",
      "Episode 1000\teps: 0.905\tAvg rwd: -19.274\n",
      "Episode 1500\teps: 0.861\tAvg rwd: -20.000\n",
      "Episode 2000\teps: 0.819\tAvg rwd: -18.872\n",
      "Episode 2500\teps: 0.779\tAvg rwd: -18.448\n",
      "Episode 3000\teps: 0.741\tAvg rwd: -18.872\n",
      "Episode 3500\teps: 0.705\tAvg rwd: -18.544\n",
      "Episode 4000\teps: 0.670\tAvg rwd: -19.636\n",
      "Episode 4500\teps: 0.638\tAvg rwd: -17.052\n",
      "Episode 5000\teps: 0.607\tAvg rwd: -18.294\n",
      "Episode 5500\teps: 0.577\tAvg rwd: -16.148\n",
      "Episode 6000\teps: 0.549\tAvg rwd: -17.178\n",
      "Episode 6500\teps: 0.522\tAvg rwd: -18.168\n",
      "Episode 7000\teps: 0.497\tAvg rwd: -15.420\n",
      "Episode 7500\teps: 0.472\tAvg rwd: -18.410\n",
      "Episode 8000\teps: 0.449\tAvg rwd: -18.188\n",
      "Episode 8500\teps: 0.427\tAvg rwd: -16.526\n",
      "Episode 9000\teps: 0.407\tAvg rwd: -17.724\n",
      "Episode 9500\teps: 0.387\tAvg rwd: -16.908\n",
      "Episode 10000\teps: 0.368\tAvg rwd: -14.976\n",
      "Episode 10500\teps: 0.350\tAvg rwd: -17.316\n",
      "Episode 11000\teps: 0.333\tAvg rwd: -17.718\n",
      "Episode 11500\teps: 0.317\tAvg rwd: -17.302\n",
      "Episode 12000\teps: 0.301\tAvg rwd: -16.528\n",
      "Episode 12500\teps: 0.286\tAvg rwd: -16.558\n",
      "Episode 13000\teps: 0.273\tAvg rwd: -16.054\n",
      "Episode 13500\teps: 0.259\tAvg rwd: -16.950\n",
      "Episode 14000\teps: 0.247\tAvg rwd: -17.656\n",
      "Episode 14500\teps: 0.235\tAvg rwd: -17.266\n",
      "Episode 15000\teps: 0.223\tAvg rwd: -16.482\n",
      "Episode 15500\teps: 0.212\tAvg rwd: -18.058\n",
      "Episode 16000\teps: 0.202\tAvg rwd: -17.252\n",
      "Episode 16500\teps: 0.192\tAvg rwd: -16.460\n",
      "Episode 17000\teps: 0.183\tAvg rwd: -15.696\n",
      "Episode 17500\teps: 0.174\tAvg rwd: -18.052\n",
      "Episode 18000\teps: 0.165\tAvg rwd: -16.086\n",
      "Episode 18500\teps: 0.157\tAvg rwd: -16.888\n",
      "Episode 19000\teps: 0.150\tAvg rwd: -18.428\n",
      "Episode 19500\teps: 0.142\tAvg rwd: -16.084\n",
      "Episode 20000\teps: 0.135\tAvg rwd: -16.486\n",
      "Episode 20500\teps: 0.129\tAvg rwd: -17.626\n",
      "Episode 21000\teps: 0.122\tAvg rwd: -16.468\n",
      "Episode 21500\teps: 0.116\tAvg rwd: -17.658\n",
      "Episode 22000\teps: 0.111\tAvg rwd: -18.046\n",
      "Episode 22500\teps: 0.105\tAvg rwd: -17.276\n",
      "Episode 23000\teps: 0.100\tAvg rwd: -16.846\n",
      "Episode 23500\teps: 0.095\tAvg rwd: -17.628\n",
      "Episode 24000\teps: 0.091\tAvg rwd: -15.278\n",
      "Episode 24500\teps: 0.086\tAvg rwd: -18.044\n",
      "Episode 25000\teps: 0.082\tAvg rwd: -17.640\n",
      "Episode 25500\teps: 0.078\tAvg rwd: -17.248\n",
      "Episode 26000\teps: 0.074\tAvg rwd: -16.850\n",
      "Episode 26500\teps: 0.071\tAvg rwd: -17.642\n",
      "Episode 27000\teps: 0.067\tAvg rwd: -18.422\n",
      "Episode 27500\teps: 0.064\tAvg rwd: -16.850\n",
      "Episode 28000\teps: 0.061\tAvg rwd: -16.468\n",
      "Episode 28500\teps: 0.058\tAvg rwd: -16.076\n",
      "Episode 29000\teps: 0.055\tAvg rwd: -18.416\n",
      "Episode 29500\teps: 0.052\tAvg rwd: -14.880\n",
      "Episode 30000\teps: 0.050\tAvg rwd: -17.246\n"
     ]
    }
   ],
   "source": [
    "env = TabularQEnv(generators, defaultdict(lambda: 0), getReward, MAX_REWARD)\n",
    "\n",
    "EPISODES = 30000\n",
    "LEARNING_RATE = .9\n",
    "DISCOUNT_FACTOR = .99\n",
    "EPSILON = 1\n",
    "EPSILON_DECAY = .9999\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# adapted from ChatGPT\n",
    "episode_reward_record = deque(maxlen=100)\n",
    "\n",
    "for i in range(EPISODES):\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    # choose a random starting row\n",
    "    # adapted from https://stackoverflow.com/questions/15923826/random-row-selection-in-pandas-dataframe\n",
    "    cur_row = df.sample(1)\n",
    "    obs = torch.tensor([\n",
    "        [int(cur_row['val1']), int(cur_row['val2'])], \n",
    "        [int(cur_row['val3']), int(cur_row['val4'])]\n",
    "        ], dtype=torch.long)\n",
    "\n",
    "    index = 1\n",
    "\n",
    "    while (not done):\n",
    "        # perform an epsilon greedy action \n",
    "        # Q(s, a) = (1-LEARNING_RATE)Q(s, a) + (LEARNING_RATE)(r + DISCOUNT_FACTOR(max a'(Q(s', a'))))\n",
    "        obs, reward, done = env.step(LEARNING_RATE, DISCOUNT_FACTOR, EPSILON, obs)\n",
    "\n",
    "        episode_reward += reward # update episode reward\n",
    "\n",
    "        index += 1\n",
    "        # if we take more than 100 steps, end this iteration early (we are probably not making progress)\n",
    "        if index > 100:\n",
    "            done=True\n",
    "\n",
    "    # decay the epsilon\n",
    "    EPSILON *= EPSILON_DECAY\n",
    "\n",
    "    # record the reward for this episode\n",
    "    episode_reward_record.append(episode_reward) \n",
    "\n",
    "    if (i+1)%500 ==0 and i>0:\n",
    "        print(\"Episode {i}\\teps: {eps:.3f}\\tAvg rwd: {rwd:.3f}\".format(i=i+1, eps=EPSILON, rwd=sum(list(episode_reward_record))/500))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def access_Q_table(mat):\n",
    "    return env.Q_table[matrix_to_tuple(mat)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with the other dataframe. \n",
    "test_df = pd.read_csv(base_fp)\n",
    "test_df['num_moves_Q_learning_needs'] = test_df.apply(lambda row: env.play(df_row_to_mat(row)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The proportion of starting positions in the test dataset that we can find a route to the origin that's <50 steps: \n",
      "1.0\n",
      "Of these, the proportion of times where we learned a path that was < 20 moves: \n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"The proportion of starting positions in the test dataset that we can find a route to the origin that's <50 steps: \")\n",
    "print(sum(test_df['num_moves_Q_learning_needs']<=50)/test_df.shape[0])\n",
    "\n",
    "print(\"Of these, the proportion of times where we learned a path that was < 20 moves: \")\n",
    "# encouraging because all of these were generated as sequences of 30 moves\n",
    "# so we've found significantly faster paths back to the origin for almost all moves that we find a path to the origin \n",
    "print(sum(test_df['num_moves_Q_learning_needs']<20)/sum(test_df['num_moves_Q_learning_needs']<=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_info_states_csv(fname_i, of_train, of_test, Q_env, prop_train=0.7):\n",
    "    \"\"\"\n",
    "    Given CSV with various states and tabular-Q environment trained on a set containing those states, \n",
    "    estimate next best move + number of moves to identity, and append them to the state information.\n",
    "    Then, split that dataset into train/test and write to corresponding CSVs\n",
    "    Args:\n",
    "        fname_i: csv to append to\n",
    "        of_train: where to write final train csv\n",
    "        of_test: where to write final test csv\n",
    "        Q_env: TabularQEnv used to make predictons\n",
    "        prop_train: proportion of data to be used for training\n",
    "    \"\"\"\n",
    "    test_df = pd.read_csv(fname_i)\n",
    "    test_df['num_moves_Q_learning_needs'] = test_df.apply(\n",
    "        lambda row: Q_env.play(df_row_to_mat(row)), axis=1)\n",
    "    filtered_df = test_df[test_df['num_moves_Q_learning_needs'] != 100]\n",
    "    filtered_df['first_move_by_Q_learning'] = filtered_df.apply(\n",
    "        lambda row: Q_env.best_move(df_row_to_mat(row)), axis=1)\n",
    "\n",
    "    print(filtered_df.shape)\n",
    "\n",
    "    bound = int(filtered_df.shape[0] * prop_train)\n",
    "    train = filtered_df.iloc[1:bound]\n",
    "    test = filtered_df.iloc[bound:filtered_df.shape[0]]\n",
    "\n",
    "    train.to_csv(of_train, index=False)\n",
    "    test.to_csv(of_test, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_info_states_csv(base_fp,\n",
    "                       train_fp,\n",
    "                       test_fp,\n",
    "                       env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train nn directly off q-table\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SL2Z_NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SL2Z_NN, self).__init__()\n",
    "\n",
    "        self.stack = nn.Sequential(\n",
    "            nn.Linear(4, 128, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 16, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 4, bias=True),\n",
    "        )\n",
    "\n",
    "        self.activation = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stack(x)\n",
    "        return self.activation(x)\n",
    "\n",
    "def train_model(model, criterion, dataloader, num_epochs=100, learning_rate=0.01, viz_accuracy=False):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.995)\n",
    "    losses = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for batch_inputs, batch_desired_outputs in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_inputs.unsqueeze(0))\n",
    "\n",
    "            loss = criterion(outputs.view(batch_desired_outputs.shape), batch_desired_outputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            cur_item = loss.item()\n",
    "            total_loss += cur_item\n",
    "        \n",
    "        scheduler.step()\n",
    "        average_loss = total_loss / len(dataloader)\n",
    "        if viz_accuracy:\n",
    "            losses.append(average_loss)\n",
    "        if (epoch+1) % 50 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.4f}')\n",
    "    \n",
    "    if viz_accuracy:\n",
    "        plt.scatter(x=range(1, len(losses)+1), y=losses)\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Average loss\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = 'cpu'\n",
    "class QDataset(Dataset):\n",
    "    def __init__(self, qenv):\n",
    "        self.keys = torch.from_numpy(np.array(list(qenv.Q_table.keys())))\n",
    "        self.values = torch.empty(len(self.keys))\n",
    "        for i in range(len(self.keys)):\n",
    "            self.values[i] = torch.tensor(qenv.best_move(self.keys[i].reshape(2, 2)))\n",
    "\n",
    "        self.keys = self.keys.to(device)\n",
    "        self.values = self.values.to(device)\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.keys[i]), nn.functional.one_hot(self.values[i], num_classes=4)\n",
    "\n",
    "dataset = QDataset(env)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "znet = SL2Z_NN()\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t9/gxb6q8zj21dff090q066td740000gn/T/ipykernel_95486/2903381304.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(self.keys[i]), nn.functional.one_hot(self.values[i], num_classes=4)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one_hot is only applicable to index tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mznet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0005\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mviz_accuracy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 30\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, dataloader, num_epochs, learning_rate, viz_accuracy)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     29\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 30\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_desired_outputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_inputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 17\u001b[0m, in \u001b[0;36mQDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys[i]), \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_hot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one_hot is only applicable to index tensor."
     ]
    }
   ],
   "source": [
    "train_model(znet, loss, dataset, 500, learning_rate=0.0005, viz_accuracy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 3., 3., 3.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.QDataset at 0x13d98ee10>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 3., 3., 3.],\n",
       "        [3., 3., 3., 3.],\n",
       "        [1., 1., 1., 1.],\n",
       "        ...,\n",
       "        [2., 2., 2., 2.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [3., 3., 3., 3.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
