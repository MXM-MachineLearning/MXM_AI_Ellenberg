{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_to_tuple(matrix):\n",
    "    return (matrix[0][0], matrix[0][1], \n",
    "            matrix[1][0], matrix[1][1]) \n",
    "\n",
    "# index 12 according to alex's paper. Is it congruent to identity mod 2 or mod 4?\n",
    "# can generate with any coset I want by starting at a representative from each coset and see if we get our way back to it\n",
    "A = np.array([[1, 2], [0, 1]])\n",
    "B = np.array([[1, 0], [2, 1]])\n",
    "\n",
    "# elements on the diagonal are 1 mod 4. \n",
    "# elements not on the diagonal are 0 mod 2. \n",
    "\n",
    "# C is the inverse of A\n",
    "# D is the inverse of B\n",
    "C = np.linalg.inv(A)\n",
    "D = np.linalg.inv(B)\n",
    "\n",
    "identity = np.array([[1, 0], [0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def epsilon_greedy_search(Epsilon, qtable, state):\n",
    "    if (random.random() < Epsilon):\n",
    "        # 0 is 'apply matrix A', 1 is 'apply matrix B'\n",
    "        # 2 is 'apply matrix C', 3 is 'apply matrix D'\n",
    "        return random.choice([0, 1, 2, 3])\n",
    "    else:\n",
    "        # get the best move for the current state\n",
    "        return best_move_for_a_state(Q_table=qtable, state=state)\n",
    "    \n",
    "# I would like to return the best move for a given state\n",
    "def best_move_for_a_state(Q_table, state):\n",
    "    # vals = Q_table[(state[0][1], state[0][2], state[1][2])]\n",
    "\n",
    "    apply_A = state @ A\n",
    "    apply_B = state @ B\n",
    "    apply_C = state @ C\n",
    "    apply_D = state @ D\n",
    "\n",
    "    vals = [0, 0, 0, 0]\n",
    "    vals[0] = Q_table[matrix_to_tuple(apply_A)]\n",
    "    vals[1] = Q_table[matrix_to_tuple(apply_B)]\n",
    "    vals[2] = Q_table[matrix_to_tuple(apply_C)]\n",
    "    vals[3] = Q_table[matrix_to_tuple(apply_D)]\n",
    "\n",
    "    # if we haven't visited this state before, return a random choice of 0, 1, 2, or 3\n",
    "    if vals==[0, 0, 0, 0]:\n",
    "        return random.choice([0, 1, 2, 3])\n",
    "    \n",
    "    # if we have visited this state before, return the current best choice\n",
    "    return np.argmax(vals)\n",
    "\n",
    "# over a given state, return the maximum value of the table for that state\n",
    "def max_a_prime(Q_table, state):\n",
    "    apply_A = state @ A\n",
    "    apply_B = state @ B\n",
    "    apply_C = state @ C\n",
    "    apply_D = state @ D\n",
    "\n",
    "    vals = [0, 0, 0, 0]\n",
    "    vals[0] = Q_table[matrix_to_tuple(apply_A)]\n",
    "    vals[1] = Q_table[matrix_to_tuple(apply_B)]\n",
    "    vals[2] = Q_table[matrix_to_tuple(apply_C)]\n",
    "    vals[3] = Q_table[matrix_to_tuple(apply_D)]\n",
    "    \n",
    "    return max(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_reward = 100\n",
    "step_penalty = -1\n",
    "\n",
    "def getReward(matrix):\n",
    "    if (matrix==identity).all():\n",
    "        return max_reward\n",
    "    else:\n",
    "        return step_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Data_Generation/Data_files/subset_sl2_Z.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_step(oldObs, action):\n",
    "    # action is always either 0, 1, 2, or 3\n",
    "    next_state = []\n",
    "    if action==0:\n",
    "        next_state = oldObs @ A\n",
    "    elif action==1:\n",
    "        next_state = oldObs @ B\n",
    "    elif action==2:\n",
    "        next_state = oldObs @ C\n",
    "    else:\n",
    "        next_state = oldObs @ D\n",
    "    curReward = getReward(next_state)\n",
    "    done = curReward==max_reward\n",
    "    return (next_state, curReward, done)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mat(mat, index):\n",
    "    if index==0:\n",
    "        return mat @ A\n",
    "    elif index==1:\n",
    "        return mat @ B\n",
    "    elif index==2:\n",
    "        return mat @ C\n",
    "    elif index==3:\n",
    "        return mat @ D\n",
    "    assert(1==2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuple_to_matrix(tuple):\n",
    "    return np.array([[tuple[0], tuple[1]], [tuple[2], tuple[3]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val1</th>\n",
       "      <th>val2</th>\n",
       "      <th>val3</th>\n",
       "      <th>val4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49.0</td>\n",
       "      <td>-226.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>-83.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>961.0</td>\n",
       "      <td>3388.0</td>\n",
       "      <td>-3138.0</td>\n",
       "      <td>-11063.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2209.0</td>\n",
       "      <td>1438.0</td>\n",
       "      <td>-3802.0</td>\n",
       "      <td>-2475.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>1509.0</td>\n",
       "      <td>-4120.0</td>\n",
       "      <td>3652.0</td>\n",
       "      <td>-9971.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>9033.0</td>\n",
       "      <td>16024.0</td>\n",
       "      <td>-4198.0</td>\n",
       "      <td>-7447.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>-35.0</td>\n",
       "      <td>-48.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>-11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>104501.0</td>\n",
       "      <td>43952.0</td>\n",
       "      <td>-22604.0</td>\n",
       "      <td>-9507.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          val1     val2     val3     val4\n",
       "0         49.0   -226.0     18.0    -83.0\n",
       "1        961.0   3388.0  -3138.0 -11063.0\n",
       "2       2209.0   1438.0  -3802.0  -2475.0\n",
       "3         17.0      4.0    140.0     33.0\n",
       "4          1.0     -2.0      0.0      1.0\n",
       "...        ...      ...      ...      ...\n",
       "9995       1.0      0.0      8.0      1.0\n",
       "9996    1509.0  -4120.0   3652.0  -9971.0\n",
       "9997    9033.0  16024.0  -4198.0  -7447.0\n",
       "9998     -35.0    -48.0     -8.0    -11.0\n",
       "9999  104501.0  43952.0 -22604.0  -9507.0\n",
       "\n",
       "[10000 rows x 4 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df['val1'] % 2 == 1) & (df['val2'] % 2 == 0) & (df['val3'] % 2 == 0) & (df['val4'] % 2 == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val1</th>\n",
       "      <th>val2</th>\n",
       "      <th>val3</th>\n",
       "      <th>val4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49.0</td>\n",
       "      <td>-226.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>-83.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>961.0</td>\n",
       "      <td>3388.0</td>\n",
       "      <td>-3138.0</td>\n",
       "      <td>-11063.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2209.0</td>\n",
       "      <td>1438.0</td>\n",
       "      <td>-3802.0</td>\n",
       "      <td>-2475.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>1509.0</td>\n",
       "      <td>-4120.0</td>\n",
       "      <td>3652.0</td>\n",
       "      <td>-9971.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>9033.0</td>\n",
       "      <td>16024.0</td>\n",
       "      <td>-4198.0</td>\n",
       "      <td>-7447.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>-35.0</td>\n",
       "      <td>-48.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>-11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>104501.0</td>\n",
       "      <td>43952.0</td>\n",
       "      <td>-22604.0</td>\n",
       "      <td>-9507.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          val1     val2     val3     val4\n",
       "0         49.0   -226.0     18.0    -83.0\n",
       "1        961.0   3388.0  -3138.0 -11063.0\n",
       "2       2209.0   1438.0  -3802.0  -2475.0\n",
       "3         17.0      4.0    140.0     33.0\n",
       "4          1.0     -2.0      0.0      1.0\n",
       "...        ...      ...      ...      ...\n",
       "9995       1.0      0.0      8.0      1.0\n",
       "9996    1509.0  -4120.0   3652.0  -9971.0\n",
       "9997    9033.0  16024.0  -4198.0  -7447.0\n",
       "9998     -35.0    -48.0     -8.0    -11.0\n",
       "9999  104501.0  43952.0 -22604.0  -9507.0\n",
       "\n",
       "[10000 rows x 4 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_df = df[df['val1'] % 2 == 1]\n",
    "filter_df = filter_df[filter_df['val2'] % 2 == 0]\n",
    "filter_df = filter_df[filter_df['val3'] % 2 == 0]\n",
    "filter_df = filter_df[filter_df['val4'] % 2 == 1]\n",
    "filter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward for the last 100 iterations: -96.09\n",
      "epsilon: 0.989950333757503\n",
      "Average reward for the last 100 iterations: -100.0\n",
      "epsilon: 0.9800996732739187\n",
      "Average reward for the last 100 iterations: -98.03\n",
      "epsilon: 0.9703470333764725\n",
      "Average reward for the last 100 iterations: -94.13\n",
      "epsilon: 0.9606914386955115\n",
      "Average reward for the last 100 iterations: -96.03\n",
      "epsilon: 0.9511319235669539\n",
      "Average reward for the last 100 iterations: -90.34\n",
      "epsilon: 0.9416675319357145\n",
      "Average reward for the last 100 iterations: -90.14\n",
      "epsilon: 0.9322973172600907\n",
      "Average reward for the last 100 iterations: -94.12\n",
      "epsilon: 0.9230203424170932\n",
      "Average reward for the last 100 iterations: -92.2\n",
      "epsilon: 0.9138356796087268\n",
      "Average reward for the last 100 iterations: -98.0\n",
      "epsilon: 0.9047424102692004\n",
      "Average reward for the last 100 iterations: -92.21\n",
      "epsilon: 0.89573962497306\n",
      "Average reward for the last 100 iterations: -92.04\n",
      "epsilon: 0.8868264233442354\n",
      "Average reward for the last 100 iterations: -90.06\n",
      "epsilon: 0.8780019139659949\n",
      "Average reward for the last 100 iterations: -90.52\n",
      "epsilon: 0.8692652142917918\n",
      "Average reward for the last 100 iterations: -90.08\n",
      "epsilon: 0.8606154505570021\n",
      "Average reward for the last 100 iterations: -88.25\n",
      "epsilon: 0.8520517576915366\n",
      "Average reward for the last 100 iterations: -88.22\n",
      "epsilon: 0.8435732792333273\n",
      "Average reward for the last 100 iterations: -84.47\n",
      "epsilon: 0.8351791672426676\n",
      "Average reward for the last 100 iterations: -92.27\n",
      "epsilon: 0.8268685822174137\n",
      "Average reward for the last 100 iterations: -86.26\n",
      "epsilon: 0.8186406930090225\n",
      "Average reward for the last 100 iterations: -79.04\n",
      "epsilon: 0.8104946767394292\n",
      "Average reward for the last 100 iterations: -82.72\n",
      "epsilon: 0.802429718718749\n",
      "Average reward for the last 100 iterations: -85.6\n",
      "epsilon: 0.7944450123638009\n",
      "Average reward for the last 100 iterations: -90.52\n",
      "epsilon: 0.78653975911744\n",
      "Average reward for the last 100 iterations: -90.99\n",
      "epsilon: 0.7787131683686925\n",
      "Average reward for the last 100 iterations: -69.33\n",
      "epsilon: 0.7709644573736867\n",
      "Average reward for the last 100 iterations: -63.94\n",
      "epsilon: 0.763292851177371\n",
      "Average reward for the last 100 iterations: -76.95\n",
      "epsilon: 0.7556975825360077\n",
      "Average reward for the last 100 iterations: -71.13\n",
      "epsilon: 0.7481778918404428\n",
      "Average reward for the last 100 iterations: -72.8\n",
      "epsilon: 0.7407330270401349\n",
      "Average reward for the last 100 iterations: -63.54\n",
      "epsilon: 0.7333622435679438\n",
      "Average reward for the last 100 iterations: -57.23\n",
      "epsilon: 0.7260648042656639\n",
      "Average reward for the last 100 iterations: -66.72\n",
      "epsilon: 0.7188399793103014\n",
      "Average reward for the last 100 iterations: -69.68\n",
      "epsilon: 0.7116870461410829\n",
      "Average reward for the last 100 iterations: -65.41\n",
      "epsilon: 0.7046052893871948\n",
      "Average reward for the last 100 iterations: -53.6\n",
      "epsilon: 0.6975940007962347\n",
      "Average reward for the last 100 iterations: -64.46\n",
      "epsilon: 0.690652479163381\n",
      "Average reward for the last 100 iterations: -63.34\n",
      "epsilon: 0.6837800302612622\n",
      "Average reward for the last 100 iterations: -55.54\n",
      "epsilon: 0.6769759667705286\n",
      "Average reward for the last 100 iterations: -56.56\n",
      "epsilon: 0.6702396082111141\n",
      "Average reward for the last 100 iterations: -54.92\n",
      "epsilon: 0.6635702808741777\n",
      "Average reward for the last 100 iterations: -58.29\n",
      "epsilon: 0.6569673177547274\n",
      "Average reward for the last 100 iterations: -51.39\n",
      "epsilon: 0.6504300584849119\n",
      "Average reward for the last 100 iterations: -45.8\n",
      "epsilon: 0.6439578492679773\n",
      "Average reward for the last 100 iterations: -49.27\n",
      "epsilon: 0.6375500428128791\n",
      "Average reward for the last 100 iterations: -35.95\n",
      "epsilon: 0.6312059982695464\n",
      "Average reward for the last 100 iterations: -33.26\n",
      "epsilon: 0.6249250811647913\n",
      "Average reward for the last 100 iterations: -28.91\n",
      "epsilon: 0.6187066633388536\n",
      "Average reward for the last 100 iterations: -30.33\n",
      "epsilon: 0.6125501228825772\n",
      "Average reward for the last 100 iterations: -33.16\n",
      "epsilon: 0.6064548440752141\n",
      "Average reward for the last 100 iterations: -45.26\n",
      "epsilon: 0.6004202173228442\n",
      "Average reward for the last 100 iterations: -34.0\n",
      "epsilon: 0.5944456390974114\n",
      "Average reward for the last 100 iterations: -22.32\n",
      "epsilon: 0.5885305118763617\n",
      "Average reward for the last 100 iterations: -21.38\n",
      "epsilon: 0.5826742440828869\n",
      "Average reward for the last 100 iterations: -26.3\n",
      "epsilon: 0.576876250026757\n",
      "Average reward for the last 100 iterations: -9.58\n",
      "epsilon: 0.5711359498457492\n",
      "Average reward for the last 100 iterations: -22.64\n",
      "epsilon: 0.5654527694476531\n",
      "Average reward for the last 100 iterations: -8.7\n",
      "epsilon: 0.559826140452854\n",
      "Average reward for the last 100 iterations: -11.32\n",
      "epsilon: 0.5542555001374916\n",
      "Average reward for the last 100 iterations: -5.27\n",
      "epsilon: 0.548740291377179\n",
      "Average reward for the last 100 iterations: -17.16\n",
      "epsilon: 0.5432799625912865\n",
      "Average reward for the last 100 iterations: -31.33\n",
      "epsilon: 0.5378739676877772\n",
      "Average reward for the last 100 iterations: -17.17\n",
      "epsilon: 0.532521766008588\n",
      "Average reward for the last 100 iterations: -22.41\n",
      "epsilon: 0.5272228222755642\n",
      "Average reward for the last 100 iterations: -12.22\n",
      "epsilon: 0.5219766065369207\n",
      "Average reward for the last 100 iterations: -19.12\n",
      "epsilon: 0.5167825941142447\n",
      "Average reward for the last 100 iterations: -12.05\n",
      "epsilon: 0.5116402655500194\n",
      "Average reward for the last 100 iterations: -7.18\n",
      "epsilon: 0.5065491065556748\n",
      "Average reward for the last 100 iterations: -15.3\n",
      "epsilon: 0.5015086079601511\n",
      "Average reward for the last 100 iterations: -15.7\n",
      "epsilon: 0.4965182656589779\n",
      "Average reward for the last 100 iterations: -9.71\n",
      "epsilon: 0.491577580563858\n",
      "Average reward for the last 100 iterations: -14.88\n",
      "epsilon: 0.4866860585527523\n",
      "Average reward for the last 100 iterations: -7.63\n",
      "epsilon: 0.4818432104204629\n",
      "Average reward for the last 100 iterations: -20.26\n",
      "epsilon: 0.4770485518297069\n",
      "Average reward for the last 100 iterations: 7.48\n",
      "epsilon: 0.47230160326267795\n",
      "Average reward for the last 100 iterations: -12.78\n",
      "epsilon: 0.46760188997308916\n",
      "Average reward for the last 100 iterations: -1.68\n",
      "epsilon: 0.4629489419386927\n",
      "Average reward for the last 100 iterations: 12.71\n",
      "epsilon: 0.45834229381427294\n",
      "Average reward for the last 100 iterations: -5.01\n",
      "epsilon: 0.4537814848851073\n",
      "Average reward for the last 100 iterations: 0.59\n",
      "epsilon: 0.4492660590208893\n",
      "Average reward for the last 100 iterations: 13.12\n",
      "epsilon: 0.44479556463011005\n",
      "Average reward for the last 100 iterations: -1.72\n",
      "epsilon: 0.44036955461489585\n",
      "Average reward for the last 100 iterations: -14.14\n",
      "epsilon: 0.4359875863262917\n",
      "Average reward for the last 100 iterations: 1.28\n",
      "epsilon: 0.4316492215199928\n",
      "Average reward for the last 100 iterations: 4.46\n",
      "epsilon: 0.42735402631251446\n",
      "Average reward for the last 100 iterations: -23.11\n",
      "epsilon: 0.4231015711378002\n",
      "Average reward for the last 100 iterations: -3.93\n",
      "epsilon: 0.4188914307042601\n",
      "Average reward for the last 100 iterations: 5.56\n",
      "epsilon: 0.41472318395223523\n",
      "Average reward for the last 100 iterations: -20.03\n",
      "epsilon: 0.4105964140118904\n",
      "Average reward for the last 100 iterations: 14.1\n",
      "epsilon: 0.406510708161521\n",
      "Average reward for the last 100 iterations: -4.53\n",
      "epsilon: 0.4024656577862749\n",
      "Average reward for the last 100 iterations: 2.52\n",
      "epsilon: 0.39846085833728956\n",
      "Average reward for the last 100 iterations: -11.85\n",
      "epsilon: 0.3944959092912299\n",
      "Average reward for the last 100 iterations: 4.57\n",
      "epsilon: 0.39057041411023374\n",
      "Average reward for the last 100 iterations: -12.51\n",
      "epsilon: 0.3866839802022521\n",
      "Average reward for the last 100 iterations: -11.29\n",
      "epsilon: 0.3828362188817869\n",
      "Average reward for the last 100 iterations: -15.72\n",
      "epsilon: 0.3790267453310186\n",
      "Average reward for the last 100 iterations: -5.56\n",
      "epsilon: 0.3752551785613178\n",
      "Average reward for the last 100 iterations: -2.58\n",
      "epsilon: 0.3715211413751451\n",
      "Average reward for the last 100 iterations: -28.75\n",
      "epsilon: 0.3678242603283259\n",
      "Average reward for the last 100 iterations: 0.07\n",
      "epsilon: 0.3641641656927023\n",
      "Average reward for the last 100 iterations: -16.93\n",
      "epsilon: 0.36054049141915495\n",
      "Average reward for the last 100 iterations: -12.95\n",
      "epsilon: 0.3569528751009966\n",
      "Average reward for the last 100 iterations: -22.63\n",
      "epsilon: 0.3534009579377257\n",
      "Average reward for the last 100 iterations: 1.02\n",
      "epsilon: 0.3498843846991425\n",
      "Average reward for the last 100 iterations: 10.59\n",
      "epsilon: 0.34640280368982374\n",
      "Average reward for the last 100 iterations: 0.41\n",
      "epsilon: 0.34295586671394696\n",
      "Average reward for the last 100 iterations: 5.34\n",
      "epsilon: 0.33954322904046974\n",
      "Average reward for the last 100 iterations: -2.61\n",
      "epsilon: 0.33616454936865003\n",
      "Average reward for the last 100 iterations: -0.87\n",
      "epsilon: 0.332819489793915\n",
      "Average reward for the last 100 iterations: -8.57\n",
      "epsilon: 0.32950771577406535\n",
      "Average reward for the last 100 iterations: 17.83\n",
      "epsilon: 0.326228896095818\n",
      "Average reward for the last 100 iterations: 7.54\n",
      "epsilon: 0.32298270284168096\n",
      "Average reward for the last 100 iterations: -13.78\n",
      "epsilon: 0.3197688113571579\n",
      "Average reward for the last 100 iterations: -6.0\n",
      "epsilon: 0.3165869002182801\n",
      "Average reward for the last 100 iterations: 1.03\n",
      "epsilon: 0.3134366511994595\n",
      "Average reward for the last 100 iterations: 6.24\n",
      "epsilon: 0.31031774924166283\n",
      "Average reward for the last 100 iterations: -2.01\n",
      "epsilon: 0.30722988242090343\n",
      "Average reward for the last 100 iterations: -3.37\n",
      "epsilon: 0.3041727419170432\n",
      "Average reward for the last 100 iterations: -4.18\n",
      "epsilon: 0.3011460219829101\n",
      "Average reward for the last 100 iterations: 8.94\n",
      "epsilon: 0.2981494199137175\n",
      "Average reward for the last 100 iterations: -4.41\n",
      "epsilon: 0.29518263601679207\n",
      "Average reward for the last 100 iterations: -6.74\n",
      "epsilon: 0.2922453735816008\n",
      "Average reward for the last 100 iterations: 5.48\n",
      "epsilon: 0.2893373388500768\n",
      "Average reward for the last 100 iterations: 3.83\n",
      "epsilon: 0.28645824098724004\n",
      "Average reward for the last 100 iterations: 0.73\n",
      "epsilon: 0.2836077920521105\n",
      "Average reward for the last 100 iterations: -5.78\n",
      "epsilon: 0.28078570696891186\n",
      "Average reward for the last 100 iterations: -22.08\n",
      "epsilon: 0.27799170349856034\n",
      "Average reward for the last 100 iterations: 11.53\n",
      "epsilon: 0.2752255022104372\n",
      "Average reward for the last 100 iterations: 11.01\n",
      "epsilon: 0.27248682645444433\n",
      "Average reward for the last 100 iterations: -11.96\n",
      "epsilon: 0.2697754023333332\n",
      "Average reward for the last 100 iterations: -15.55\n",
      "epsilon: 0.2670909586753151\n",
      "Average reward for the last 100 iterations: 2.11\n",
      "epsilon: 0.2644332270069402\n",
      "Average reward for the last 100 iterations: 3.83\n",
      "epsilon: 0.26180194152624664\n",
      "Average reward for the last 100 iterations: 1.4\n",
      "epsilon: 0.2591968390761776\n",
      "Average reward for the last 100 iterations: -1.61\n",
      "epsilon: 0.2566176591182636\n",
      "Average reward for the last 100 iterations: -1.64\n",
      "epsilon: 0.2540641437065648\n",
      "Average reward for the last 100 iterations: 8.23\n",
      "epsilon: 0.2515360374618739\n",
      "Average reward for the last 100 iterations: 0.24\n",
      "epsilon: 0.24903308754617642\n",
      "Average reward for the last 100 iterations: -3.36\n",
      "epsilon: 0.24655504363736244\n",
      "Average reward for the last 100 iterations: 4.1\n",
      "epsilon: 0.24410165790419308\n",
      "Average reward for the last 100 iterations: 17.35\n",
      "epsilon: 0.24167268498151395\n",
      "Average reward for the last 100 iterations: 0.58\n",
      "epsilon: 0.2392678819457161\n",
      "Average reward for the last 100 iterations: -1.69\n",
      "epsilon: 0.2368870082904414\n",
      "Average reward for the last 100 iterations: 6.05\n",
      "epsilon: 0.23452982590252897\n",
      "Average reward for the last 100 iterations: -1.38\n",
      "epsilon: 0.2321960990382015\n",
      "Average reward for the last 100 iterations: 4.3\n",
      "epsilon: 0.22988559429948774\n",
      "Average reward for the last 100 iterations: 4.24\n",
      "epsilon: 0.2275980806108809\n",
      "Average reward for the last 100 iterations: -13.34\n",
      "epsilon: 0.2253333291962282\n",
      "Average reward for the last 100 iterations: -13.29\n",
      "epsilon: 0.22309111355585096\n",
      "Average reward for the last 100 iterations: 2.36\n",
      "epsilon: 0.2208712094438921\n",
      "Average reward for the last 100 iterations: 11.44\n",
      "epsilon: 0.2186733948458889\n",
      "Average reward for the last 100 iterations: 10.09\n",
      "epsilon: 0.2164974499565696\n",
      "Average reward for the last 100 iterations: -16.89\n",
      "epsilon: 0.2143431571578701\n",
      "Average reward for the last 100 iterations: 8.07\n",
      "epsilon: 0.21221030099717023\n",
      "Average reward for the last 100 iterations: 2.7\n",
      "epsilon: 0.2100986681657454\n",
      "Average reward for the last 100 iterations: 14.81\n",
      "epsilon: 0.2080080474774341\n",
      "Average reward for the last 100 iterations: -9.31\n",
      "epsilon: 0.20593822984751717\n",
      "Average reward for the last 100 iterations: -5.4\n",
      "epsilon: 0.20388900827180606\n",
      "Average reward for the last 100 iterations: 13.97\n",
      "epsilon: 0.20186017780594118\n",
      "Average reward for the last 100 iterations: 10.62\n",
      "epsilon: 0.19985153554489493\n",
      "Average reward for the last 100 iterations: 15.79\n",
      "epsilon: 0.19786288060267845\n",
      "Average reward for the last 100 iterations: 0.2\n",
      "epsilon: 0.19589401409225174\n",
      "Average reward for the last 100 iterations: -14.38\n",
      "epsilon: 0.19394473910563204\n",
      "Average reward for the last 100 iterations: -6.77\n",
      "epsilon: 0.19201486069420162\n",
      "Average reward for the last 100 iterations: -13.09\n",
      "epsilon: 0.1901041858492102\n",
      "Average reward for the last 100 iterations: -7.04\n",
      "epsilon: 0.1882125234824722\n",
      "Average reward for the last 100 iterations: -2.9\n",
      "epsilon: 0.18633968440725585\n",
      "Average reward for the last 100 iterations: -2.94\n",
      "epsilon: 0.18448548131936265\n",
      "Average reward for the last 100 iterations: -6.86\n",
      "epsilon: 0.1826497287783945\n",
      "Average reward for the last 100 iterations: -0.96\n",
      "epsilon: 0.1808322431892079\n",
      "Average reward for the last 100 iterations: 4.66\n",
      "epsilon: 0.17903284278355266\n",
      "Average reward for the last 100 iterations: -7.22\n",
      "epsilon: 0.17725134760189254\n",
      "Average reward for the last 100 iterations: -7.38\n",
      "epsilon: 0.1754875794754081\n",
      "Average reward for the last 100 iterations: -6.9\n",
      "epsilon: 0.17374136200817725\n",
      "Average reward for the last 100 iterations: -5.07\n",
      "epsilon: 0.17201252055953417\n",
      "Average reward for the last 100 iterations: 12.44\n",
      "epsilon: 0.17030088222660275\n",
      "Average reward for the last 100 iterations: 10.58\n",
      "epsilon: 0.16860627582700524\n",
      "Average reward for the last 100 iterations: -2.73\n",
      "epsilon: 0.16692853188174173\n",
      "Average reward for the last 100 iterations: 3.23\n",
      "epsilon: 0.16526748259824006\n",
      "Average reward for the last 100 iterations: 6.72\n",
      "epsilon: 0.1636229618535754\n",
      "Average reward for the last 100 iterations: -2.96\n",
      "epsilon: 0.16199480517785583\n",
      "Average reward for the last 100 iterations: 1.01\n",
      "epsilon: 0.16038284973777372\n",
      "Average reward for the last 100 iterations: -14.96\n",
      "epsilon: 0.15878693432032062\n",
      "Average reward for the last 100 iterations: 0.97\n",
      "epsilon: 0.1572068993166638\n",
      "Average reward for the last 100 iterations: -12.65\n",
      "epsilon: 0.15564258670618408\n",
      "Average reward for the last 100 iterations: 4.19\n",
      "epsilon: 0.15409384004067206\n",
      "Average reward for the last 100 iterations: 10.75\n",
      "epsilon: 0.15256050442868138\n",
      "Average reward for the last 100 iterations: -1.4\n",
      "epsilon: 0.1510424265200381\n",
      "Average reward for the last 100 iterations: 18.51\n",
      "epsilon: 0.14953945449050376\n",
      "Average reward for the last 100 iterations: -3.16\n",
      "epsilon: 0.1480514380265917\n",
      "Average reward for the last 100 iterations: 0.79\n",
      "epsilon: 0.14657822831053363\n",
      "Average reward for the last 100 iterations: 18.39\n",
      "epsilon: 0.14511967800539669\n",
      "Average reward for the last 100 iterations: 7.14\n",
      "epsilon: 0.14367564124034787\n",
      "Average reward for the last 100 iterations: -2.74\n",
      "epsilon: 0.14224597359606533\n",
      "Average reward for the last 100 iterations: -5.1\n",
      "epsilon: 0.1408305320902949\n",
      "Average reward for the last 100 iterations: -5.02\n",
      "epsilon: 0.13942917516355058\n",
      "Average reward for the last 100 iterations: 2.66\n",
      "epsilon: 0.1380417626649567\n",
      "Average reward for the last 100 iterations: 6.65\n",
      "epsilon: 0.13666815583823175\n",
      "Average reward for the last 100 iterations: 7.22\n",
      "epsilon: 0.13530821730781062\n",
      "Average reward for the last 100 iterations: -4.11\n",
      "epsilon: 0.1339618110651063\n",
      "Average reward for the last 100 iterations: 1.11\n",
      "epsilon: 0.13262880245490677\n",
      "Average reward for the last 100 iterations: 6.67\n",
      "epsilon: 0.13130905816190905\n",
      "Average reward for the last 100 iterations: 7.14\n",
      "epsilon: 0.1300024461973849\n",
      "Average reward for the last 100 iterations: 25.81\n",
      "epsilon: 0.1287088358859816\n",
      "Average reward for the last 100 iterations: 25.93\n",
      "epsilon: 0.12742809785265258\n",
      "Average reward for the last 100 iterations: 0.77\n",
      "epsilon: 0.12616010400971825\n",
      "Average reward for the last 100 iterations: 26.54\n",
      "epsilon: 0.1249047275440563\n",
      "Average reward for the last 100 iterations: -2.97\n",
      "epsilon: 0.12366184290441894\n",
      "Average reward for the last 100 iterations: -2.96\n",
      "epsilon: 0.12243132578887629\n",
      "Average reward for the last 100 iterations: -6.68\n",
      "epsilon: 0.12121305313238478\n",
      "Average reward for the last 100 iterations: -0.37\n",
      "epsilon: 0.1200069030944797\n",
      "Average reward for the last 100 iterations: 1.4\n",
      "epsilon: 0.11881275504708917\n",
      "Average reward for the last 100 iterations: 0.98\n",
      "epsilon: 0.11763048956247056\n",
      "Average reward for the last 100 iterations: 6.75\n",
      "epsilon: 0.11645998840126634\n",
      "Average reward for the last 100 iterations: 18.83\n",
      "epsilon: 0.11530113450067855\n",
      "Average reward for the last 100 iterations: 4.82\n",
      "epsilon: 0.11415381196276177\n",
      "Average reward for the last 100 iterations: 10.55\n",
      "epsilon: 0.11301790604283157\n",
      "Average reward for the last 100 iterations: -14.47\n",
      "epsilon: 0.11189330313798898\n",
      "Average reward for the last 100 iterations: 9.04\n",
      "epsilon: 0.11077989077575923\n",
      "Average reward for the last 100 iterations: 12.84\n",
      "epsilon: 0.10967755760284283\n",
      "Average reward for the last 100 iterations: 9.49\n",
      "epsilon: 0.10858619337397935\n",
      "Average reward for the last 100 iterations: -3.22\n",
      "epsilon: 0.10750568894092176\n",
      "Average reward for the last 100 iterations: -3.23\n",
      "epsilon: 0.10643593624151992\n",
      "Average reward for the last 100 iterations: -0.14\n",
      "epsilon: 0.1053768282889138\n",
      "Average reward for the last 100 iterations: 5.42\n",
      "epsilon: 0.1043282591608334\n",
      "Average reward for the last 100 iterations: 3.64\n",
      "epsilon: 0.10329012398900515\n",
      "Average reward for the last 100 iterations: 18.64\n",
      "epsilon: 0.10226231894866437\n",
      "Average reward for the last 100 iterations: 9.16\n",
      "epsilon: 0.10124474124817139\n",
      "Average reward for the last 100 iterations: 3.49\n",
      "epsilon: 0.10023728911873117\n",
      "Average reward for the last 100 iterations: 5.41\n",
      "epsilon: 0.09923986180421565\n",
      "Average reward for the last 100 iterations: 28.74\n",
      "epsilon: 0.0982523595510868\n",
      "Average reward for the last 100 iterations: -4.39\n",
      "epsilon: 0.09727468359842038\n",
      "Average reward for the last 100 iterations: 14.92\n",
      "epsilon: 0.09630673616802857\n",
      "Average reward for the last 100 iterations: 16.9\n",
      "epsilon: 0.09534842045468113\n",
      "Average reward for the last 100 iterations: 2.79\n",
      "epsilon: 0.09439964061642395\n",
      "Average reward for the last 100 iterations: 14.48\n",
      "epsilon: 0.09346030176499369\n",
      "Average reward for the last 100 iterations: 16.48\n",
      "epsilon: 0.09253030995632805\n",
      "Average reward for the last 100 iterations: -2.6\n",
      "epsilon: 0.09160957218117023\n",
      "Average reward for the last 100 iterations: 7.17\n",
      "epsilon: 0.09069799635576713\n",
      "Average reward for the last 100 iterations: 3.26\n",
      "epsilon: 0.08979549131265965\n",
      "Average reward for the last 100 iterations: 12.66\n",
      "epsilon: 0.08890196679156555\n",
      "Average reward for the last 100 iterations: 17.33\n",
      "epsilon: 0.08801733343035181\n",
      "Average reward for the last 100 iterations: 12.7\n",
      "epsilon: 0.0871415027560978\n",
      "Average reward for the last 100 iterations: -0.8\n",
      "epsilon: 0.0862743871762469\n",
      "Average reward for the last 100 iterations: 3.69\n",
      "epsilon: 0.08541589996984655\n",
      "Average reward for the last 100 iterations: -4.69\n",
      "epsilon: 0.08456595527887494\n",
      "Average reward for the last 100 iterations: 7.09\n",
      "epsilon: 0.08372446809965417\n",
      "Average reward for the last 100 iterations: 3.0\n",
      "epsilon: 0.08289135427434946\n",
      "Average reward for the last 100 iterations: 15.45\n",
      "epsilon: 0.08206653048255198\n",
      "Average reward for the last 100 iterations: 12.83\n",
      "epsilon: 0.08124991423294586\n",
      "Average reward for the last 100 iterations: 8.97\n",
      "epsilon: 0.08044142385505873\n",
      "Average reward for the last 100 iterations: 13.27\n",
      "epsilon: 0.07964097849109329\n",
      "Average reward for the last 100 iterations: -0.9\n",
      "epsilon: 0.07884849808784071\n",
      "Average reward for the last 100 iterations: 11.09\n",
      "epsilon: 0.07806390338867449\n",
      "Average reward for the last 100 iterations: 12.74\n",
      "epsilon: 0.0772871159256243\n",
      "Average reward for the last 100 iterations: 1.26\n",
      "epsilon: 0.07651805801152776\n",
      "Average reward for the last 100 iterations: 4.94\n",
      "epsilon: 0.07575665273226108\n",
      "Average reward for the last 100 iterations: 9.41\n",
      "epsilon: 0.07500282393904703\n",
      "Average reward for the last 100 iterations: -12.43\n",
      "epsilon: 0.0742564962408389\n",
      "Average reward for the last 100 iterations: 8.91\n",
      "epsilon: 0.07351759499678091\n",
      "Average reward for the last 100 iterations: 7.54\n",
      "epsilon: 0.07278604630874305\n",
      "Average reward for the last 100 iterations: 11.35\n",
      "epsilon: 0.07206177701393056\n",
      "Average reward for the last 100 iterations: 8.97\n",
      "epsilon: 0.07134471467756703\n",
      "Average reward for the last 100 iterations: 2.85\n",
      "epsilon: 0.07063478758564987\n",
      "Average reward for the last 100 iterations: 15.1\n",
      "epsilon: 0.06993192473777823\n",
      "Average reward for the last 100 iterations: 11.37\n",
      "epsilon: 0.06923605584005207\n",
      "Average reward for the last 100 iterations: 12.8\n",
      "epsilon: 0.06854711129804245\n",
      "Average reward for the last 100 iterations: 1.16\n",
      "epsilon: 0.0678650222098307\n",
      "Average reward for the last 100 iterations: 1.24\n",
      "epsilon: 0.0671897203591181\n",
      "Average reward for the last 100 iterations: 20.71\n",
      "epsilon: 0.0665211382084031\n",
      "Average reward for the last 100 iterations: 5.5\n",
      "epsilon: 0.06585920889222686\n",
      "Average reward for the last 100 iterations: 20.63\n",
      "epsilon: 0.06520386621048607\n",
      "Average reward for the last 100 iterations: 9.5\n",
      "epsilon: 0.06455504462181237\n",
      "Average reward for the last 100 iterations: 8.97\n",
      "epsilon: 0.06391267923701738\n",
      "Average reward for the last 100 iterations: 14.74\n",
      "epsilon: 0.0632767058126029\n",
      "Average reward for the last 100 iterations: 9.13\n",
      "epsilon: 0.06264706074433596\n",
      "Average reward for the last 100 iterations: -0.66\n",
      "epsilon: 0.06202368106088804\n",
      "Average reward for the last 100 iterations: -4.03\n",
      "epsilon: 0.061406504417536784\n",
      "Average reward for the last 100 iterations: -2.8\n",
      "epsilon: 0.06079546908993113\n",
      "Average reward for the last 100 iterations: 11.56\n",
      "epsilon: 0.060190513967918045\n",
      "Average reward for the last 100 iterations: 21.13\n",
      "epsilon: 0.059591578549431055\n",
      "Average reward for the last 100 iterations: -17.78\n",
      "epsilon: 0.05899860293443916\n",
      "Average reward for the last 100 iterations: 11.49\n",
      "epsilon: 0.05841152781895632\n",
      "Average reward for the last 100 iterations: 9.27\n",
      "epsilon: 0.05783029448911036\n",
      "Average reward for the last 100 iterations: 14.9\n",
      "epsilon: 0.057254844815271044\n",
      "Average reward for the last 100 iterations: -2.35\n",
      "epsilon: 0.056685121246236224\n",
      "Average reward for the last 100 iterations: -2.47\n",
      "epsilon: 0.05612106680347639\n",
      "Average reward for the last 100 iterations: 17.1\n",
      "epsilon: 0.055562625075436065\n",
      "Average reward for the last 100 iterations: 21.1\n",
      "epsilon: 0.0550097402118921\n",
      "Average reward for the last 100 iterations: 14.93\n",
      "epsilon: 0.05446235691836792\n",
      "Average reward for the last 100 iterations: 7.35\n",
      "epsilon: 0.05392042045060362\n",
      "Average reward for the last 100 iterations: 4.97\n",
      "epsilon: 0.053383876609080824\n",
      "Average reward for the last 100 iterations: 20.84\n",
      "epsilon: 0.052852671733602266\n",
      "Average reward for the last 100 iterations: 7.38\n",
      "epsilon: 0.052326752697925104\n",
      "Average reward for the last 100 iterations: 7.19\n",
      "epsilon: 0.051806066904447695\n",
      "Average reward for the last 100 iterations: 13.0\n",
      "epsilon: 0.051290562278949396\n",
      "Average reward for the last 100 iterations: 11.16\n",
      "epsilon: 0.05078018726538244\n",
      "Average reward for the last 100 iterations: -0.92\n",
      "epsilon: 0.050274890820715915\n"
     ]
    }
   ],
   "source": [
    "EPISODES = 30000\n",
    "LEARNING_RATE = .9\n",
    "DISCOUNT_FACTOR = .99\n",
    "EPSILON = 1\n",
    "EPSILON_DECAY = .9999\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# starts with an estimate of zero reward for each state.\n",
    "# adapted from ChatGPT\n",
    "Q_table = defaultdict(lambda: 0)\n",
    "\n",
    "episode_reward_record = deque(maxlen=100)\n",
    "\n",
    "for i in range(EPISODES):\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    # choose a random starting row\n",
    "    # adapted from https://stackoverflow.com/questions/15923826/random-row-selection-in-pandas-dataframe\n",
    "    cur_row = df.sample(1)\n",
    "    obs = np.array([\n",
    "        [int(cur_row['val1']), int(cur_row['val2'])], \n",
    "        [int(cur_row['val3']), int(cur_row['val4'])]\n",
    "        ])\n",
    "\n",
    "    index = 1\n",
    "\n",
    "    while (not done):\n",
    "        # perform an epsilon greedy action \n",
    "        # Q(s, a) = (1-LEARNING_RATE)Q(s, a) + (LEARNING_RATE)(r + DISCOUNT_FACTOR(max a'(Q(s', a'))))\n",
    "        action = epsilon_greedy_search(Epsilon=EPSILON, qtable=Q_table, state=obs)\n",
    "\n",
    "        oldObs = obs\n",
    "        obs,reward,done = get_next_step(oldObs, action)\n",
    "\n",
    "        # if done:\n",
    "        #     assert(1==2)\n",
    "        \n",
    "        Q_table[matrix_to_tuple(obs)] = (1-LEARNING_RATE) * Q_table[matrix_to_tuple(obs)] + (LEARNING_RATE) * (reward + DISCOUNT_FACTOR * (max_a_prime(Q_table, obs)))\n",
    "\n",
    "        episode_reward += reward # update episode reward\n",
    "\n",
    "        index += 1\n",
    "        # if we take more than 100 steps, end this iteration early (we are probably not making progress)\n",
    "        if index > 100:\n",
    "            done=True\n",
    "\n",
    "    # decay the epsilon\n",
    "    EPSILON *= EPSILON_DECAY\n",
    "\n",
    "    # record the reward for this episode\n",
    "    episode_reward_record.append(episode_reward) \n",
    "\n",
    "    if i%100 ==0 and i>0:\n",
    "        print(\"Average reward for the last 100 iterations: \" + str(sum(list(episode_reward_record))/100))\n",
    "        print(\"epsilon: \" + str(EPSILON) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def access_Q_table(mat):\n",
    "    return Q_table[matrix_to_tuple(mat)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "access_Q_table(np.array([[1, 1], [0, 1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4924.623115577806"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "access_Q_table(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with the other dataframe. \n",
    "test_df = pd.read_csv(\"../Data_Generation/Data_files/subset_sl2_Z_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_to_num_steps(cur_matrix):\n",
    "    index = 1\n",
    "    for i in range(50):\n",
    "        if (cur_matrix==identity).all():\n",
    "            return i\n",
    "        outputs = [0, 0, 0, 0]\n",
    "        outputs[0] = Q_table[matrix_to_tuple(cur_matrix@ A)]\n",
    "        outputs[1] = Q_table[matrix_to_tuple(cur_matrix@ B)]\n",
    "        outputs[2] = Q_table[matrix_to_tuple(cur_matrix@ C)]\n",
    "        outputs[3] = Q_table[matrix_to_tuple(cur_matrix@ D)]\n",
    "        index = np.argmax(outputs)\n",
    "        if index==0:\n",
    "            cur_matrix = cur_matrix @ A\n",
    "        elif index==1:\n",
    "            cur_matrix = cur_matrix @ B\n",
    "        elif index==2:\n",
    "            cur_matrix = cur_matrix @ C\n",
    "        elif index==3:\n",
    "            cur_matrix = cur_matrix @ D\n",
    "    return 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_Q_learning(cur_row):\n",
    "    cur_matrix = np.array([\n",
    "        [int(cur_row['val1']), int(cur_row['val2'])], \n",
    "        [int(cur_row['val3']), int(cur_row['val4'])]\n",
    "        ])\n",
    "    return matrix_to_num_steps(cur_matrix)\n",
    "\n",
    "test_df['num_moves_Q_learning_needs'] = test_df.apply(test_Q_learning, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The proportion of starting positions in the test dataset that we can find a route to the origin that's <50 steps: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2764"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The proportion of starting positions in the test dataset that we can find a route to the origin that's <50 steps: \")\n",
    "sum(test_df['num_moves_Q_learning_needs']!=100)/test_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of these, the proportion of times where we learned a path that was < 20 moves: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Of these, the proportion of times where we learned a path that was < 20 moves: \")\n",
    "# encouraging because all of these were generated as sequences of 30 moves\n",
    "# so we've found significantly faster paths back to the origin for almost all moves that we find a path to the origin \n",
    "sum(test_df['num_moves_Q_learning_needs']<20)/sum(test_df['num_moves_Q_learning_needs']!=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = test_df[test_df['num_moves_Q_learning_needs']!=100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16089\\AppData\\Local\\Temp\\ipykernel_14152\\2721632581.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['first_move_by_Q_learning'] = filtered_df.apply(first_matrix_to_apply, axis=1)\n"
     ]
    }
   ],
   "source": [
    "def first_matrix_to_apply(cur_row):\n",
    "    outputs = [0, 0, 0, 0]\n",
    "    cur_matrix = np.array([\n",
    "        [int(cur_row['val1']), int(cur_row['val2'])], \n",
    "        [int(cur_row['val3']), int(cur_row['val4'])]\n",
    "        ])\n",
    "    outputs[0] = Q_table[matrix_to_tuple(cur_matrix@ A)]\n",
    "    outputs[1] = Q_table[matrix_to_tuple(cur_matrix@ B)]\n",
    "    outputs[2] = Q_table[matrix_to_tuple(cur_matrix@ C)]\n",
    "    outputs[3] = Q_table[matrix_to_tuple(cur_matrix@ D)]\n",
    "    return np.argmax(outputs)\n",
    "\n",
    "filtered_df['first_move_by_Q_learning'] = filtered_df.apply(first_matrix_to_apply, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val1</th>\n",
       "      <th>val2</th>\n",
       "      <th>val3</th>\n",
       "      <th>val4</th>\n",
       "      <th>num_moves_Q_learning_needs</th>\n",
       "      <th>first_move_by_Q_learning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-43.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>-707.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-159.0</td>\n",
       "      <td>574.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>361.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>37.0</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>-172.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-43.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>-98.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9986</th>\n",
       "      <td>-27.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-23.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9989</th>\n",
       "      <td>25.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9991</th>\n",
       "      <td>65.0</td>\n",
       "      <td>-142.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>-83.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>29.0</td>\n",
       "      <td>-16.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>-75.0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>-223.0</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>504.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2764 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       val1   val2   val3   val4  num_moves_Q_learning_needs  \\\n",
       "7     -43.0  200.0  152.0 -707.0                          10   \n",
       "9       1.0    0.0    4.0    1.0                           2   \n",
       "10   -159.0  574.0 -100.0  361.0                           9   \n",
       "11     37.0  -20.0 -172.0   93.0                           8   \n",
       "16    -43.0   18.0  -98.0   41.0                           6   \n",
       "...     ...    ...    ...    ...                         ...   \n",
       "9986  -27.0   62.0   10.0  -23.0                           6   \n",
       "9989   25.0   -4.0   -6.0    1.0                           5   \n",
       "9991   65.0 -142.0   38.0  -83.0                           8   \n",
       "9996   29.0  -16.0  136.0  -75.0                           8   \n",
       "9998 -223.0  -50.0  504.0  113.0                           9   \n",
       "\n",
       "      first_move_by_Q_learning  \n",
       "7                            0  \n",
       "9                            3  \n",
       "10                           0  \n",
       "11                           1  \n",
       "16                           1  \n",
       "...                        ...  \n",
       "9986                         0  \n",
       "9989                         1  \n",
       "9991                         0  \n",
       "9996                         1  \n",
       "9998                         3  \n",
       "\n",
       "[2764 rows x 6 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "bound = int(filtered_df.shape[0] * 0.6)\n",
    "plus_one = bound+1\n",
    "train = filtered_df.iloc[1:bound]\n",
    "test = filtered_df.iloc[plus_one:filtered_df.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Q_value(row):\n",
    "    return Q_table[(int(row['val1']), \n",
    "    int(row['val2']), \n",
    "    int(row['val3']),\n",
    "    int(row['val4'])\n",
    "    )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"../Data_Generation/Data_files/subset_train_rows_SL2Z_Q_learn.csv\", index=False)\n",
    "test.to_csv(\"../Data_Generation/Data_files/subset_test_rows_SL2Z_Q_learn.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mod_2_is_identity(test_tuple):\n",
    "    assert len(test_tuple)==4\n",
    "    return (test_tuple[0] % 2 == 1 and \n",
    "            test_tuple[1] % 2 == 0 and \n",
    "            test_tuple[2] % 2 == 0 and \n",
    "            test_tuple[3] % 2 == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_2_is_identity([1, 2, 1, 1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
