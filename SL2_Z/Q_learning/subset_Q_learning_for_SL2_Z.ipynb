{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_to_tuple(matrix):\n",
    "    return (matrix[0][0], matrix[0][1], \n",
    "            matrix[1][0], matrix[1][1]) \n",
    "\n",
    "# index 12 according to alex's paper. Is it congruent to identity mod 2 or mod 4?\n",
    "# can generate with any coset I want by starting at a representative from each coset and see if we get our way back to it\n",
    "A = np.array([[1, 2], [0, 1]])\n",
    "B = np.array([[1, 0], [2, 1]])\n",
    "\n",
    "# elements on the diagonal are 1 mod 4. \n",
    "# elements not on the diagonal are 0 mod 2. \n",
    "\n",
    "# C is the inverse of A\n",
    "# D is the inverse of B\n",
    "C = np.linalg.inv(A)\n",
    "D = np.linalg.inv(B)\n",
    "\n",
    "identity = np.array([[1, 0], [0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def epsilon_greedy_search(Epsilon, qtable, state):\n",
    "    if (random.random() < Epsilon):\n",
    "        # 0 is 'apply matrix A', 1 is 'apply matrix B'\n",
    "        # 2 is 'apply matrix C', 3 is 'apply matrix D'\n",
    "        return random.choice([0, 1, 2, 3])\n",
    "    else:\n",
    "        # get the best move for the current state\n",
    "        return best_move_for_a_state(Q_table=qtable, state=state)\n",
    "    \n",
    "# I would like to return the best move for a given state\n",
    "def best_move_for_a_state(Q_table, state):\n",
    "    # vals = Q_table[(state[0][1], state[0][2], state[1][2])]\n",
    "\n",
    "    apply_A = state @ A\n",
    "    apply_B = state @ B\n",
    "    apply_C = state @ C\n",
    "    apply_D = state @ D\n",
    "\n",
    "    vals = [0, 0, 0, 0]\n",
    "    vals[0] = Q_table[matrix_to_tuple(apply_A)]\n",
    "    vals[1] = Q_table[matrix_to_tuple(apply_B)]\n",
    "    vals[2] = Q_table[matrix_to_tuple(apply_C)]\n",
    "    vals[3] = Q_table[matrix_to_tuple(apply_D)]\n",
    "\n",
    "    # if we haven't visited this state before, return a random choice of 0, 1, 2, or 3\n",
    "    if vals==[0, 0, 0, 0]:\n",
    "        return random.choice([0, 1, 2, 3])\n",
    "    \n",
    "    # if we have visited this state before, return the current best choice\n",
    "    return np.argmax(vals)\n",
    "\n",
    "# over a given state, return the maximum value of the table for that state\n",
    "def max_a_prime(Q_table, state):\n",
    "    apply_A = state @ A\n",
    "    apply_B = state @ B\n",
    "    apply_C = state @ C\n",
    "    apply_D = state @ D\n",
    "\n",
    "    vals = [0, 0, 0, 0]\n",
    "    vals[0] = Q_table[matrix_to_tuple(apply_A)]\n",
    "    vals[1] = Q_table[matrix_to_tuple(apply_B)]\n",
    "    vals[2] = Q_table[matrix_to_tuple(apply_C)]\n",
    "    vals[3] = Q_table[matrix_to_tuple(apply_D)]\n",
    "    \n",
    "    return max(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_reward = 100\n",
    "step_penalty = -1\n",
    "\n",
    "def getReward(matrix):\n",
    "    if (matrix==identity).all():\n",
    "        return max_reward\n",
    "    else:\n",
    "        return step_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Data_Generation/Data_files/sl2_Z.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_step(oldObs, action):\n",
    "    # action is always either 0, 1, 2, or 3\n",
    "    next_state = []\n",
    "    if action==0:\n",
    "        next_state = oldObs @ A\n",
    "    elif action==1:\n",
    "        next_state = oldObs @ B\n",
    "    elif action==2:\n",
    "        next_state = oldObs @ C\n",
    "    else:\n",
    "        next_state = oldObs @ D\n",
    "    curReward = getReward(next_state)\n",
    "    done = curReward==max_reward\n",
    "    return (next_state, curReward, done)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def are_we_done_yet(my_matrix):\n",
    "    return (my_matrix==identity).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mat(mat, index):\n",
    "    if index==0:\n",
    "        return mat @ A\n",
    "    elif index==1:\n",
    "        return mat @ B\n",
    "    elif index==2:\n",
    "        return mat @ C\n",
    "    elif index==3:\n",
    "        return mat @ D\n",
    "    assert(1==2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuple_to_matrix(tuple):\n",
    "    return np.array([[tuple[0], tuple[1]], [tuple[2], tuple[3]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val1</th>\n",
       "      <th>val2</th>\n",
       "      <th>val3</th>\n",
       "      <th>val4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5.0</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9976</th>\n",
       "      <td>-9.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9988</th>\n",
       "      <td>-7.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9989</th>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1660 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      val1  val2  val3  val4\n",
       "4     47.0  10.0  14.0   3.0\n",
       "7     -5.0   2.0   2.0  -1.0\n",
       "9      1.0   0.0  -2.0   1.0\n",
       "22     5.0  -2.0   8.0  -3.0\n",
       "28     5.0 -12.0  -2.0   5.0\n",
       "...    ...   ...   ...   ...\n",
       "9976  -9.0  16.0  -4.0   7.0\n",
       "9988  -7.0  24.0   2.0  -7.0\n",
       "9989   9.0   4.0   2.0   1.0\n",
       "9995   3.0   8.0   4.0  11.0\n",
       "9999  -1.0   2.0   0.0  -1.0\n",
       "\n",
       "[1660 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df['val1'] % 2 == 1) & (df['val2'] % 2 == 0) & (df['val3'] % 2 == 0) & (df['val4'] % 2 == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val1</th>\n",
       "      <th>val2</th>\n",
       "      <th>val3</th>\n",
       "      <th>val4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5.0</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9976</th>\n",
       "      <td>-9.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9988</th>\n",
       "      <td>-7.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9989</th>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1660 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      val1  val2  val3  val4\n",
       "4     47.0  10.0  14.0   3.0\n",
       "7     -5.0   2.0   2.0  -1.0\n",
       "9      1.0   0.0  -2.0   1.0\n",
       "22     5.0  -2.0   8.0  -3.0\n",
       "28     5.0 -12.0  -2.0   5.0\n",
       "...    ...   ...   ...   ...\n",
       "9976  -9.0  16.0  -4.0   7.0\n",
       "9988  -7.0  24.0   2.0  -7.0\n",
       "9989   9.0   4.0   2.0   1.0\n",
       "9995   3.0   8.0   4.0  11.0\n",
       "9999  -1.0   2.0   0.0  -1.0\n",
       "\n",
       "[1660 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_df = df[df['val1'] % 2 == 1]\n",
    "filter_df = filter_df[filter_df['val2'] % 2 == 0]\n",
    "filter_df = filter_df[filter_df['val3'] % 2 == 0]\n",
    "filter_df = filter_df[filter_df['val4'] % 2 == 1]\n",
    "filter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward for the last 100 iterations: -100.0\n",
      "epsilon: 0.989950333757503\n",
      "Average reward for the last 100 iterations: -100.0\n",
      "epsilon: 0.9800996732739187\n",
      "Average reward for the last 100 iterations: -100.0\n",
      "epsilon: 0.9703470333764725\n",
      "Average reward for the last 100 iterations: -100.0\n",
      "epsilon: 0.9606914386955115\n",
      "Average reward for the last 100 iterations: -100.0\n",
      "epsilon: 0.9511319235669539\n",
      "Average reward for the last 100 iterations: -96.05\n",
      "epsilon: 0.9416675319357145\n",
      "Average reward for the last 100 iterations: -98.11\n",
      "epsilon: 0.9322973172600907\n",
      "Average reward for the last 100 iterations: -98.02\n",
      "epsilon: 0.9230203424170932\n",
      "Average reward for the last 100 iterations: -98.11\n",
      "epsilon: 0.9138356796087268\n",
      "Average reward for the last 100 iterations: -100.0\n",
      "epsilon: 0.9047424102692004\n",
      "Average reward for the last 100 iterations: -98.55\n",
      "epsilon: 0.89573962497306\n",
      "Average reward for the last 100 iterations: -100.0\n",
      "epsilon: 0.8868264233442354\n",
      "Average reward for the last 100 iterations: -100.0\n",
      "epsilon: 0.8780019139659949\n",
      "Average reward for the last 100 iterations: -96.09\n",
      "epsilon: 0.8692652142917918\n",
      "Average reward for the last 100 iterations: -96.06\n",
      "epsilon: 0.8606154505570021\n",
      "Average reward for the last 100 iterations: -100.0\n",
      "epsilon: 0.8520517576915366\n",
      "Average reward for the last 100 iterations: -100.0\n",
      "epsilon: 0.8435732792333273\n",
      "Average reward for the last 100 iterations: -94.33\n",
      "epsilon: 0.8351791672426676\n",
      "Average reward for the last 100 iterations: -100.0\n",
      "epsilon: 0.8268685822174137\n",
      "Average reward for the last 100 iterations: -96.22\n",
      "epsilon: 0.8186406930090225\n",
      "Average reward for the last 100 iterations: -98.04\n",
      "epsilon: 0.8104946767394292\n",
      "Average reward for the last 100 iterations: -98.3\n",
      "epsilon: 0.802429718718749\n",
      "Average reward for the last 100 iterations: -96.34\n",
      "epsilon: 0.7944450123638009\n",
      "Average reward for the last 100 iterations: -98.0\n",
      "epsilon: 0.78653975911744\n",
      "Average reward for the last 100 iterations: -94.39\n",
      "epsilon: 0.7787131683686925\n",
      "Average reward for the last 100 iterations: -88.98\n",
      "epsilon: 0.7709644573736867\n",
      "Average reward for the last 100 iterations: -98.08\n",
      "epsilon: 0.763292851177371\n",
      "Average reward for the last 100 iterations: -94.42\n",
      "epsilon: 0.7556975825360077\n",
      "Average reward for the last 100 iterations: -92.92\n",
      "epsilon: 0.7481778918404428\n",
      "Average reward for the last 100 iterations: -98.0\n",
      "epsilon: 0.7407330270401349\n",
      "Average reward for the last 100 iterations: -98.1\n",
      "epsilon: 0.7333622435679438\n",
      "Average reward for the last 100 iterations: -89.21\n",
      "epsilon: 0.7260648042656639\n",
      "Average reward for the last 100 iterations: -92.8\n",
      "epsilon: 0.7188399793103014\n",
      "Average reward for the last 100 iterations: -90.59\n",
      "epsilon: 0.7116870461410829\n",
      "Average reward for the last 100 iterations: -93.47\n",
      "epsilon: 0.7046052893871948\n",
      "Average reward for the last 100 iterations: -93.0\n",
      "epsilon: 0.6975940007962347\n",
      "Average reward for the last 100 iterations: -87.58\n",
      "epsilon: 0.690652479163381\n",
      "Average reward for the last 100 iterations: -94.29\n",
      "epsilon: 0.6837800302612622\n",
      "Average reward for the last 100 iterations: -98.78\n",
      "epsilon: 0.6769759667705286\n",
      "Average reward for the last 100 iterations: -88.56\n",
      "epsilon: 0.6702396082111141\n",
      "Average reward for the last 100 iterations: -90.47\n",
      "epsilon: 0.6635702808741777\n",
      "Average reward for the last 100 iterations: -90.72\n",
      "epsilon: 0.6569673177547274\n",
      "Average reward for the last 100 iterations: -95.65\n",
      "epsilon: 0.6504300584849119\n",
      "Average reward for the last 100 iterations: -85.06\n",
      "epsilon: 0.6439578492679773\n",
      "Average reward for the last 100 iterations: -92.44\n",
      "epsilon: 0.6375500428128791\n",
      "Average reward for the last 100 iterations: -92.34\n",
      "epsilon: 0.6312059982695464\n",
      "Average reward for the last 100 iterations: -91.02\n",
      "epsilon: 0.6249250811647913\n",
      "Average reward for the last 100 iterations: -88.84\n",
      "epsilon: 0.6187066633388536\n",
      "Average reward for the last 100 iterations: -91.67\n",
      "epsilon: 0.6125501228825772\n",
      "Average reward for the last 100 iterations: -93.55\n",
      "epsilon: 0.6064548440752141\n",
      "Average reward for the last 100 iterations: -90.7\n",
      "epsilon: 0.6004202173228442\n",
      "Average reward for the last 100 iterations: -91.51\n",
      "epsilon: 0.5944456390974114\n",
      "Average reward for the last 100 iterations: -93.6\n",
      "epsilon: 0.5885305118763617\n",
      "Average reward for the last 100 iterations: -87.71\n",
      "epsilon: 0.5826742440828869\n",
      "Average reward for the last 100 iterations: -88.76\n",
      "epsilon: 0.576876250026757\n",
      "Average reward for the last 100 iterations: -90.69\n",
      "epsilon: 0.5711359498457492\n",
      "Average reward for the last 100 iterations: -80.85\n",
      "epsilon: 0.5654527694476531\n",
      "Average reward for the last 100 iterations: -89.68\n",
      "epsilon: 0.559826140452854\n",
      "Average reward for the last 100 iterations: -89.12\n",
      "epsilon: 0.5542555001374916\n",
      "Average reward for the last 100 iterations: -93.07\n",
      "epsilon: 0.548740291377179\n",
      "Average reward for the last 100 iterations: -90.88\n",
      "epsilon: 0.5432799625912865\n",
      "Average reward for the last 100 iterations: -91.76\n",
      "epsilon: 0.5378739676877772\n",
      "Average reward for the last 100 iterations: -85.6\n",
      "epsilon: 0.532521766008588\n",
      "Average reward for the last 100 iterations: -94.2\n",
      "epsilon: 0.5272228222755642\n",
      "Average reward for the last 100 iterations: -93.11\n",
      "epsilon: 0.5219766065369207\n",
      "Average reward for the last 100 iterations: -86.72\n",
      "epsilon: 0.5167825941142447\n",
      "Average reward for the last 100 iterations: -85.07\n",
      "epsilon: 0.5116402655500194\n",
      "Average reward for the last 100 iterations: -74.68\n",
      "epsilon: 0.5065491065556748\n",
      "Average reward for the last 100 iterations: -83.79\n",
      "epsilon: 0.5015086079601511\n",
      "Average reward for the last 100 iterations: -85.82\n",
      "epsilon: 0.4965182656589779\n",
      "Average reward for the last 100 iterations: -85.47\n",
      "epsilon: 0.491577580563858\n",
      "Average reward for the last 100 iterations: -85.03\n",
      "epsilon: 0.4866860585527523\n",
      "Average reward for the last 100 iterations: -79.24\n",
      "epsilon: 0.4818432104204629\n",
      "Average reward for the last 100 iterations: -90.54\n",
      "epsilon: 0.4770485518297069\n",
      "Average reward for the last 100 iterations: -94.25\n",
      "epsilon: 0.47230160326267795\n",
      "Average reward for the last 100 iterations: -92.96\n",
      "epsilon: 0.46760188997308916\n",
      "Average reward for the last 100 iterations: -85.36\n",
      "epsilon: 0.4629489419386927\n",
      "Average reward for the last 100 iterations: -79.48\n",
      "epsilon: 0.45834229381427294\n",
      "Average reward for the last 100 iterations: -89.01\n",
      "epsilon: 0.4537814848851073\n",
      "Average reward for the last 100 iterations: -90.44\n",
      "epsilon: 0.4492660590208893\n",
      "Average reward for the last 100 iterations: -88.47\n",
      "epsilon: 0.44479556463011005\n",
      "Average reward for the last 100 iterations: -92.2\n",
      "epsilon: 0.44036955461489585\n",
      "Average reward for the last 100 iterations: -88.62\n",
      "epsilon: 0.4359875863262917\n",
      "Average reward for the last 100 iterations: -92.31\n",
      "epsilon: 0.4316492215199928\n",
      "Average reward for the last 100 iterations: -78.95\n",
      "epsilon: 0.42735402631251446\n",
      "Average reward for the last 100 iterations: -88.38\n",
      "epsilon: 0.4231015711378002\n",
      "Average reward for the last 100 iterations: -84.57\n",
      "epsilon: 0.4188914307042601\n",
      "Average reward for the last 100 iterations: -77.5\n",
      "epsilon: 0.41472318395223523\n",
      "Average reward for the last 100 iterations: -80.64\n",
      "epsilon: 0.4105964140118904\n",
      "Average reward for the last 100 iterations: -84.69\n",
      "epsilon: 0.406510708161521\n",
      "Average reward for the last 100 iterations: -81.07\n",
      "epsilon: 0.4024656577862749\n",
      "Average reward for the last 100 iterations: -86.49\n",
      "epsilon: 0.39846085833728956\n",
      "Average reward for the last 100 iterations: -83.23\n",
      "epsilon: 0.3944959092912299\n",
      "Average reward for the last 100 iterations: -86.31\n",
      "epsilon: 0.39057041411023374\n",
      "Average reward for the last 100 iterations: -85.07\n",
      "epsilon: 0.3866839802022521\n",
      "Average reward for the last 100 iterations: -80.51\n",
      "epsilon: 0.3828362188817869\n",
      "Average reward for the last 100 iterations: -82.96\n",
      "epsilon: 0.3790267453310186\n",
      "Average reward for the last 100 iterations: -86.42\n",
      "epsilon: 0.3752551785613178\n",
      "Average reward for the last 100 iterations: -90.42\n",
      "epsilon: 0.3715211413751451\n",
      "Average reward for the last 100 iterations: -90.34\n",
      "epsilon: 0.3678242603283259\n",
      "Average reward for the last 100 iterations: -86.3\n",
      "epsilon: 0.3641641656927023\n",
      "Average reward for the last 100 iterations: -82.71\n",
      "epsilon: 0.36054049141915495\n",
      "Average reward for the last 100 iterations: -92.67\n",
      "epsilon: 0.3569528751009966\n",
      "Average reward for the last 100 iterations: -84.75\n",
      "epsilon: 0.3534009579377257\n",
      "Average reward for the last 100 iterations: -86.33\n",
      "epsilon: 0.3498843846991425\n",
      "Average reward for the last 100 iterations: -82.53\n",
      "epsilon: 0.34640280368982374\n",
      "Average reward for the last 100 iterations: -84.36\n",
      "epsilon: 0.34295586671394696\n",
      "Average reward for the last 100 iterations: -86.31\n",
      "epsilon: 0.33954322904046974\n",
      "Average reward for the last 100 iterations: -88.33\n",
      "epsilon: 0.33616454936865003\n",
      "Average reward for the last 100 iterations: -82.87\n",
      "epsilon: 0.332819489793915\n",
      "Average reward for the last 100 iterations: -84.49\n",
      "epsilon: 0.32950771577406535\n",
      "Average reward for the last 100 iterations: -84.49\n",
      "epsilon: 0.326228896095818\n",
      "Average reward for the last 100 iterations: -86.61\n",
      "epsilon: 0.32298270284168096\n",
      "Average reward for the last 100 iterations: -88.33\n",
      "epsilon: 0.3197688113571579\n",
      "Average reward for the last 100 iterations: -90.19\n",
      "epsilon: 0.3165869002182801\n",
      "Average reward for the last 100 iterations: -78.51\n",
      "epsilon: 0.3134366511994595\n",
      "Average reward for the last 100 iterations: -88.23\n",
      "epsilon: 0.31031774924166283\n",
      "Average reward for the last 100 iterations: -84.33\n",
      "epsilon: 0.30722988242090343\n",
      "Average reward for the last 100 iterations: -86.43\n",
      "epsilon: 0.3041727419170432\n",
      "Average reward for the last 100 iterations: -92.15\n",
      "epsilon: 0.3011460219829101\n",
      "Average reward for the last 100 iterations: -76.73\n",
      "epsilon: 0.2981494199137175\n",
      "Average reward for the last 100 iterations: -84.71\n",
      "epsilon: 0.29518263601679207\n",
      "Average reward for the last 100 iterations: -84.83\n",
      "epsilon: 0.2922453735816008\n",
      "Average reward for the last 100 iterations: -82.46\n",
      "epsilon: 0.2893373388500768\n",
      "Average reward for the last 100 iterations: -88.42\n",
      "epsilon: 0.28645824098724004\n",
      "Average reward for the last 100 iterations: -82.53\n",
      "epsilon: 0.2836077920521105\n",
      "Average reward for the last 100 iterations: -78.36\n",
      "epsilon: 0.28078570696891186\n",
      "Average reward for the last 100 iterations: -88.3\n",
      "epsilon: 0.27799170349856034\n",
      "Average reward for the last 100 iterations: -84.24\n",
      "epsilon: 0.2752255022104372\n",
      "Average reward for the last 100 iterations: -72.58\n",
      "epsilon: 0.27248682645444433\n",
      "Average reward for the last 100 iterations: -86.39\n",
      "epsilon: 0.2697754023333332\n",
      "Average reward for the last 100 iterations: -74.68\n",
      "epsilon: 0.2670909586753151\n",
      "Average reward for the last 100 iterations: -84.34\n",
      "epsilon: 0.2644332270069402\n",
      "Average reward for the last 100 iterations: -84.44\n",
      "epsilon: 0.26180194152624664\n",
      "Average reward for the last 100 iterations: -80.29\n",
      "epsilon: 0.2591968390761776\n",
      "Average reward for the last 100 iterations: -90.28\n",
      "epsilon: 0.2566176591182636\n",
      "Average reward for the last 100 iterations: -78.52\n",
      "epsilon: 0.2540641437065648\n",
      "Average reward for the last 100 iterations: -90.22\n",
      "epsilon: 0.2515360374618739\n",
      "Average reward for the last 100 iterations: -78.61\n",
      "epsilon: 0.24903308754617642\n",
      "Average reward for the last 100 iterations: -84.38\n",
      "epsilon: 0.24655504363736244\n",
      "Average reward for the last 100 iterations: -76.62\n",
      "epsilon: 0.24410165790419308\n",
      "Average reward for the last 100 iterations: -82.59\n",
      "epsilon: 0.24167268498151395\n",
      "Average reward for the last 100 iterations: -88.35\n",
      "epsilon: 0.2392678819457161\n",
      "Average reward for the last 100 iterations: -82.45\n",
      "epsilon: 0.2368870082904414\n",
      "Average reward for the last 100 iterations: -84.5\n",
      "epsilon: 0.23452982590252897\n",
      "Average reward for the last 100 iterations: -96.07\n",
      "epsilon: 0.2321960990382015\n",
      "Average reward for the last 100 iterations: -88.14\n",
      "epsilon: 0.22988559429948774\n",
      "Average reward for the last 100 iterations: -86.23\n",
      "epsilon: 0.2275980806108809\n",
      "Average reward for the last 100 iterations: -70.87\n",
      "epsilon: 0.2253333291962282\n",
      "Average reward for the last 100 iterations: -94.13\n",
      "epsilon: 0.22309111355585096\n",
      "Average reward for the last 100 iterations: -78.57\n",
      "epsilon: 0.2208712094438921\n",
      "Average reward for the last 100 iterations: -80.56\n",
      "epsilon: 0.2186733948458889\n",
      "Average reward for the last 100 iterations: -84.35\n",
      "epsilon: 0.2164974499565696\n",
      "Average reward for the last 100 iterations: -92.16\n",
      "epsilon: 0.2143431571578701\n",
      "Average reward for the last 100 iterations: -84.36\n",
      "epsilon: 0.21221030099717023\n",
      "Average reward for the last 100 iterations: -84.4\n",
      "epsilon: 0.2100986681657454\n",
      "Average reward for the last 100 iterations: -80.72\n",
      "epsilon: 0.2080080474774341\n",
      "Average reward for the last 100 iterations: -88.47\n",
      "epsilon: 0.20593822984751717\n",
      "Average reward for the last 100 iterations: -92.15\n",
      "epsilon: 0.20388900827180606\n",
      "Average reward for the last 100 iterations: -96.11\n",
      "epsilon: 0.20186017780594118\n",
      "Average reward for the last 100 iterations: -82.45\n",
      "epsilon: 0.19985153554489493\n",
      "Average reward for the last 100 iterations: -78.54\n",
      "epsilon: 0.19786288060267845\n",
      "Average reward for the last 100 iterations: -80.33\n",
      "epsilon: 0.19589401409225174\n",
      "Average reward for the last 100 iterations: -82.33\n",
      "epsilon: 0.19394473910563204\n",
      "Average reward for the last 100 iterations: -76.78\n",
      "epsilon: 0.19201486069420162\n",
      "Average reward for the last 100 iterations: -92.19\n",
      "epsilon: 0.1901041858492102\n",
      "Average reward for the last 100 iterations: -90.19\n",
      "epsilon: 0.1882125234824722\n",
      "Average reward for the last 100 iterations: -84.4\n",
      "epsilon: 0.18633968440725585\n",
      "Average reward for the last 100 iterations: -80.53\n",
      "epsilon: 0.18448548131936265\n",
      "Average reward for the last 100 iterations: -86.33\n",
      "epsilon: 0.1826497287783945\n",
      "Average reward for the last 100 iterations: -80.36\n",
      "epsilon: 0.1808322431892079\n",
      "Average reward for the last 100 iterations: -80.32\n",
      "epsilon: 0.17903284278355266\n",
      "Average reward for the last 100 iterations: -72.58\n",
      "epsilon: 0.17725134760189254\n",
      "Average reward for the last 100 iterations: -80.26\n",
      "epsilon: 0.1754875794754081\n",
      "Average reward for the last 100 iterations: -76.6\n",
      "epsilon: 0.17374136200817725\n",
      "Average reward for the last 100 iterations: -82.3\n",
      "epsilon: 0.17201252055953417\n",
      "Average reward for the last 100 iterations: -86.18\n",
      "epsilon: 0.17030088222660275\n",
      "Average reward for the last 100 iterations: -80.26\n",
      "epsilon: 0.16860627582700524\n",
      "Average reward for the last 100 iterations: -94.12\n",
      "epsilon: 0.16692853188174173\n",
      "Average reward for the last 100 iterations: -86.43\n",
      "epsilon: 0.16526748259824006\n",
      "Average reward for the last 100 iterations: -88.39\n",
      "epsilon: 0.1636229618535754\n",
      "Average reward for the last 100 iterations: -90.15\n",
      "epsilon: 0.16199480517785583\n",
      "Average reward for the last 100 iterations: -92.15\n",
      "epsilon: 0.16038284973777372\n",
      "Average reward for the last 100 iterations: -74.43\n",
      "epsilon: 0.15878693432032062\n",
      "Average reward for the last 100 iterations: -78.31\n",
      "epsilon: 0.1572068993166638\n",
      "Average reward for the last 100 iterations: -80.35\n",
      "epsilon: 0.15564258670618408\n",
      "Average reward for the last 100 iterations: -88.17\n",
      "epsilon: 0.15409384004067206\n",
      "Average reward for the last 100 iterations: -76.48\n",
      "epsilon: 0.15256050442868138\n",
      "Average reward for the last 100 iterations: -80.28\n",
      "epsilon: 0.1510424265200381\n",
      "Average reward for the last 100 iterations: -96.06\n",
      "epsilon: 0.14953945449050376\n",
      "Average reward for the last 100 iterations: -88.24\n",
      "epsilon: 0.1480514380265917\n",
      "Average reward for the last 100 iterations: -84.28\n",
      "epsilon: 0.14657822831053363\n",
      "Average reward for the last 100 iterations: -90.11\n",
      "epsilon: 0.14511967800539669\n",
      "Average reward for the last 100 iterations: -82.16\n",
      "epsilon: 0.14367564124034787\n",
      "Average reward for the last 100 iterations: -82.33\n",
      "epsilon: 0.14224597359606533\n",
      "Average reward for the last 100 iterations: -82.26\n",
      "epsilon: 0.1408305320902949\n",
      "Average reward for the last 100 iterations: -88.2\n",
      "epsilon: 0.13942917516355058\n",
      "Average reward for the last 100 iterations: -70.71\n",
      "epsilon: 0.1380417626649567\n",
      "Average reward for the last 100 iterations: -86.19\n",
      "epsilon: 0.13666815583823175\n",
      "Average reward for the last 100 iterations: -84.29\n",
      "epsilon: 0.13530821730781062\n",
      "Average reward for the last 100 iterations: -80.38\n",
      "epsilon: 0.1339618110651063\n",
      "Average reward for the last 100 iterations: -86.15\n",
      "epsilon: 0.13262880245490677\n",
      "Average reward for the last 100 iterations: -88.1\n",
      "epsilon: 0.13130905816190905\n",
      "Average reward for the last 100 iterations: -80.31\n",
      "epsilon: 0.1300024461973849\n",
      "Average reward for the last 100 iterations: -72.6\n",
      "epsilon: 0.1287088358859816\n",
      "Average reward for the last 100 iterations: -90.29\n",
      "epsilon: 0.12742809785265258\n",
      "Average reward for the last 100 iterations: -88.05\n",
      "epsilon: 0.12616010400971825\n",
      "Average reward for the last 100 iterations: -82.28\n",
      "epsilon: 0.1249047275440563\n",
      "Average reward for the last 100 iterations: -76.51\n",
      "epsilon: 0.12366184290441894\n",
      "Average reward for the last 100 iterations: -84.28\n",
      "epsilon: 0.12243132578887629\n",
      "Average reward for the last 100 iterations: -78.47\n",
      "epsilon: 0.12121305313238478\n",
      "Average reward for the last 100 iterations: -96.1\n",
      "epsilon: 0.1200069030944797\n",
      "Average reward for the last 100 iterations: -84.48\n",
      "epsilon: 0.11881275504708917\n",
      "Average reward for the last 100 iterations: -84.28\n",
      "epsilon: 0.11763048956247056\n",
      "Average reward for the last 100 iterations: -86.17\n",
      "epsilon: 0.11645998840126634\n",
      "Average reward for the last 100 iterations: -76.48\n",
      "epsilon: 0.11530113450067855\n",
      "Average reward for the last 100 iterations: -90.18\n",
      "epsilon: 0.11415381196276177\n",
      "Average reward for the last 100 iterations: -76.31\n",
      "epsilon: 0.11301790604283157\n",
      "Average reward for the last 100 iterations: -82.31\n",
      "epsilon: 0.11189330313798898\n",
      "Average reward for the last 100 iterations: -80.48\n",
      "epsilon: 0.11077989077575923\n",
      "Average reward for the last 100 iterations: -82.36\n",
      "epsilon: 0.10967755760284283\n",
      "Average reward for the last 100 iterations: -82.38\n",
      "epsilon: 0.10858619337397935\n",
      "Average reward for the last 100 iterations: -86.19\n",
      "epsilon: 0.10750568894092176\n",
      "Average reward for the last 100 iterations: -84.26\n",
      "epsilon: 0.10643593624151992\n",
      "Average reward for the last 100 iterations: -90.16\n",
      "epsilon: 0.1053768282889138\n",
      "Average reward for the last 100 iterations: -80.45\n",
      "epsilon: 0.1043282591608334\n",
      "Average reward for the last 100 iterations: -88.16\n",
      "epsilon: 0.10329012398900515\n",
      "Average reward for the last 100 iterations: -78.45\n",
      "epsilon: 0.10226231894866437\n",
      "Average reward for the last 100 iterations: -86.25\n",
      "epsilon: 0.10124474124817139\n",
      "Average reward for the last 100 iterations: -88.28\n",
      "epsilon: 0.10023728911873117\n",
      "Average reward for the last 100 iterations: -74.46\n",
      "epsilon: 0.09923986180421565\n",
      "Average reward for the last 100 iterations: -84.32\n",
      "epsilon: 0.0982523595510868\n",
      "Average reward for the last 100 iterations: -86.18\n",
      "epsilon: 0.09727468359842038\n",
      "Average reward for the last 100 iterations: -82.27\n",
      "epsilon: 0.09630673616802857\n",
      "Average reward for the last 100 iterations: -86.28\n",
      "epsilon: 0.09534842045468113\n",
      "Average reward for the last 100 iterations: -80.44\n",
      "epsilon: 0.09439964061642395\n",
      "Average reward for the last 100 iterations: -80.4\n",
      "epsilon: 0.09346030176499369\n",
      "Average reward for the last 100 iterations: -94.1\n",
      "epsilon: 0.09253030995632805\n",
      "Average reward for the last 100 iterations: -74.63\n",
      "epsilon: 0.09160957218117023\n",
      "Average reward for the last 100 iterations: -90.19\n",
      "epsilon: 0.09069799635576713\n",
      "Average reward for the last 100 iterations: -78.31\n",
      "epsilon: 0.08979549131265965\n",
      "Average reward for the last 100 iterations: -76.5\n",
      "epsilon: 0.08890196679156555\n",
      "Average reward for the last 100 iterations: -86.25\n",
      "epsilon: 0.08801733343035181\n",
      "Average reward for the last 100 iterations: -86.23\n",
      "epsilon: 0.0871415027560978\n",
      "Average reward for the last 100 iterations: -80.4\n",
      "epsilon: 0.0862743871762469\n",
      "Average reward for the last 100 iterations: -84.17\n",
      "epsilon: 0.08541589996984655\n",
      "Average reward for the last 100 iterations: -94.14\n",
      "epsilon: 0.08456595527887494\n",
      "Average reward for the last 100 iterations: -84.42\n",
      "epsilon: 0.08372446809965417\n",
      "Average reward for the last 100 iterations: -90.1\n",
      "epsilon: 0.08289135427434946\n",
      "Average reward for the last 100 iterations: -78.35\n",
      "epsilon: 0.08206653048255198\n",
      "Average reward for the last 100 iterations: -84.32\n",
      "epsilon: 0.08124991423294586\n",
      "Average reward for the last 100 iterations: -80.33\n",
      "epsilon: 0.08044142385505873\n",
      "Average reward for the last 100 iterations: -84.33\n",
      "epsilon: 0.07964097849109329\n",
      "Average reward for the last 100 iterations: -92.13\n",
      "epsilon: 0.07884849808784071\n",
      "Average reward for the last 100 iterations: -90.14\n",
      "epsilon: 0.07806390338867449\n",
      "Average reward for the last 100 iterations: -84.21\n",
      "epsilon: 0.0772871159256243\n",
      "Average reward for the last 100 iterations: -88.29\n",
      "epsilon: 0.07651805801152776\n",
      "Average reward for the last 100 iterations: -82.42\n",
      "epsilon: 0.07575665273226108\n",
      "Average reward for the last 100 iterations: -78.25\n",
      "epsilon: 0.07500282393904703\n",
      "Average reward for the last 100 iterations: -84.29\n",
      "epsilon: 0.0742564962408389\n",
      "Average reward for the last 100 iterations: -86.27\n",
      "epsilon: 0.07351759499678091\n",
      "Average reward for the last 100 iterations: -90.17\n",
      "epsilon: 0.07278604630874305\n",
      "Average reward for the last 100 iterations: -82.31\n",
      "epsilon: 0.07206177701393056\n",
      "Average reward for the last 100 iterations: -80.36\n",
      "epsilon: 0.07134471467756703\n",
      "Average reward for the last 100 iterations: -88.2\n",
      "epsilon: 0.07063478758564987\n",
      "Average reward for the last 100 iterations: -90.22\n",
      "epsilon: 0.06993192473777823\n",
      "Average reward for the last 100 iterations: -86.21\n",
      "epsilon: 0.06923605584005207\n",
      "Average reward for the last 100 iterations: -72.37\n",
      "epsilon: 0.06854711129804245\n",
      "Average reward for the last 100 iterations: -86.22\n",
      "epsilon: 0.0678650222098307\n",
      "Average reward for the last 100 iterations: -86.26\n",
      "epsilon: 0.0671897203591181\n",
      "Average reward for the last 100 iterations: -80.34\n",
      "epsilon: 0.0665211382084031\n",
      "Average reward for the last 100 iterations: -90.09\n",
      "epsilon: 0.06585920889222686\n",
      "Average reward for the last 100 iterations: -84.22\n",
      "epsilon: 0.06520386621048607\n",
      "Average reward for the last 100 iterations: -86.32\n",
      "epsilon: 0.06455504462181237\n",
      "Average reward for the last 100 iterations: -84.32\n",
      "epsilon: 0.06391267923701738\n",
      "Average reward for the last 100 iterations: -90.16\n",
      "epsilon: 0.0632767058126029\n",
      "Average reward for the last 100 iterations: -84.24\n",
      "epsilon: 0.06264706074433596\n",
      "Average reward for the last 100 iterations: -90.23\n",
      "epsilon: 0.06202368106088804\n",
      "Average reward for the last 100 iterations: -92.21\n",
      "epsilon: 0.061406504417536784\n",
      "Average reward for the last 100 iterations: -90.17\n",
      "epsilon: 0.06079546908993113\n",
      "Average reward for the last 100 iterations: -88.18\n",
      "epsilon: 0.060190513967918045\n",
      "Average reward for the last 100 iterations: -94.09\n",
      "epsilon: 0.059591578549431055\n",
      "Average reward for the last 100 iterations: -74.32\n",
      "epsilon: 0.05899860293443916\n",
      "Average reward for the last 100 iterations: -94.09\n",
      "epsilon: 0.05841152781895632\n",
      "Average reward for the last 100 iterations: -84.2\n",
      "epsilon: 0.05783029448911036\n",
      "Average reward for the last 100 iterations: -84.28\n",
      "epsilon: 0.057254844815271044\n",
      "Average reward for the last 100 iterations: -94.11\n",
      "epsilon: 0.056685121246236224\n",
      "Average reward for the last 100 iterations: -82.25\n",
      "epsilon: 0.05612106680347639\n",
      "Average reward for the last 100 iterations: -88.22\n",
      "epsilon: 0.055562625075436065\n",
      "Average reward for the last 100 iterations: -78.32\n",
      "epsilon: 0.0550097402118921\n",
      "Average reward for the last 100 iterations: -84.24\n",
      "epsilon: 0.05446235691836792\n",
      "Average reward for the last 100 iterations: -86.23\n",
      "epsilon: 0.05392042045060362\n",
      "Average reward for the last 100 iterations: -84.31\n",
      "epsilon: 0.053383876609080824\n",
      "Average reward for the last 100 iterations: -88.23\n",
      "epsilon: 0.052852671733602266\n",
      "Average reward for the last 100 iterations: -72.42\n",
      "epsilon: 0.052326752697925104\n",
      "Average reward for the last 100 iterations: -94.07\n",
      "epsilon: 0.051806066904447695\n",
      "Average reward for the last 100 iterations: -80.24\n",
      "epsilon: 0.051290562278949396\n",
      "Average reward for the last 100 iterations: -86.17\n",
      "epsilon: 0.05078018726538244\n",
      "Average reward for the last 100 iterations: -92.12\n",
      "epsilon: 0.050274890820715915\n"
     ]
    }
   ],
   "source": [
    "EPISODES = 30000\n",
    "LEARNING_RATE = .9\n",
    "DISCOUNT_FACTOR = .99\n",
    "EPSILON = 1\n",
    "EPSILON_DECAY = .9999\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# starts with an estimate of zero reward for each state.\n",
    "# adapted from ChatGPT\n",
    "Q_table = defaultdict(lambda: 0)\n",
    "\n",
    "episode_reward_record = deque(maxlen=100)\n",
    "\n",
    "for i in range(EPISODES):\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    # choose a random starting row\n",
    "    # adapted from https://stackoverflow.com/questions/15923826/random-row-selection-in-pandas-dataframe\n",
    "    cur_row = df.sample(1)\n",
    "    obs = np.array([\n",
    "        [int(cur_row['val1']), int(cur_row['val2'])], \n",
    "        [int(cur_row['val3']), int(cur_row['val4'])]\n",
    "        ])\n",
    "    # print(obs)\n",
    "    # assert 1==2\n",
    "    # obs = np.array([[1, 0, 1], [0, -1, -1], [0, 1, 0]])\n",
    "\n",
    "    index = 1\n",
    "\n",
    "    while (not done):\n",
    "        # perform an epsilon greedy action \n",
    "        # Q(s, a) = (1-LEARNING_RATE)Q(s, a) + (LEARNING_RATE)(r + DISCOUNT_FACTOR(max a'(Q(s', a'))))\n",
    "        action = epsilon_greedy_search(Epsilon=EPSILON, qtable=Q_table, state=obs)\n",
    "\n",
    "        oldObs = obs\n",
    "        obs,reward,done = get_next_step(oldObs, action)\n",
    "\n",
    "        # if done:\n",
    "        #     assert(1==2)\n",
    "        \n",
    "        Q_table[matrix_to_tuple(obs)] = (1-LEARNING_RATE) * Q_table[matrix_to_tuple(obs)] + (LEARNING_RATE) * (reward + DISCOUNT_FACTOR * (max_a_prime(Q_table, obs)))\n",
    "\n",
    "        episode_reward += reward # update episode reward\n",
    "\n",
    "        index += 1\n",
    "        # if we take more than 100 steps, end this iteration early (we are probably not making progress)\n",
    "        if index > 100:\n",
    "            done=True\n",
    "\n",
    "    # decay the epsilon\n",
    "    EPSILON *= EPSILON_DECAY\n",
    "\n",
    "    # record the reward for this episode\n",
    "    episode_reward_record.append(episode_reward) \n",
    "\n",
    "    if i%100 ==0 and i>0:\n",
    "        print(\"Average reward for the last 100 iterations: \" + str(sum(list(episode_reward_record))/100))\n",
    "        print(\"epsilon: \" + str(EPSILON) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def access_Q_table(mat):\n",
    "    return Q_table[matrix_to_tuple(mat)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4824.6331155349735\n",
      "4824.607514566448\n",
      "4824.6325879229225\n",
      "4824.63311549693\n",
      "4824.633115557055\n",
      "4824.633108239876\n",
      "4824.633115230087\n"
     ]
    }
   ],
   "source": [
    "print(access_Q_table(A @ A @ A))\n",
    "print(access_Q_table(B @ A @ B))\n",
    "print(access_Q_table(B @ B @ A))\n",
    "print(access_Q_table(C @ C @ C))\n",
    "print(access_Q_table(C @ C @ B))\n",
    "print(access_Q_table(D @ D @ C))\n",
    "print(access_Q_table(D @ D @ D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.801360596445397"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "access_Q_table(np.array([[1, 1], [0, 1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4924.623115562208"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "access_Q_table(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with the other dataframe. \n",
    "test_df = pd.read_csv(\"../Data_Generation/Data_files/sl2_Z_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_to_num_steps(cur_matrix):\n",
    "    index = 1\n",
    "    for i in range(50):\n",
    "        if (cur_matrix==identity).all():\n",
    "            return i\n",
    "        outputs = [0, 0, 0, 0]\n",
    "        outputs[0] = Q_table[matrix_to_tuple(cur_matrix@ A)]\n",
    "        outputs[1] = Q_table[matrix_to_tuple(cur_matrix@ B)]\n",
    "        outputs[2] = Q_table[matrix_to_tuple(cur_matrix@ C)]\n",
    "        outputs[3] = Q_table[matrix_to_tuple(cur_matrix@ D)]\n",
    "        index = np.argmax(outputs)\n",
    "        if index==0:\n",
    "            cur_matrix = cur_matrix @ A\n",
    "        elif index==1:\n",
    "            cur_matrix = cur_matrix @ B\n",
    "        elif index==2:\n",
    "            cur_matrix = cur_matrix @ C\n",
    "        elif index==3:\n",
    "            cur_matrix = cur_matrix @ D\n",
    "    return 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_Q_learning(cur_row):\n",
    "    cur_matrix = np.array([\n",
    "        [int(cur_row['val1']), int(cur_row['val2'])], \n",
    "        [int(cur_row['val3']), int(cur_row['val4'])]\n",
    "        ])\n",
    "    return matrix_to_num_steps(cur_matrix)\n",
    "\n",
    "test_df['num_moves_Q_learning_needs'] = test_df.apply(test_Q_learning, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The proportion of starting positions in the test dataset that we can find a route to the origin that's <50 steps: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0753"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The proportion of starting positions in the test dataset that we can find a route to the origin that's <50 steps: \")\n",
    "sum(test_df['num_moves_Q_learning_needs']!=100)/test_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of these, the proportion of times where we learned a path that was < 20 moves: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Of these, the proportion of times where we learned a path that was < 20 moves: \")\n",
    "# encouraging because all of these were generated as sequences of 30 moves\n",
    "# so we've found significantly faster paths back to the origin for almost all moves that we find a path to the origin \n",
    "sum(test_df['num_moves_Q_learning_needs']<20)/sum(test_df['num_moves_Q_learning_needs']!=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = test_df[test_df['num_moves_Q_learning_needs']!=100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16089\\AppData\\Local\\Temp\\ipykernel_12204\\2721632581.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['first_move_by_Q_learning'] = filtered_df.apply(first_matrix_to_apply, axis=1)\n"
     ]
    }
   ],
   "source": [
    "def first_matrix_to_apply(cur_row):\n",
    "    outputs = [0, 0, 0, 0]\n",
    "    cur_matrix = np.array([\n",
    "        [int(cur_row['val1']), int(cur_row['val2'])], \n",
    "        [int(cur_row['val3']), int(cur_row['val4'])]\n",
    "        ])\n",
    "    outputs[0] = Q_table[matrix_to_tuple(cur_matrix@ A)]\n",
    "    outputs[1] = Q_table[matrix_to_tuple(cur_matrix@ B)]\n",
    "    outputs[2] = Q_table[matrix_to_tuple(cur_matrix@ C)]\n",
    "    outputs[3] = Q_table[matrix_to_tuple(cur_matrix@ D)]\n",
    "    return np.argmax(outputs)\n",
    "\n",
    "filtered_df['first_move_by_Q_learning'] = filtered_df.apply(first_matrix_to_apply, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val1</th>\n",
       "      <th>val2</th>\n",
       "      <th>val3</th>\n",
       "      <th>val4</th>\n",
       "      <th>num_moves_Q_learning_needs</th>\n",
       "      <th>first_move_by_Q_learning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9941</th>\n",
       "      <td>-7.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9943</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9958</th>\n",
       "      <td>-3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9971</th>\n",
       "      <td>5.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>753 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      val1  val2  val3  val4  num_moves_Q_learning_needs  \\\n",
       "31     1.0   2.0  -2.0  -3.0                           2   \n",
       "32     1.0   4.0  -2.0  -7.0                           3   \n",
       "40     1.0   0.0  -2.0   1.0                           1   \n",
       "67     1.0   4.0   0.0   1.0                           2   \n",
       "90     1.0   2.0   2.0   5.0                           2   \n",
       "...    ...   ...   ...   ...                         ...   \n",
       "9941  -7.0  -4.0  16.0   9.0                           4   \n",
       "9943   1.0   0.0   0.0   1.0                           0   \n",
       "9958  -3.0   2.0  -2.0   1.0                           2   \n",
       "9971   5.0  -8.0   2.0  -3.0                           3   \n",
       "9994   1.0  -4.0   0.0   1.0                           2   \n",
       "\n",
       "      first_move_by_Q_learning  \n",
       "31                           2  \n",
       "32                           2  \n",
       "40                           1  \n",
       "67                           2  \n",
       "90                           2  \n",
       "...                        ...  \n",
       "9941                         3  \n",
       "9943                         0  \n",
       "9958                         1  \n",
       "9971                         0  \n",
       "9994                         0  \n",
       "\n",
       "[753 rows x 6 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_df = filtered_df.drop('num_moves_Q_learning_needs', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "bound = int(filtered_df.shape[0] * 0.6)\n",
    "plus_one = bound+1\n",
    "train = filtered_df.iloc[1:bound]\n",
    "test = filtered_df.iloc[plus_one:filtered_df.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Q_value(row):\n",
    "    return Q_table[(int(row['val1']), \n",
    "    int(row['val2']), \n",
    "    int(row['val3']),\n",
    "    int(row['val4'])\n",
    "    )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"../Data_Generation/Data_files/train_rows_SL2Z_Q_learn.csv\", index=False)\n",
    "test.to_csv(\"../Data_Generation/Data_files/test_rows_SL2Z_Q_learn.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mod_2_is_identity(test_tuple):\n",
    "    assert len(test_tuple)==4\n",
    "    return (test_tuple[0] % 2 == 1 and \n",
    "            test_tuple[1] % 2 == 0 and \n",
    "            test_tuple[2] % 2 == 0 and \n",
    "            test_tuple[3] % 2 == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_2_is_identity([1, 2, 1, 1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
