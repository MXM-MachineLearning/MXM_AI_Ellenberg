{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from util import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_reward = 100\n",
    "step_penalty = -1\n",
    "\n",
    "def getReward(matrix):\n",
    "    if (matrix==np.identity(2)).all():\n",
    "        return max_reward\n",
    "    else:\n",
    "        return step_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '../Data_Generation/Data_files/'\n",
    "base_fp = base_dir + 'sl2_Z.csv'\n",
    "generators = k_sl2z_gen\n",
    "\n",
    "df = pd.read_csv(base_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t9/gxb6q8zj21dff090q066td740000gn/T/ipykernel_53871/2642381623.py:22: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
      "  [int(cur_row['val1']), int(cur_row['val2'])],\n",
      "/var/folders/t9/gxb6q8zj21dff090q066td740000gn/T/ipykernel_53871/2642381623.py:23: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
      "  [int(cur_row['val3']), int(cur_row['val4'])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 500\teps: 0.951\tAvg rwd: -93.714\n",
      "Episode 1000\teps: 0.905\tAvg rwd: -92.858\n",
      "Episode 1500\teps: 0.861\tAvg rwd: -84.416\n",
      "Episode 2000\teps: 0.819\tAvg rwd: -66.762\n",
      "Episode 2500\teps: 0.779\tAvg rwd: -52.202\n",
      "Episode 3000\teps: 0.741\tAvg rwd: -37.990\n",
      "Episode 3500\teps: 0.705\tAvg rwd: -10.186\n",
      "Episode 4000\teps: 0.670\tAvg rwd: 11.762\n",
      "Episode 4500\teps: 0.638\tAvg rwd: 22.352\n",
      "Episode 5000\teps: 0.607\tAvg rwd: 44.602\n",
      "Episode 5500\teps: 0.577\tAvg rwd: 58.050\n",
      "Episode 6000\teps: 0.549\tAvg rwd: 61.030\n",
      "Episode 6500\teps: 0.522\tAvg rwd: 64.400\n",
      "Episode 7000\teps: 0.497\tAvg rwd: 70.676\n",
      "Episode 7500\teps: 0.472\tAvg rwd: 70.752\n",
      "Episode 8000\teps: 0.449\tAvg rwd: 72.726\n",
      "Episode 8500\teps: 0.427\tAvg rwd: 75.504\n",
      "Episode 9000\teps: 0.407\tAvg rwd: 75.994\n",
      "Episode 9500\teps: 0.387\tAvg rwd: 77.792\n",
      "Episode 10000\teps: 0.368\tAvg rwd: 76.474\n",
      "Episode 10500\teps: 0.350\tAvg rwd: 78.032\n",
      "Episode 11000\teps: 0.333\tAvg rwd: 81.162\n",
      "Episode 11500\teps: 0.317\tAvg rwd: 82.904\n",
      "Episode 12000\teps: 0.301\tAvg rwd: 81.620\n",
      "Episode 12500\teps: 0.286\tAvg rwd: 83.374\n",
      "Episode 13000\teps: 0.273\tAvg rwd: 81.672\n",
      "Episode 13500\teps: 0.259\tAvg rwd: 84.108\n",
      "Episode 14000\teps: 0.247\tAvg rwd: 86.438\n",
      "Episode 14500\teps: 0.235\tAvg rwd: 83.046\n",
      "Episode 15000\teps: 0.223\tAvg rwd: 83.572\n",
      "Episode 15500\teps: 0.212\tAvg rwd: 83.406\n",
      "Episode 16000\teps: 0.202\tAvg rwd: 86.688\n",
      "Episode 16500\teps: 0.192\tAvg rwd: 85.788\n",
      "Episode 17000\teps: 0.183\tAvg rwd: 84.950\n",
      "Episode 17500\teps: 0.174\tAvg rwd: 86.838\n",
      "Episode 18000\teps: 0.165\tAvg rwd: 87.696\n",
      "Episode 18500\teps: 0.157\tAvg rwd: 86.968\n",
      "Episode 19000\teps: 0.150\tAvg rwd: 88.710\n",
      "Episode 19500\teps: 0.142\tAvg rwd: 86.562\n",
      "Episode 20000\teps: 0.135\tAvg rwd: 89.434\n",
      "Episode 20500\teps: 0.129\tAvg rwd: 87.966\n",
      "Episode 21000\teps: 0.122\tAvg rwd: 87.552\n",
      "Episode 21500\teps: 0.116\tAvg rwd: 82.652\n",
      "Episode 22000\teps: 0.111\tAvg rwd: 85.992\n",
      "Episode 22500\teps: 0.105\tAvg rwd: 88.028\n",
      "Episode 23000\teps: 0.100\tAvg rwd: 84.996\n",
      "Episode 23500\teps: 0.095\tAvg rwd: 88.276\n",
      "Episode 24000\teps: 0.091\tAvg rwd: 88.700\n",
      "Episode 24500\teps: 0.086\tAvg rwd: 89.554\n",
      "Episode 25000\teps: 0.082\tAvg rwd: 88.072\n",
      "Episode 25500\teps: 0.078\tAvg rwd: 86.238\n",
      "Episode 26000\teps: 0.074\tAvg rwd: 87.484\n",
      "Episode 26500\teps: 0.071\tAvg rwd: 86.996\n",
      "Episode 27000\teps: 0.067\tAvg rwd: 87.420\n",
      "Episode 27500\teps: 0.064\tAvg rwd: 86.230\n",
      "Episode 28000\teps: 0.061\tAvg rwd: 87.540\n",
      "Episode 28500\teps: 0.058\tAvg rwd: 88.456\n",
      "Episode 29000\teps: 0.055\tAvg rwd: 88.098\n",
      "Episode 29500\teps: 0.052\tAvg rwd: 87.982\n",
      "Episode 30000\teps: 0.050\tAvg rwd: 89.864\n"
     ]
    }
   ],
   "source": [
    "env = TabularQEnv(generators, defaultdict(lambda: 0), getReward, max_reward)\n",
    "\n",
    "EPISODES = 30000\n",
    "LEARNING_RATE = .9\n",
    "DISCOUNT_FACTOR = .99\n",
    "EPSILON = 1\n",
    "EPSILON_DECAY = .9999\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# starts with an estimate of zero reward for each state.\n",
    "# adapted from ChatGPT\n",
    "episode_reward_record = deque(maxlen=500)\n",
    "\n",
    "for i in range(EPISODES):\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    # choose a random starting row\n",
    "    # adapted from https://stackoverflow.com/questions/15923826/random-row-selection-in-pandas-dataframe\n",
    "    cur_row = df.sample(1)\n",
    "    obs = np.array([\n",
    "        [int(cur_row['val1']), int(cur_row['val2'])], \n",
    "        [int(cur_row['val3']), int(cur_row['val4'])]\n",
    "        ])\n",
    "\n",
    "    index = 1\n",
    "\n",
    "    while (not done):\n",
    "        # perform an epsilon greedy action \n",
    "        # Q(s, a) = (1-LEARNING_RATE)Q(s, a) + (LEARNING_RATE)(r + DISCOUNT_FACTOR(max a'(Q(s', a'))))\n",
    "        obs, reward, done = env.step(LEARNING_RATE, DISCOUNT_FACTOR, EPSILON, obs)\n",
    "\n",
    "        episode_reward += reward # update episode reward\n",
    "\n",
    "        index += 1\n",
    "        # if we take more than 100 steps, end this iteration early (we are probably not making progress)\n",
    "        if index > 100:\n",
    "            done=True\n",
    "\n",
    "    # decay the epsilon\n",
    "    EPSILON *= EPSILON_DECAY\n",
    "\n",
    "    # record the reward for this episode\n",
    "    episode_reward_record.append(episode_reward) \n",
    "\n",
    "    if (i+1)%500 ==0 and i>0:\n",
    "        print(\"Episode {i}\\teps: {eps:.3f}\\tAvg rwd: {rwd:.3f}\".format(i=i+1, eps=EPSILON, rwd=sum(list(episode_reward_record))/500))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def access_Q_table(mat):\n",
    "    return env.Q_table[matrix_to_tuple(mat)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fp = base_dir + 'test_rows_SL2Z_Q_learn.csv'\n",
    "train_fp = base_dir + 'train_rows_SL2Z_Q_learn.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with the other dataframe. \n",
    "test_df = pd.read_csv(base_fp)\n",
    "test_df['num_moves_Q_learning_needs'] = test_df.apply(lambda row: env.play(df_row_to_mat(row)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The proportion of starting positions in the test dataset that we can find a route to the origin that's <50 steps: \n",
      "1.0\n",
      "Of these, the proportion of times where we learned a path that was < 20 moves: \n",
      "0.996\n"
     ]
    }
   ],
   "source": [
    "print(\"The proportion of starting positions in the test dataset that we can find a route to the origin that's <50 steps: \")\n",
    "print(sum(test_df['num_moves_Q_learning_needs']<=50)/test_df.shape[0])\n",
    "\n",
    "print(\"Of these, the proportion of times where we learned a path that was < 20 moves: \")\n",
    "# encouraging because all of these were generated as sequences of 30 moves\n",
    "# so we've found significantly faster paths back to the origin for almost all moves that we find a path to the origin \n",
    "print(sum(test_df['num_moves_Q_learning_needs']<20)/sum(test_df['num_moves_Q_learning_needs']<=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_info_states_csv(fname_i, of_train, of_test, Q_env, prop_train=0.7):\n",
    "    \"\"\"\n",
    "    Given CSV with various states and tabular-Q environment trained on a set containing those states, \n",
    "    estimate next best move + number of moves to identity, and append them to the state information.\n",
    "    Then, split that dataset into train/test and write to corresponding CSVs\n",
    "    Args:\n",
    "        fname_i: csv to append to\n",
    "        of_train: where to write final train csv\n",
    "        of_test: where to write final test csv\n",
    "        Q_env: TabularQEnv used to make predictons\n",
    "        prop_train: proportion of data to be used for training\n",
    "    \"\"\"\n",
    "    test_df = pd.read_csv(fname_i)\n",
    "    test_df['num_moves_Q_learning_needs'] = test_df.apply(lambda row: Q_env.play(df_row_to_mat(row)), axis=1)\n",
    "    filtered_df = test_df[test_df['num_moves_Q_learning_needs']!=100]\n",
    "    filtered_df['first_move_by_Q_learning'] = filtered_df.apply(lambda row: Q_env.best_move(df_row_to_mat(row)), axis=1)\n",
    "\n",
    "    print(filtered_df.shape)\n",
    "\n",
    "    bound = int(filtered_df.shape[0] * prop_train)\n",
    "    train = filtered_df.iloc[1:bound]\n",
    "    test = filtered_df.iloc[bound:filtered_df.shape[0]]\n",
    "\n",
    "    train.to_csv(of_train, index=False)\n",
    "    test.to_csv(of_test, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 6)\n"
     ]
    }
   ],
   "source": [
    "append_info_states_csv(base_fp,\n",
    "                       train_fp, \n",
    "                       test_fp,\n",
    "                       env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
