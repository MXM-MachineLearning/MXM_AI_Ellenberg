{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning: this doesn't work. This was from Donald's code that I just changed a few things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(string): \n",
    "    a = np.array([[1, 1, 2],[0, 1, 1],[0,-3,-2]])\n",
    "    b = np.array([[-2, 0, -1],[-5, 1, -1],[3,0,1]])\n",
    "\n",
    "    maxMats = 50\n",
    "    dataPoints = 1000\n",
    "    prevMat = -1\n",
    "    consecutiveA = 0\n",
    "    consecutiveB = 0\n",
    "    mat = np.array([[1,0,0],[0,1,0],[0,0,1]])\n",
    "    \n",
    "\n",
    "\n",
    "    for j in range(dataPoints):\n",
    "        mat = np.array([[1,0,0],[0,1,0],[0,0,1]])\n",
    "        nextMat = random.randint(0,1)\n",
    "        if nextMat == 0: \n",
    "            mat = np.matmul(a, mat)\n",
    "            consecutiveA = consecutiveA + 1 \n",
    "            consecutiveB = 0 \n",
    "            prevMat = 0\n",
    "        elif nextMat == 1:\n",
    "            mat = np.matmul(b, mat)\n",
    "            consecutiveB = consecutiveB + 1\n",
    "            consecutiveA = 0\n",
    "            prevMat = 1\n",
    "        numOfMatrices = random.randint(1, maxMats)\n",
    "        for i in range(numOfMatrices):\n",
    "            nextMat = random.randint(0,1)\n",
    "            if nextMat == 0 and consecutiveA == 2: \n",
    "                map = np.matmul(b, mat)\n",
    "                consecutiveB = consecutiveB + 1\n",
    "                consecutiveA = 0\n",
    "                prevMat = 1\n",
    "            if nextMat == 1 and consecutiveB == 2:\n",
    "                mat = np.matmul(a, mat)\n",
    "                consecutiveA = consecutiveA + 1 \n",
    "                consecutiveB = 0 \n",
    "                prevMat = 0\n",
    "            elif (nextMat == 0):\n",
    "                mat = np.matmul(a, mat)\n",
    "                consecutiveA = consecutiveA + 1 \n",
    "                consecutiveB = 0 \n",
    "                prevMat = 0\n",
    "            elif (nextMat == 1):\n",
    "                mat = np.matmul(b, mat)\n",
    "                consecutiveB = consecutiveB + 1\n",
    "                consecutiveA = 0\n",
    "                prevMat = 1\n",
    "            \n",
    "        if j == 0: \n",
    "            df = pd.DataFrame([[mat[0][0],mat[0][1],mat[0][2], mat[1][0],mat[1][1],mat[1][2],mat[2][0],mat[2][1],mat[2][2], prevMat]], columns = ['00', '01', '02','10','11','12', '20', '21','22',\"last\"])\n",
    "        else: \n",
    "            df = pd.concat([df, pd.DataFrame([[mat[0][0],mat[0][1],mat[0][2], mat[1][0],mat[1][1],mat[1][2],mat[2][0],mat[2][1],mat[2][2], prevMat]],columns = ['00', '01', '02','10','11','12', '20', '21','22',\"last\"])])\n",
    "# mat[0][0] = mat[0][0]  % 7\n",
    "# mat [0][1] = mat[0][1]  % 7\n",
    "# mat[0][2] = mat[0][2]  % 7\n",
    "# mat [1][0] = mat[1][0]  % 7\n",
    "# mat [1][1] = mat[1][1]  % 7\n",
    "# mat [1][1] = mat[1][1]  % 7\n",
    "    df.to_csv(string, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code adapted from ChatGPT\n",
    "\n",
    "# Define your neural network model\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "\n",
    "        self.step1 = nn.Linear(9, 128, bias=True)\n",
    "        self.step2 = nn.ReLU()\n",
    "        self.step3 = nn.Linear(128, 64, bias=True)\n",
    "        self.step4 = nn.ReLU()\n",
    "        self.step5 = nn.Linear(64, 16, bias=True)\n",
    "        self.step6 = nn.ReLU()\n",
    "        self.step7 = nn.Linear(16, 2, bias=True)\n",
    "\n",
    "        # multi-class classification adapted from ChatGPT\n",
    "        self.step8 = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # RUN IT ON A GPU if it exists\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.to(\"cuda\")\n",
    "\n",
    "        x = self.step1(x)\n",
    "        x = self.step2(x)\n",
    "        x = self.step3(x)\n",
    "        x = self.step4(x)\n",
    "        x = self.step5(x)\n",
    "        x = self.step6(x)\n",
    "        x = self.step7(x)\n",
    "        x = self.step8(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def train_model(inputs, desired_outputs, num_epochs=100, learning_rate=0.01, viz_accuracy=False):\n",
    "    # Convert inputs and desired_outputs to PyTorch tensors\n",
    "    inputs = torch.tensor(inputs, dtype=torch.float32)\n",
    "    desired_outputs = torch.tensor(desired_outputs, dtype=torch.float32)\n",
    "    \n",
    "    # Create a DataLoader to handle batching (if needed)\n",
    "    dataset = TensorDataset(inputs, desired_outputs)\n",
    "    dataloader = DataLoader(dataset, batch_size=1000, shuffle=True)  # Adjust batch_size as needed\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = SimpleModel()\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()  # Mean Squared Error loss\n",
    "    # optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "    # criterion = nn.MSELoss()\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.995)\n",
    "    losses = []\n",
    "\n",
    "    inputs = inputs.float()\n",
    "    desired_outputs = desired_outputs.long()\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for batch_inputs, batch_desired_outputs in dataloader:\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            outputs = model(batch_inputs)  # Forward pass\n",
    "\n",
    "            batch_desired_outputs = batch_desired_outputs.long()\n",
    "\n",
    "            loss = criterion(outputs, batch_desired_outputs)  # Compute the loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update the model's parameters\n",
    "            cur_item = loss.item()\n",
    "            total_loss += cur_item\n",
    "\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Print the average loss for this epoch\n",
    "        print(f\"total loss: {total_loss}\")\n",
    "        average_loss = total_loss / len(dataloader)\n",
    "        if viz_accuracy:\n",
    "            losses.append(average_loss)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.4f}')\n",
    "    \n",
    "    if viz_accuracy:\n",
    "        plt.scatter(x=range(1, len(losses)+1), y=losses)\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Average loss\")\n",
    "        plt.show()\n",
    "\n",
    "    # Return the trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>last</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-47</td>\n",
       "      <td>-24</td>\n",
       "      <td>-78</td>\n",
       "      <td>-142</td>\n",
       "      <td>-71</td>\n",
       "      <td>-234</td>\n",
       "      <td>87</td>\n",
       "      <td>45</td>\n",
       "      <td>145</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-27</td>\n",
       "      <td>-25</td>\n",
       "      <td>-57</td>\n",
       "      <td>-77</td>\n",
       "      <td>-71</td>\n",
       "      <td>-162</td>\n",
       "      <td>51</td>\n",
       "      <td>48</td>\n",
       "      <td>109</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>-12</td>\n",
       "      <td>-3</td>\n",
       "      <td>-17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27</td>\n",
       "      <td>25</td>\n",
       "      <td>57</td>\n",
       "      <td>77</td>\n",
       "      <td>71</td>\n",
       "      <td>162</td>\n",
       "      <td>-342</td>\n",
       "      <td>-315</td>\n",
       "      <td>-719</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>963</td>\n",
       "      <td>-263</td>\n",
       "      <td>126</td>\n",
       "      <td>971</td>\n",
       "      <td>-265</td>\n",
       "      <td>127</td>\n",
       "      <td>-7812</td>\n",
       "      <td>2133</td>\n",
       "      <td>-1022</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1</td>\n",
       "      <td>-4</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>288</td>\n",
       "      <td>-1262</td>\n",
       "      <td>-369</td>\n",
       "      <td>284</td>\n",
       "      <td>-1243</td>\n",
       "      <td>-363</td>\n",
       "      <td>-261</td>\n",
       "      <td>1143</td>\n",
       "      <td>334</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>172</td>\n",
       "      <td>41</td>\n",
       "      <td>164</td>\n",
       "      <td>156</td>\n",
       "      <td>22</td>\n",
       "      <td>123</td>\n",
       "      <td>-153</td>\n",
       "      <td>-33</td>\n",
       "      <td>-140</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>-697</td>\n",
       "      <td>-322</td>\n",
       "      <td>-126</td>\n",
       "      <td>-702</td>\n",
       "      <td>-325</td>\n",
       "      <td>-127</td>\n",
       "      <td>636</td>\n",
       "      <td>294</td>\n",
       "      <td>115</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>41007</td>\n",
       "      <td>-7079</td>\n",
       "      <td>10918</td>\n",
       "      <td>40310</td>\n",
       "      <td>-6961</td>\n",
       "      <td>10729</td>\n",
       "      <td>-37116</td>\n",
       "      <td>6408</td>\n",
       "      <td>-9881</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        00    01     02     10    11     12     20    21    22  last\n",
       "0      -47   -24    -78   -142   -71   -234     87    45   145     1\n",
       "1      -27   -25    -57    -77   -71   -162     51    48   109     1\n",
       "2       15     5     22      7     2     10    -12    -3   -17     0\n",
       "3       27    25     57     77    71    162   -342  -315  -719     0\n",
       "4      963  -263    126    971  -265    127  -7812  2133 -1022     0\n",
       "..     ...   ...    ...    ...   ...    ...    ...   ...   ...   ...\n",
       "995      1    -4     -1      0    -2     -1      0     3     1     0\n",
       "996    288 -1262   -369    284 -1243   -363   -261  1143   334     0\n",
       "997    172    41    164    156    22    123   -153   -33  -140     1\n",
       "998   -697  -322   -126   -702  -325   -127    636   294   115     1\n",
       "999  41007 -7079  10918  40310 -6961  10729 -37116  6408 -9881     1\n",
       "\n",
       "[1000 rows x 10 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate('matrices_train.csv')\n",
    "generate('matrices_test.csv')\n",
    "df = pd.read_csv(\"matrices_train.csv\")\n",
    "just_input = df.drop('last', axis=1)\n",
    "\n",
    "# adapted from https://stackoverflow.com/questions/43898035/pandas-combine-column-values-into-a-list-in-a-new-column\n",
    "input_data = np.array(just_input.values.tolist())\n",
    "desired_output = torch.tensor(df['last'].tolist(), dtype=torch.float32).long()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xs/6yx8fshd7_v5fg9sl09cfpdh0000gn/T/ipykernel_16955/363369511.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  desired_outputs = torch.tensor(desired_outputs, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total loss: 0.7363004088401794\n",
      "Epoch [1/500], Loss: 0.7363\n",
      "total loss: 0.7582065463066101\n",
      "Epoch [2/500], Loss: 0.7582\n",
      "total loss: 0.6879680156707764\n",
      "Epoch [3/500], Loss: 0.6880\n",
      "total loss: 0.6778313517570496\n",
      "Epoch [4/500], Loss: 0.6778\n",
      "total loss: 0.6684020161628723\n",
      "Epoch [5/500], Loss: 0.6684\n",
      "total loss: 0.667506217956543\n",
      "Epoch [6/500], Loss: 0.6675\n",
      "total loss: 0.6763600707054138\n",
      "Epoch [7/500], Loss: 0.6764\n",
      "total loss: 0.673717737197876\n",
      "Epoch [8/500], Loss: 0.6737\n",
      "total loss: 0.6928824186325073\n",
      "Epoch [9/500], Loss: 0.6929\n",
      "total loss: 0.6994746923446655\n",
      "Epoch [10/500], Loss: 0.6995\n",
      "total loss: 0.6922014951705933\n",
      "Epoch [11/500], Loss: 0.6922\n",
      "total loss: 0.6644048094749451\n",
      "Epoch [12/500], Loss: 0.6644\n",
      "total loss: 0.6601322889328003\n",
      "Epoch [13/500], Loss: 0.6601\n",
      "total loss: 0.705714225769043\n",
      "Epoch [14/500], Loss: 0.7057\n",
      "total loss: 0.7157435417175293\n",
      "Epoch [15/500], Loss: 0.7157\n",
      "total loss: 0.7202609181404114\n",
      "Epoch [16/500], Loss: 0.7203\n",
      "total loss: 0.7284219264984131\n",
      "Epoch [17/500], Loss: 0.7284\n",
      "total loss: 0.7210081815719604\n",
      "Epoch [18/500], Loss: 0.7210\n",
      "total loss: 0.7167009115219116\n",
      "Epoch [19/500], Loss: 0.7167\n",
      "total loss: 0.7165337204933167\n",
      "Epoch [20/500], Loss: 0.7165\n",
      "total loss: 0.7115280032157898\n",
      "Epoch [21/500], Loss: 0.7115\n",
      "total loss: 0.708414614200592\n",
      "Epoch [22/500], Loss: 0.7084\n",
      "total loss: 0.7054510712623596\n",
      "Epoch [23/500], Loss: 0.7055\n",
      "total loss: 0.7074289917945862\n",
      "Epoch [24/500], Loss: 0.7074\n",
      "total loss: 0.7002344727516174\n",
      "Epoch [25/500], Loss: 0.7002\n",
      "total loss: 0.7069604396820068\n",
      "Epoch [26/500], Loss: 0.7070\n",
      "total loss: 0.7087754011154175\n",
      "Epoch [27/500], Loss: 0.7088\n",
      "total loss: 0.710675060749054\n",
      "Epoch [28/500], Loss: 0.7107\n",
      "total loss: 0.7081153988838196\n",
      "Epoch [29/500], Loss: 0.7081\n",
      "total loss: 0.7058083415031433\n",
      "Epoch [30/500], Loss: 0.7058\n",
      "total loss: 0.6947180032730103\n",
      "Epoch [31/500], Loss: 0.6947\n",
      "total loss: 0.6914421319961548\n",
      "Epoch [32/500], Loss: 0.6914\n",
      "total loss: 0.6849442720413208\n",
      "Epoch [33/500], Loss: 0.6849\n",
      "total loss: 0.6810340285301208\n",
      "Epoch [34/500], Loss: 0.6810\n",
      "total loss: 0.6806550025939941\n",
      "Epoch [35/500], Loss: 0.6807\n",
      "total loss: 0.6777170896530151\n",
      "Epoch [36/500], Loss: 0.6777\n",
      "total loss: 0.6828727126121521\n",
      "Epoch [37/500], Loss: 0.6829\n",
      "total loss: 0.6875941753387451\n",
      "Epoch [38/500], Loss: 0.6876\n",
      "total loss: 0.6897342801094055\n",
      "Epoch [39/500], Loss: 0.6897\n",
      "total loss: 0.6897446513175964\n",
      "Epoch [40/500], Loss: 0.6897\n",
      "total loss: 0.6846897006034851\n",
      "Epoch [41/500], Loss: 0.6847\n",
      "total loss: 0.6845639944076538\n",
      "Epoch [42/500], Loss: 0.6846\n",
      "total loss: 0.6841467618942261\n",
      "Epoch [43/500], Loss: 0.6841\n",
      "total loss: 0.6842092275619507\n",
      "Epoch [44/500], Loss: 0.6842\n",
      "total loss: 0.6797679662704468\n",
      "Epoch [45/500], Loss: 0.6798\n",
      "total loss: 0.6801002025604248\n",
      "Epoch [46/500], Loss: 0.6801\n",
      "total loss: 0.6801584959030151\n",
      "Epoch [47/500], Loss: 0.6802\n",
      "total loss: 0.6888406276702881\n",
      "Epoch [48/500], Loss: 0.6888\n",
      "total loss: 0.6852267980575562\n",
      "Epoch [49/500], Loss: 0.6852\n",
      "total loss: 0.6822952628135681\n",
      "Epoch [50/500], Loss: 0.6823\n",
      "total loss: 0.6904619336128235\n",
      "Epoch [51/500], Loss: 0.6905\n",
      "total loss: 0.6992147564888\n",
      "Epoch [52/500], Loss: 0.6992\n",
      "total loss: 0.6998277306556702\n",
      "Epoch [53/500], Loss: 0.6998\n",
      "total loss: 0.6999412178993225\n",
      "Epoch [54/500], Loss: 0.6999\n",
      "total loss: 0.6893407106399536\n",
      "Epoch [55/500], Loss: 0.6893\n",
      "total loss: 0.6728833913803101\n",
      "Epoch [56/500], Loss: 0.6729\n",
      "total loss: 0.6737081408500671\n",
      "Epoch [57/500], Loss: 0.6737\n",
      "total loss: 0.6724463105201721\n",
      "Epoch [58/500], Loss: 0.6724\n",
      "total loss: 0.6680065989494324\n",
      "Epoch [59/500], Loss: 0.6680\n",
      "total loss: 0.6660256385803223\n",
      "Epoch [60/500], Loss: 0.6660\n",
      "total loss: 0.6671925783157349\n",
      "Epoch [61/500], Loss: 0.6672\n",
      "total loss: 0.6660833954811096\n",
      "Epoch [62/500], Loss: 0.6661\n",
      "total loss: 0.6647722125053406\n",
      "Epoch [63/500], Loss: 0.6648\n",
      "total loss: 0.6640167236328125\n",
      "Epoch [64/500], Loss: 0.6640\n",
      "total loss: 0.6630436778068542\n",
      "Epoch [65/500], Loss: 0.6630\n",
      "total loss: 0.6630989909172058\n",
      "Epoch [66/500], Loss: 0.6631\n",
      "total loss: 0.6616334915161133\n",
      "Epoch [67/500], Loss: 0.6616\n",
      "total loss: 0.6615487337112427\n",
      "Epoch [68/500], Loss: 0.6615\n",
      "total loss: 0.6614916920661926\n",
      "Epoch [69/500], Loss: 0.6615\n",
      "total loss: 0.6614518165588379\n",
      "Epoch [70/500], Loss: 0.6615\n",
      "total loss: 0.66400146484375\n",
      "Epoch [71/500], Loss: 0.6640\n",
      "total loss: 0.6616072058677673\n",
      "Epoch [72/500], Loss: 0.6616\n",
      "total loss: 0.6628887057304382\n",
      "Epoch [73/500], Loss: 0.6629\n",
      "total loss: 0.6623322367668152\n",
      "Epoch [74/500], Loss: 0.6623\n",
      "total loss: 0.6632634401321411\n",
      "Epoch [75/500], Loss: 0.6633\n",
      "total loss: 0.6624033451080322\n",
      "Epoch [76/500], Loss: 0.6624\n",
      "total loss: 0.6633548736572266\n",
      "Epoch [77/500], Loss: 0.6634\n",
      "total loss: 0.6645758748054504\n",
      "Epoch [78/500], Loss: 0.6646\n",
      "total loss: 0.6635355949401855\n",
      "Epoch [79/500], Loss: 0.6635\n",
      "total loss: 0.6635379791259766\n",
      "Epoch [80/500], Loss: 0.6635\n",
      "total loss: 0.663521409034729\n",
      "Epoch [81/500], Loss: 0.6635\n",
      "total loss: 0.6634841561317444\n",
      "Epoch [82/500], Loss: 0.6635\n",
      "total loss: 0.6634228229522705\n",
      "Epoch [83/500], Loss: 0.6634\n",
      "total loss: 0.6633408665657043\n",
      "Epoch [84/500], Loss: 0.6633\n",
      "total loss: 0.664272665977478\n",
      "Epoch [85/500], Loss: 0.6643\n",
      "total loss: 0.6631726026535034\n",
      "Epoch [86/500], Loss: 0.6632\n",
      "total loss: 0.6630571484565735\n",
      "Epoch [87/500], Loss: 0.6631\n",
      "total loss: 0.6631617546081543\n",
      "Epoch [88/500], Loss: 0.6632\n",
      "total loss: 0.6631829142570496\n",
      "Epoch [89/500], Loss: 0.6632\n",
      "total loss: 0.6632033586502075\n",
      "Epoch [90/500], Loss: 0.6632\n",
      "total loss: 0.6631654500961304\n",
      "Epoch [91/500], Loss: 0.6632\n",
      "total loss: 0.6631069183349609\n",
      "Epoch [92/500], Loss: 0.6631\n",
      "total loss: 0.6630066633224487\n",
      "Epoch [93/500], Loss: 0.6630\n",
      "total loss: 0.6629013419151306\n",
      "Epoch [94/500], Loss: 0.6629\n",
      "total loss: 0.6629064083099365\n",
      "Epoch [95/500], Loss: 0.6629\n",
      "total loss: 0.6626787185668945\n",
      "Epoch [96/500], Loss: 0.6627\n",
      "total loss: 0.6619352698326111\n",
      "Epoch [97/500], Loss: 0.6619\n",
      "total loss: 0.6619580984115601\n",
      "Epoch [98/500], Loss: 0.6620\n",
      "total loss: 0.6617990732192993\n",
      "Epoch [99/500], Loss: 0.6618\n",
      "total loss: 0.6618268489837646\n",
      "Epoch [100/500], Loss: 0.6618\n",
      "total loss: 0.6617100834846497\n",
      "Epoch [101/500], Loss: 0.6617\n",
      "total loss: 0.6627480983734131\n",
      "Epoch [102/500], Loss: 0.6627\n",
      "total loss: 0.6626573801040649\n",
      "Epoch [103/500], Loss: 0.6627\n",
      "total loss: 0.6625959873199463\n",
      "Epoch [104/500], Loss: 0.6626\n",
      "total loss: 0.6625670194625854\n",
      "Epoch [105/500], Loss: 0.6626\n",
      "total loss: 0.6624481081962585\n",
      "Epoch [106/500], Loss: 0.6624\n",
      "total loss: 0.6624448299407959\n",
      "Epoch [107/500], Loss: 0.6624\n",
      "total loss: 0.6623337864875793\n",
      "Epoch [108/500], Loss: 0.6623\n",
      "total loss: 0.6613200902938843\n",
      "Epoch [109/500], Loss: 0.6613\n",
      "total loss: 0.661180317401886\n",
      "Epoch [110/500], Loss: 0.6612\n",
      "total loss: 0.661130964756012\n",
      "Epoch [111/500], Loss: 0.6611\n",
      "total loss: 0.6609955430030823\n",
      "Epoch [112/500], Loss: 0.6610\n",
      "total loss: 0.6609436869621277\n",
      "Epoch [113/500], Loss: 0.6609\n",
      "total loss: 0.6608017683029175\n",
      "Epoch [114/500], Loss: 0.6608\n",
      "total loss: 0.6607571840286255\n",
      "Epoch [115/500], Loss: 0.6608\n",
      "total loss: 0.6606422066688538\n",
      "Epoch [116/500], Loss: 0.6606\n",
      "total loss: 0.6606363654136658\n",
      "Epoch [117/500], Loss: 0.6606\n",
      "total loss: 0.6605881452560425\n",
      "Epoch [118/500], Loss: 0.6606\n",
      "total loss: 0.6595373749732971\n",
      "Epoch [119/500], Loss: 0.6595\n",
      "total loss: 0.6595101356506348\n",
      "Epoch [120/500], Loss: 0.6595\n",
      "total loss: 0.6594836711883545\n",
      "Epoch [121/500], Loss: 0.6595\n",
      "total loss: 0.6594577431678772\n",
      "Epoch [122/500], Loss: 0.6595\n",
      "total loss: 0.659450888633728\n",
      "Epoch [123/500], Loss: 0.6595\n",
      "total loss: 0.6594243049621582\n",
      "Epoch [124/500], Loss: 0.6594\n",
      "total loss: 0.6574290990829468\n",
      "Epoch [125/500], Loss: 0.6574\n",
      "total loss: 0.6573918461799622\n",
      "Epoch [126/500], Loss: 0.6574\n",
      "total loss: 0.660393476486206\n",
      "Epoch [127/500], Loss: 0.6604\n",
      "total loss: 0.6575039029121399\n",
      "Epoch [128/500], Loss: 0.6575\n",
      "total loss: 0.656558096408844\n",
      "Epoch [129/500], Loss: 0.6566\n",
      "total loss: 0.6564345359802246\n",
      "Epoch [130/500], Loss: 0.6564\n",
      "total loss: 0.6585788130760193\n",
      "Epoch [131/500], Loss: 0.6586\n",
      "total loss: 0.6575925350189209\n",
      "Epoch [132/500], Loss: 0.6576\n",
      "total loss: 0.6607517600059509\n",
      "Epoch [133/500], Loss: 0.6608\n",
      "total loss: 0.6609290838241577\n",
      "Epoch [134/500], Loss: 0.6609\n",
      "total loss: 0.660146951675415\n",
      "Epoch [135/500], Loss: 0.6601\n",
      "total loss: 0.6598283648490906\n",
      "Epoch [136/500], Loss: 0.6598\n",
      "total loss: 0.6606287956237793\n",
      "Epoch [137/500], Loss: 0.6606\n",
      "total loss: 0.6593901515007019\n",
      "Epoch [138/500], Loss: 0.6594\n",
      "total loss: 0.6586204171180725\n",
      "Epoch [139/500], Loss: 0.6586\n",
      "total loss: 0.6597203612327576\n",
      "Epoch [140/500], Loss: 0.6597\n",
      "total loss: 0.6587854027748108\n",
      "Epoch [141/500], Loss: 0.6588\n",
      "total loss: 0.6588258743286133\n",
      "Epoch [142/500], Loss: 0.6588\n",
      "total loss: 0.6608570218086243\n",
      "Epoch [143/500], Loss: 0.6609\n",
      "total loss: 0.6598769426345825\n",
      "Epoch [144/500], Loss: 0.6599\n",
      "total loss: 0.6598891019821167\n",
      "Epoch [145/500], Loss: 0.6599\n",
      "total loss: 0.6598949432373047\n",
      "Epoch [146/500], Loss: 0.6599\n",
      "total loss: 0.6598961353302002\n",
      "Epoch [147/500], Loss: 0.6599\n",
      "total loss: 0.6598921418190002\n",
      "Epoch [148/500], Loss: 0.6599\n",
      "total loss: 0.659880518913269\n",
      "Epoch [149/500], Loss: 0.6599\n",
      "total loss: 0.659862756729126\n",
      "Epoch [150/500], Loss: 0.6599\n",
      "total loss: 0.6598410606384277\n",
      "Epoch [151/500], Loss: 0.6598\n",
      "total loss: 0.6598161458969116\n",
      "Epoch [152/500], Loss: 0.6598\n",
      "total loss: 0.6597886681556702\n",
      "Epoch [153/500], Loss: 0.6598\n",
      "total loss: 0.6597607135772705\n",
      "Epoch [154/500], Loss: 0.6598\n",
      "total loss: 0.659732460975647\n",
      "Epoch [155/500], Loss: 0.6597\n",
      "total loss: 0.6597058176994324\n",
      "Epoch [156/500], Loss: 0.6597\n",
      "total loss: 0.6596828699111938\n",
      "Epoch [157/500], Loss: 0.6597\n",
      "total loss: 0.6596651673316956\n",
      "Epoch [158/500], Loss: 0.6597\n",
      "total loss: 0.6596542000770569\n",
      "Epoch [159/500], Loss: 0.6597\n",
      "total loss: 0.6596477031707764\n",
      "Epoch [160/500], Loss: 0.6596\n",
      "total loss: 0.6596435904502869\n",
      "Epoch [161/500], Loss: 0.6596\n",
      "total loss: 0.6596397757530212\n",
      "Epoch [162/500], Loss: 0.6596\n",
      "total loss: 0.6596353650093079\n",
      "Epoch [163/500], Loss: 0.6596\n",
      "total loss: 0.6596288084983826\n",
      "Epoch [164/500], Loss: 0.6596\n",
      "total loss: 0.6596194505691528\n",
      "Epoch [165/500], Loss: 0.6596\n",
      "total loss: 0.6596080660820007\n",
      "Epoch [166/500], Loss: 0.6596\n",
      "total loss: 0.659595251083374\n",
      "Epoch [167/500], Loss: 0.6596\n",
      "total loss: 0.6595818400382996\n",
      "Epoch [168/500], Loss: 0.6596\n",
      "total loss: 0.6595685482025146\n",
      "Epoch [169/500], Loss: 0.6596\n",
      "total loss: 0.6595555543899536\n",
      "Epoch [170/500], Loss: 0.6596\n",
      "total loss: 0.6595433354377747\n",
      "Epoch [171/500], Loss: 0.6595\n",
      "total loss: 0.6595321297645569\n",
      "Epoch [172/500], Loss: 0.6595\n",
      "total loss: 0.6595216393470764\n",
      "Epoch [173/500], Loss: 0.6595\n",
      "total loss: 0.6595116853713989\n",
      "Epoch [174/500], Loss: 0.6595\n",
      "total loss: 0.6594768166542053\n",
      "Epoch [175/500], Loss: 0.6595\n",
      "total loss: 0.6604502201080322\n",
      "Epoch [176/500], Loss: 0.6605\n",
      "total loss: 0.6592791080474854\n",
      "Epoch [177/500], Loss: 0.6593\n",
      "total loss: 0.6602150201797485\n",
      "Epoch [178/500], Loss: 0.6602\n",
      "total loss: 0.6596105694770813\n",
      "Epoch [179/500], Loss: 0.6596\n",
      "total loss: 0.6586848497390747\n",
      "Epoch [180/500], Loss: 0.6587\n",
      "total loss: 0.659701406955719\n",
      "Epoch [181/500], Loss: 0.6597\n",
      "total loss: 0.6596586108207703\n",
      "Epoch [182/500], Loss: 0.6597\n",
      "total loss: 0.6589977145195007\n",
      "Epoch [183/500], Loss: 0.6590\n",
      "total loss: 0.6567417979240417\n",
      "Epoch [184/500], Loss: 0.6567\n",
      "total loss: 0.6560513973236084\n",
      "Epoch [185/500], Loss: 0.6561\n",
      "total loss: 0.6567083597183228\n",
      "Epoch [186/500], Loss: 0.6567\n",
      "total loss: 0.6562782526016235\n",
      "Epoch [187/500], Loss: 0.6563\n",
      "total loss: 0.6549406051635742\n",
      "Epoch [188/500], Loss: 0.6549\n",
      "total loss: 0.655182421207428\n",
      "Epoch [189/500], Loss: 0.6552\n",
      "total loss: 0.6519989371299744\n",
      "Epoch [190/500], Loss: 0.6520\n",
      "total loss: 0.6485379934310913\n",
      "Epoch [191/500], Loss: 0.6485\n",
      "total loss: 0.6134782433509827\n",
      "Epoch [192/500], Loss: 0.6135\n",
      "total loss: 0.5635652542114258\n",
      "Epoch [193/500], Loss: 0.5636\n",
      "total loss: 0.5591058731079102\n",
      "Epoch [194/500], Loss: 0.5591\n",
      "total loss: 0.5734421014785767\n",
      "Epoch [195/500], Loss: 0.5734\n",
      "total loss: 0.5929387807846069\n",
      "Epoch [196/500], Loss: 0.5929\n",
      "total loss: 0.6047660708427429\n",
      "Epoch [197/500], Loss: 0.6048\n",
      "total loss: 0.6102654933929443\n",
      "Epoch [198/500], Loss: 0.6103\n",
      "total loss: 0.6114363670349121\n",
      "Epoch [199/500], Loss: 0.6114\n",
      "total loss: 0.6088289022445679\n",
      "Epoch [200/500], Loss: 0.6088\n",
      "total loss: 0.6063787937164307\n",
      "Epoch [201/500], Loss: 0.6064\n",
      "total loss: 0.6067099571228027\n",
      "Epoch [202/500], Loss: 0.6067\n",
      "total loss: 0.6039804220199585\n",
      "Epoch [203/500], Loss: 0.6040\n",
      "total loss: 0.6058761477470398\n",
      "Epoch [204/500], Loss: 0.6059\n",
      "total loss: 0.6129652261734009\n",
      "Epoch [205/500], Loss: 0.6130\n",
      "total loss: 0.6181744933128357\n",
      "Epoch [206/500], Loss: 0.6182\n",
      "total loss: 0.6180594563484192\n",
      "Epoch [207/500], Loss: 0.6181\n",
      "total loss: 0.6162492036819458\n",
      "Epoch [208/500], Loss: 0.6162\n",
      "total loss: 0.6137133836746216\n",
      "Epoch [209/500], Loss: 0.6137\n",
      "total loss: 0.6134347915649414\n",
      "Epoch [210/500], Loss: 0.6134\n",
      "total loss: 0.6107184886932373\n",
      "Epoch [211/500], Loss: 0.6107\n",
      "total loss: 0.6070632934570312\n",
      "Epoch [212/500], Loss: 0.6071\n",
      "total loss: 0.6013262271881104\n",
      "Epoch [213/500], Loss: 0.6013\n",
      "total loss: 0.5970483422279358\n",
      "Epoch [214/500], Loss: 0.5970\n",
      "total loss: 0.5945863127708435\n",
      "Epoch [215/500], Loss: 0.5946\n",
      "total loss: 0.5870428085327148\n",
      "Epoch [216/500], Loss: 0.5870\n",
      "total loss: 0.5825200080871582\n",
      "Epoch [217/500], Loss: 0.5825\n",
      "total loss: 0.5817359685897827\n",
      "Epoch [218/500], Loss: 0.5817\n",
      "total loss: 0.589968204498291\n",
      "Epoch [219/500], Loss: 0.5900\n",
      "total loss: 0.5962957739830017\n",
      "Epoch [220/500], Loss: 0.5963\n",
      "total loss: 0.5993268489837646\n",
      "Epoch [221/500], Loss: 0.5993\n",
      "total loss: 0.5942084193229675\n",
      "Epoch [222/500], Loss: 0.5942\n",
      "total loss: 0.5919763445854187\n",
      "Epoch [223/500], Loss: 0.5920\n",
      "total loss: 0.5879509449005127\n",
      "Epoch [224/500], Loss: 0.5880\n",
      "total loss: 0.5822171568870544\n",
      "Epoch [225/500], Loss: 0.5822\n",
      "total loss: 0.5788231492042542\n",
      "Epoch [226/500], Loss: 0.5788\n",
      "total loss: 0.5781210064888\n",
      "Epoch [227/500], Loss: 0.5781\n",
      "total loss: 0.5782285332679749\n",
      "Epoch [228/500], Loss: 0.5782\n",
      "total loss: 0.5786696672439575\n",
      "Epoch [229/500], Loss: 0.5787\n",
      "total loss: 0.5803017616271973\n",
      "Epoch [230/500], Loss: 0.5803\n",
      "total loss: 0.5818074941635132\n",
      "Epoch [231/500], Loss: 0.5818\n",
      "total loss: 0.5830518007278442\n",
      "Epoch [232/500], Loss: 0.5831\n",
      "total loss: 0.584259033203125\n",
      "Epoch [233/500], Loss: 0.5843\n",
      "total loss: 0.5838121771812439\n",
      "Epoch [234/500], Loss: 0.5838\n",
      "total loss: 0.5826752781867981\n",
      "Epoch [235/500], Loss: 0.5827\n",
      "total loss: 0.57684326171875\n",
      "Epoch [236/500], Loss: 0.5768\n",
      "total loss: 0.576467752456665\n",
      "Epoch [237/500], Loss: 0.5765\n",
      "total loss: 0.5743072032928467\n",
      "Epoch [238/500], Loss: 0.5743\n",
      "total loss: 0.5731635689735413\n",
      "Epoch [239/500], Loss: 0.5732\n",
      "total loss: 0.5706286430358887\n",
      "Epoch [240/500], Loss: 0.5706\n",
      "total loss: 0.5676904320716858\n",
      "Epoch [241/500], Loss: 0.5677\n",
      "total loss: 0.5613539814949036\n",
      "Epoch [242/500], Loss: 0.5614\n",
      "total loss: 0.5616376399993896\n",
      "Epoch [243/500], Loss: 0.5616\n",
      "total loss: 0.5621734857559204\n",
      "Epoch [244/500], Loss: 0.5622\n",
      "total loss: 0.5611302256584167\n",
      "Epoch [245/500], Loss: 0.5611\n",
      "total loss: 0.5606064200401306\n",
      "Epoch [246/500], Loss: 0.5606\n",
      "total loss: 0.5601677894592285\n",
      "Epoch [247/500], Loss: 0.5602\n",
      "total loss: 0.5612649917602539\n",
      "Epoch [248/500], Loss: 0.5613\n",
      "total loss: 0.55669105052948\n",
      "Epoch [249/500], Loss: 0.5567\n",
      "total loss: 0.5550370812416077\n",
      "Epoch [250/500], Loss: 0.5550\n",
      "total loss: 0.5519383549690247\n",
      "Epoch [251/500], Loss: 0.5519\n",
      "total loss: 0.5490297675132751\n",
      "Epoch [252/500], Loss: 0.5490\n",
      "total loss: 0.5491169691085815\n",
      "Epoch [253/500], Loss: 0.5491\n",
      "total loss: 0.5492590069770813\n",
      "Epoch [254/500], Loss: 0.5493\n",
      "total loss: 0.5504025220870972\n",
      "Epoch [255/500], Loss: 0.5504\n",
      "total loss: 0.5505051612854004\n",
      "Epoch [256/500], Loss: 0.5505\n",
      "total loss: 0.5504221320152283\n",
      "Epoch [257/500], Loss: 0.5504\n",
      "total loss: 0.5503216981887817\n",
      "Epoch [258/500], Loss: 0.5503\n",
      "total loss: 0.5492067337036133\n",
      "Epoch [259/500], Loss: 0.5492\n",
      "total loss: 0.5488307476043701\n",
      "Epoch [260/500], Loss: 0.5488\n",
      "total loss: 0.548505961894989\n",
      "Epoch [261/500], Loss: 0.5485\n",
      "total loss: 0.5482877492904663\n",
      "Epoch [262/500], Loss: 0.5483\n",
      "total loss: 0.5479735136032104\n",
      "Epoch [263/500], Loss: 0.5480\n",
      "total loss: 0.5477754473686218\n",
      "Epoch [264/500], Loss: 0.5478\n",
      "total loss: 0.5466120839118958\n",
      "Epoch [265/500], Loss: 0.5466\n",
      "total loss: 0.5445225238800049\n",
      "Epoch [266/500], Loss: 0.5445\n",
      "total loss: 0.5438705086708069\n",
      "Epoch [267/500], Loss: 0.5439\n",
      "total loss: 0.5486161112785339\n",
      "Epoch [268/500], Loss: 0.5486\n",
      "total loss: 0.5495611429214478\n",
      "Epoch [269/500], Loss: 0.5496\n",
      "total loss: 0.5505737066268921\n",
      "Epoch [270/500], Loss: 0.5506\n",
      "total loss: 0.5516009330749512\n",
      "Epoch [271/500], Loss: 0.5516\n",
      "total loss: 0.5516467690467834\n",
      "Epoch [272/500], Loss: 0.5516\n",
      "total loss: 0.5517198443412781\n",
      "Epoch [273/500], Loss: 0.5517\n",
      "total loss: 0.5534917116165161\n",
      "Epoch [274/500], Loss: 0.5535\n",
      "total loss: 0.5530450940132141\n",
      "Epoch [275/500], Loss: 0.5530\n",
      "total loss: 0.5535849928855896\n",
      "Epoch [276/500], Loss: 0.5536\n",
      "total loss: 0.5535712242126465\n",
      "Epoch [277/500], Loss: 0.5536\n",
      "total loss: 0.5531141757965088\n",
      "Epoch [278/500], Loss: 0.5531\n",
      "total loss: 0.5530523657798767\n",
      "Epoch [279/500], Loss: 0.5531\n",
      "total loss: 0.5504660606384277\n",
      "Epoch [280/500], Loss: 0.5505\n",
      "total loss: 0.5503737330436707\n",
      "Epoch [281/500], Loss: 0.5504\n",
      "total loss: 0.5513150691986084\n",
      "Epoch [282/500], Loss: 0.5513\n",
      "total loss: 0.5512620210647583\n",
      "Epoch [283/500], Loss: 0.5513\n",
      "total loss: 0.5522142052650452\n",
      "Epoch [284/500], Loss: 0.5522\n",
      "total loss: 0.5499555468559265\n",
      "Epoch [285/500], Loss: 0.5500\n",
      "total loss: 0.5481522083282471\n",
      "Epoch [286/500], Loss: 0.5482\n",
      "total loss: 0.5480738282203674\n",
      "Epoch [287/500], Loss: 0.5481\n",
      "total loss: 0.5469327569007874\n",
      "Epoch [288/500], Loss: 0.5469\n",
      "total loss: 0.5455507636070251\n",
      "Epoch [289/500], Loss: 0.5456\n",
      "total loss: 0.5450977087020874\n",
      "Epoch [290/500], Loss: 0.5451\n",
      "total loss: 0.5431188941001892\n",
      "Epoch [291/500], Loss: 0.5431\n",
      "total loss: 0.5432063341140747\n",
      "Epoch [292/500], Loss: 0.5432\n",
      "total loss: 0.5432069897651672\n",
      "Epoch [293/500], Loss: 0.5432\n",
      "total loss: 0.5431447625160217\n",
      "Epoch [294/500], Loss: 0.5431\n",
      "total loss: 0.5423227548599243\n",
      "Epoch [295/500], Loss: 0.5423\n",
      "total loss: 0.5423976182937622\n",
      "Epoch [296/500], Loss: 0.5424\n",
      "total loss: 0.5449637174606323\n",
      "Epoch [297/500], Loss: 0.5450\n",
      "total loss: 0.5431706309318542\n",
      "Epoch [298/500], Loss: 0.5432\n",
      "total loss: 0.5420471429824829\n",
      "Epoch [299/500], Loss: 0.5420\n",
      "total loss: 0.5419760942459106\n",
      "Epoch [300/500], Loss: 0.5420\n",
      "total loss: 0.5429103970527649\n",
      "Epoch [301/500], Loss: 0.5429\n",
      "total loss: 0.5428262948989868\n",
      "Epoch [302/500], Loss: 0.5428\n",
      "total loss: 0.5426086187362671\n",
      "Epoch [303/500], Loss: 0.5426\n",
      "total loss: 0.542104959487915\n",
      "Epoch [304/500], Loss: 0.5421\n",
      "total loss: 0.541903018951416\n",
      "Epoch [305/500], Loss: 0.5419\n",
      "total loss: 0.5418519377708435\n",
      "Epoch [306/500], Loss: 0.5419\n",
      "total loss: 0.5408321022987366\n",
      "Epoch [307/500], Loss: 0.5408\n",
      "total loss: 0.5398524403572083\n",
      "Epoch [308/500], Loss: 0.5399\n",
      "total loss: 0.5397962331771851\n",
      "Epoch [309/500], Loss: 0.5398\n",
      "total loss: 0.5397873520851135\n",
      "Epoch [310/500], Loss: 0.5398\n",
      "total loss: 0.5388230085372925\n",
      "Epoch [311/500], Loss: 0.5388\n",
      "total loss: 0.5396929383277893\n",
      "Epoch [312/500], Loss: 0.5397\n",
      "total loss: 0.5397621989250183\n",
      "Epoch [313/500], Loss: 0.5398\n",
      "total loss: 0.5397651195526123\n",
      "Epoch [314/500], Loss: 0.5398\n",
      "total loss: 0.5417912006378174\n",
      "Epoch [315/500], Loss: 0.5418\n",
      "total loss: 0.5417876839637756\n",
      "Epoch [316/500], Loss: 0.5418\n",
      "total loss: 0.5418059229850769\n",
      "Epoch [317/500], Loss: 0.5418\n",
      "total loss: 0.5418205857276917\n",
      "Epoch [318/500], Loss: 0.5418\n",
      "total loss: 0.5418302416801453\n",
      "Epoch [319/500], Loss: 0.5418\n",
      "total loss: 0.5418333411216736\n",
      "Epoch [320/500], Loss: 0.5418\n",
      "total loss: 0.5418316125869751\n",
      "Epoch [321/500], Loss: 0.5418\n",
      "total loss: 0.5418285131454468\n",
      "Epoch [322/500], Loss: 0.5418\n",
      "total loss: 0.5418260097503662\n",
      "Epoch [323/500], Loss: 0.5418\n",
      "total loss: 0.5418238043785095\n",
      "Epoch [324/500], Loss: 0.5418\n",
      "total loss: 0.5418198108673096\n",
      "Epoch [325/500], Loss: 0.5418\n",
      "total loss: 0.5418128371238708\n",
      "Epoch [326/500], Loss: 0.5418\n",
      "total loss: 0.5418023467063904\n",
      "Epoch [327/500], Loss: 0.5418\n",
      "total loss: 0.5417895317077637\n",
      "Epoch [328/500], Loss: 0.5418\n",
      "total loss: 0.541775107383728\n",
      "Epoch [329/500], Loss: 0.5418\n",
      "total loss: 0.541759729385376\n",
      "Epoch [330/500], Loss: 0.5418\n",
      "total loss: 0.5417441725730896\n",
      "Epoch [331/500], Loss: 0.5417\n",
      "total loss: 0.542728841304779\n",
      "Epoch [332/500], Loss: 0.5427\n",
      "total loss: 0.5427144169807434\n",
      "Epoch [333/500], Loss: 0.5427\n",
      "total loss: 0.5427010655403137\n",
      "Epoch [334/500], Loss: 0.5427\n",
      "total loss: 0.5426887273788452\n",
      "Epoch [335/500], Loss: 0.5427\n",
      "total loss: 0.5436772108078003\n",
      "Epoch [336/500], Loss: 0.5437\n",
      "total loss: 0.543665885925293\n",
      "Epoch [337/500], Loss: 0.5437\n",
      "total loss: 0.5436540246009827\n",
      "Epoch [338/500], Loss: 0.5437\n",
      "total loss: 0.543631911277771\n",
      "Epoch [339/500], Loss: 0.5436\n",
      "total loss: 0.5433005690574646\n",
      "Epoch [340/500], Loss: 0.5433\n",
      "total loss: 0.5415928959846497\n",
      "Epoch [341/500], Loss: 0.5416\n",
      "total loss: 0.5415792465209961\n",
      "Epoch [342/500], Loss: 0.5416\n",
      "total loss: 0.5405820608139038\n",
      "Epoch [343/500], Loss: 0.5406\n",
      "total loss: 0.5405980348587036\n",
      "Epoch [344/500], Loss: 0.5406\n",
      "total loss: 0.5406200885772705\n",
      "Epoch [345/500], Loss: 0.5406\n",
      "total loss: 0.5403410196304321\n",
      "Epoch [346/500], Loss: 0.5403\n",
      "total loss: 0.5387436747550964\n",
      "Epoch [347/500], Loss: 0.5387\n",
      "total loss: 0.5378852486610413\n",
      "Epoch [348/500], Loss: 0.5379\n",
      "total loss: 0.5380390882492065\n",
      "Epoch [349/500], Loss: 0.5380\n",
      "total loss: 0.538147509098053\n",
      "Epoch [350/500], Loss: 0.5381\n",
      "total loss: 0.5371902585029602\n",
      "Epoch [351/500], Loss: 0.5372\n",
      "total loss: 0.5361720323562622\n",
      "Epoch [352/500], Loss: 0.5362\n",
      "total loss: 0.536101758480072\n",
      "Epoch [353/500], Loss: 0.5361\n",
      "total loss: 0.5359922647476196\n",
      "Epoch [354/500], Loss: 0.5360\n",
      "total loss: 0.5358719229698181\n",
      "Epoch [355/500], Loss: 0.5359\n",
      "total loss: 0.5357769131660461\n",
      "Epoch [356/500], Loss: 0.5358\n",
      "total loss: 0.5367056727409363\n",
      "Epoch [357/500], Loss: 0.5367\n",
      "total loss: 0.5366677045822144\n",
      "Epoch [358/500], Loss: 0.5367\n",
      "total loss: 0.5366467237472534\n",
      "Epoch [359/500], Loss: 0.5366\n",
      "total loss: 0.5376386642456055\n",
      "Epoch [360/500], Loss: 0.5376\n",
      "total loss: 0.5376344919204712\n",
      "Epoch [361/500], Loss: 0.5376\n",
      "total loss: 0.5376322865486145\n",
      "Epoch [362/500], Loss: 0.5376\n",
      "total loss: 0.5386314988136292\n",
      "Epoch [363/500], Loss: 0.5386\n",
      "total loss: 0.5386313199996948\n",
      "Epoch [364/500], Loss: 0.5386\n",
      "total loss: 0.5386311411857605\n",
      "Epoch [365/500], Loss: 0.5386\n",
      "total loss: 0.5396303534507751\n",
      "Epoch [366/500], Loss: 0.5396\n",
      "total loss: 0.5396291613578796\n",
      "Epoch [367/500], Loss: 0.5396\n",
      "total loss: 0.5396276712417603\n",
      "Epoch [368/500], Loss: 0.5396\n",
      "total loss: 0.5396257042884827\n",
      "Epoch [369/500], Loss: 0.5396\n",
      "total loss: 0.5396236181259155\n",
      "Epoch [370/500], Loss: 0.5396\n",
      "total loss: 0.5396211743354797\n",
      "Epoch [371/500], Loss: 0.5396\n",
      "total loss: 0.5386189222335815\n",
      "Epoch [372/500], Loss: 0.5386\n",
      "total loss: 0.5386168360710144\n",
      "Epoch [373/500], Loss: 0.5386\n",
      "total loss: 0.5386146306991577\n",
      "Epoch [374/500], Loss: 0.5386\n",
      "total loss: 0.5386120676994324\n",
      "Epoch [375/500], Loss: 0.5386\n",
      "total loss: 0.5386090278625488\n",
      "Epoch [376/500], Loss: 0.5386\n",
      "total loss: 0.538605272769928\n",
      "Epoch [377/500], Loss: 0.5386\n",
      "total loss: 0.5376011729240417\n",
      "Epoch [378/500], Loss: 0.5376\n",
      "total loss: 0.5375969409942627\n",
      "Epoch [379/500], Loss: 0.5376\n",
      "total loss: 0.5375925302505493\n",
      "Epoch [380/500], Loss: 0.5376\n",
      "total loss: 0.5375882387161255\n",
      "Epoch [381/500], Loss: 0.5376\n",
      "total loss: 0.5375843644142151\n",
      "Epoch [382/500], Loss: 0.5376\n",
      "total loss: 0.5375804901123047\n",
      "Epoch [383/500], Loss: 0.5376\n",
      "total loss: 0.5375767946243286\n",
      "Epoch [384/500], Loss: 0.5376\n",
      "total loss: 0.5375730991363525\n",
      "Epoch [385/500], Loss: 0.5376\n",
      "total loss: 0.5375692248344421\n",
      "Epoch [386/500], Loss: 0.5376\n",
      "total loss: 0.5375655293464661\n",
      "Epoch [387/500], Loss: 0.5376\n",
      "total loss: 0.5375617146492004\n",
      "Epoch [388/500], Loss: 0.5376\n",
      "total loss: 0.5375578999519348\n",
      "Epoch [389/500], Loss: 0.5376\n",
      "total loss: 0.5375540256500244\n",
      "Epoch [390/500], Loss: 0.5376\n",
      "total loss: 0.5375502705574036\n",
      "Epoch [391/500], Loss: 0.5376\n",
      "total loss: 0.5375466346740723\n",
      "Epoch [392/500], Loss: 0.5375\n",
      "total loss: 0.5375432372093201\n",
      "Epoch [393/500], Loss: 0.5375\n",
      "total loss: 0.5375398397445679\n",
      "Epoch [394/500], Loss: 0.5375\n",
      "total loss: 0.5375365018844604\n",
      "Epoch [395/500], Loss: 0.5375\n",
      "total loss: 0.5375332236289978\n",
      "Epoch [396/500], Loss: 0.5375\n",
      "total loss: 0.5375297665596008\n",
      "Epoch [397/500], Loss: 0.5375\n",
      "total loss: 0.5375264286994934\n",
      "Epoch [398/500], Loss: 0.5375\n",
      "total loss: 0.5375230312347412\n",
      "Epoch [399/500], Loss: 0.5375\n",
      "total loss: 0.5375196933746338\n",
      "Epoch [400/500], Loss: 0.5375\n",
      "total loss: 0.5375164151191711\n",
      "Epoch [401/500], Loss: 0.5375\n",
      "total loss: 0.5375133156776428\n",
      "Epoch [402/500], Loss: 0.5375\n",
      "total loss: 0.5375102162361145\n",
      "Epoch [403/500], Loss: 0.5375\n",
      "total loss: 0.5375070571899414\n",
      "Epoch [404/500], Loss: 0.5375\n",
      "total loss: 0.5375040173530579\n",
      "Epoch [405/500], Loss: 0.5375\n",
      "total loss: 0.5375009775161743\n",
      "Epoch [406/500], Loss: 0.5375\n",
      "total loss: 0.5374980568885803\n",
      "Epoch [407/500], Loss: 0.5375\n",
      "total loss: 0.5374950170516968\n",
      "Epoch [408/500], Loss: 0.5375\n",
      "total loss: 0.5374919772148132\n",
      "Epoch [409/500], Loss: 0.5375\n",
      "total loss: 0.5374889969825745\n",
      "Epoch [410/500], Loss: 0.5375\n",
      "total loss: 0.5374860763549805\n",
      "Epoch [411/500], Loss: 0.5375\n",
      "total loss: 0.5364832282066345\n",
      "Epoch [412/500], Loss: 0.5365\n",
      "total loss: 0.5364803075790405\n",
      "Epoch [413/500], Loss: 0.5365\n",
      "total loss: 0.5364775657653809\n",
      "Epoch [414/500], Loss: 0.5365\n",
      "total loss: 0.5364747643470764\n",
      "Epoch [415/500], Loss: 0.5365\n",
      "total loss: 0.5364720225334167\n",
      "Epoch [416/500], Loss: 0.5365\n",
      "total loss: 0.5364692807197571\n",
      "Epoch [417/500], Loss: 0.5365\n",
      "total loss: 0.536466658115387\n",
      "Epoch [418/500], Loss: 0.5365\n",
      "total loss: 0.5364639759063721\n",
      "Epoch [419/500], Loss: 0.5365\n",
      "total loss: 0.5364615321159363\n",
      "Epoch [420/500], Loss: 0.5365\n",
      "total loss: 0.536459743976593\n",
      "Epoch [421/500], Loss: 0.5365\n",
      "total loss: 0.5364578366279602\n",
      "Epoch [422/500], Loss: 0.5365\n",
      "total loss: 0.5364560484886169\n",
      "Epoch [423/500], Loss: 0.5365\n",
      "total loss: 0.5364542007446289\n",
      "Epoch [424/500], Loss: 0.5365\n",
      "total loss: 0.5364524722099304\n",
      "Epoch [425/500], Loss: 0.5365\n",
      "total loss: 0.5364506244659424\n",
      "Epoch [426/500], Loss: 0.5365\n",
      "total loss: 0.5364488363265991\n",
      "Epoch [427/500], Loss: 0.5364\n",
      "total loss: 0.5364471077919006\n",
      "Epoch [428/500], Loss: 0.5364\n",
      "total loss: 0.5364453196525574\n",
      "Epoch [429/500], Loss: 0.5364\n",
      "total loss: 0.5364434719085693\n",
      "Epoch [430/500], Loss: 0.5364\n",
      "total loss: 0.5364416837692261\n",
      "Epoch [431/500], Loss: 0.5364\n",
      "total loss: 0.5364399552345276\n",
      "Epoch [432/500], Loss: 0.5364\n",
      "total loss: 0.5364381074905396\n",
      "Epoch [433/500], Loss: 0.5364\n",
      "total loss: 0.5364363789558411\n",
      "Epoch [434/500], Loss: 0.5364\n",
      "total loss: 0.5364346504211426\n",
      "Epoch [435/500], Loss: 0.5364\n",
      "total loss: 0.5364328026771545\n",
      "Epoch [436/500], Loss: 0.5364\n",
      "total loss: 0.5364312529563904\n",
      "Epoch [437/500], Loss: 0.5364\n",
      "total loss: 0.536429762840271\n",
      "Epoch [438/500], Loss: 0.5364\n",
      "total loss: 0.5364282727241516\n",
      "Epoch [439/500], Loss: 0.5364\n",
      "total loss: 0.5364267826080322\n",
      "Epoch [440/500], Loss: 0.5364\n",
      "total loss: 0.5364251732826233\n",
      "Epoch [441/500], Loss: 0.5364\n",
      "total loss: 0.5364236235618591\n",
      "Epoch [442/500], Loss: 0.5364\n",
      "total loss: 0.536422073841095\n",
      "Epoch [443/500], Loss: 0.5364\n",
      "total loss: 0.536420464515686\n",
      "Epoch [444/500], Loss: 0.5364\n",
      "total loss: 0.5364188551902771\n",
      "Epoch [445/500], Loss: 0.5364\n",
      "total loss: 0.5364173650741577\n",
      "Epoch [446/500], Loss: 0.5364\n",
      "total loss: 0.5364157557487488\n",
      "Epoch [447/500], Loss: 0.5364\n",
      "total loss: 0.5364142656326294\n",
      "Epoch [448/500], Loss: 0.5364\n",
      "total loss: 0.5364127159118652\n",
      "Epoch [449/500], Loss: 0.5364\n",
      "total loss: 0.5364111661911011\n",
      "Epoch [450/500], Loss: 0.5364\n",
      "total loss: 0.5364096164703369\n",
      "Epoch [451/500], Loss: 0.5364\n",
      "total loss: 0.5364080667495728\n",
      "Epoch [452/500], Loss: 0.5364\n",
      "total loss: 0.5364065766334534\n",
      "Epoch [453/500], Loss: 0.5364\n",
      "total loss: 0.5364050269126892\n",
      "Epoch [454/500], Loss: 0.5364\n",
      "total loss: 0.5364034175872803\n",
      "Epoch [455/500], Loss: 0.5364\n",
      "total loss: 0.5364019870758057\n",
      "Epoch [456/500], Loss: 0.5364\n",
      "total loss: 0.5364004373550415\n",
      "Epoch [457/500], Loss: 0.5364\n",
      "total loss: 0.5363990068435669\n",
      "Epoch [458/500], Loss: 0.5364\n",
      "total loss: 0.5363975763320923\n",
      "Epoch [459/500], Loss: 0.5364\n",
      "total loss: 0.5373963713645935\n",
      "Epoch [460/500], Loss: 0.5374\n",
      "total loss: 0.5373950004577637\n",
      "Epoch [461/500], Loss: 0.5374\n",
      "total loss: 0.5373937487602234\n",
      "Epoch [462/500], Loss: 0.5374\n",
      "total loss: 0.5373921990394592\n",
      "Epoch [463/500], Loss: 0.5374\n",
      "total loss: 0.5373908877372742\n",
      "Epoch [464/500], Loss: 0.5374\n",
      "total loss: 0.5373894572257996\n",
      "Epoch [465/500], Loss: 0.5374\n",
      "total loss: 0.5373881459236145\n",
      "Epoch [466/500], Loss: 0.5374\n",
      "total loss: 0.5373867750167847\n",
      "Epoch [467/500], Loss: 0.5374\n",
      "total loss: 0.5373855233192444\n",
      "Epoch [468/500], Loss: 0.5374\n",
      "total loss: 0.5373842120170593\n",
      "Epoch [469/500], Loss: 0.5374\n",
      "total loss: 0.537382960319519\n",
      "Epoch [470/500], Loss: 0.5374\n",
      "total loss: 0.537381649017334\n",
      "Epoch [471/500], Loss: 0.5374\n",
      "total loss: 0.5373803973197937\n",
      "Epoch [472/500], Loss: 0.5374\n",
      "total loss: 0.5373790860176086\n",
      "Epoch [473/500], Loss: 0.5374\n",
      "total loss: 0.5373777747154236\n",
      "Epoch [474/500], Loss: 0.5374\n",
      "total loss: 0.5373764038085938\n",
      "Epoch [475/500], Loss: 0.5374\n",
      "total loss: 0.5373752117156982\n",
      "Epoch [476/500], Loss: 0.5374\n",
      "total loss: 0.5373739004135132\n",
      "Epoch [477/500], Loss: 0.5374\n",
      "total loss: 0.5373726487159729\n",
      "Epoch [478/500], Loss: 0.5374\n",
      "total loss: 0.5373713970184326\n",
      "Epoch [479/500], Loss: 0.5374\n",
      "total loss: 0.5373701453208923\n",
      "Epoch [480/500], Loss: 0.5374\n",
      "total loss: 0.537368893623352\n",
      "Epoch [481/500], Loss: 0.5374\n",
      "total loss: 0.5373677611351013\n",
      "Epoch [482/500], Loss: 0.5374\n",
      "total loss: 0.5373664498329163\n",
      "Epoch [483/500], Loss: 0.5374\n",
      "total loss: 0.5373653173446655\n",
      "Epoch [484/500], Loss: 0.5374\n",
      "total loss: 0.5373640656471252\n",
      "Epoch [485/500], Loss: 0.5374\n",
      "total loss: 0.5373628735542297\n",
      "Epoch [486/500], Loss: 0.5374\n",
      "total loss: 0.5373616218566895\n",
      "Epoch [487/500], Loss: 0.5374\n",
      "total loss: 0.5373604893684387\n",
      "Epoch [488/500], Loss: 0.5374\n",
      "total loss: 0.5373592972755432\n",
      "Epoch [489/500], Loss: 0.5374\n",
      "total loss: 0.5373582243919373\n",
      "Epoch [490/500], Loss: 0.5374\n",
      "total loss: 0.5373570919036865\n",
      "Epoch [491/500], Loss: 0.5374\n",
      "total loss: 0.5373560190200806\n",
      "Epoch [492/500], Loss: 0.5374\n",
      "total loss: 0.5373549461364746\n",
      "Epoch [493/500], Loss: 0.5374\n",
      "total loss: 0.5373538136482239\n",
      "Epoch [494/500], Loss: 0.5374\n",
      "total loss: 0.5373527407646179\n",
      "Epoch [495/500], Loss: 0.5374\n",
      "total loss: 0.537351667881012\n",
      "Epoch [496/500], Loss: 0.5374\n",
      "total loss: 0.537350594997406\n",
      "Epoch [497/500], Loss: 0.5374\n",
      "total loss: 0.5373495221138\n",
      "Epoch [498/500], Loss: 0.5373\n",
      "total loss: 0.5373483896255493\n",
      "Epoch [499/500], Loss: 0.5373\n",
      "total loss: 0.5373473167419434\n",
      "Epoch [500/500], Loss: 0.5373\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Train the model\n",
    "trained_model = train_model(input_data, desired_output, 500, viz_accuracy=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(trained_model, new_data, expected_result):\n",
    "    test_input = torch.tensor(new_data, dtype=torch.float32)\n",
    "\n",
    "    test_output = trained_model(test_input)\n",
    "    print(test_output)\n",
    "\n",
    "    predicted_classes = torch.argmax(test_output, dim=1)\n",
    "    print(predicted_classes)\n",
    "\n",
    "    test_output_compare = torch.tensor(expected_result, dtype=torch.int)\n",
    "    print(test_output_compare)\n",
    "\n",
    "    return float(sum(test_output_compare == predicted_classes)/len(test_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000e+00, 0.0000e+00],\n",
      "        [1.0000e+00, 3.6165e-06],\n",
      "        [1.0000e+00, 0.0000e+00],\n",
      "        ...,\n",
      "        [1.0000e+00, 0.0000e+00],\n",
      "        [1.0000e+00, 3.9777e-10],\n",
      "        [0.0000e+00, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
      "        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
      "        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,\n",
      "        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
      "        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n",
      "        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
      "        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,\n",
      "        0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
      "        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,\n",
      "        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,\n",
      "        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,\n",
      "        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,\n",
      "        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,\n",
      "        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n",
      "        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,\n",
      "        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,\n",
      "        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,\n",
      "        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
      "        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,\n",
      "        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,\n",
      "        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,\n",
      "        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,\n",
      "        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,\n",
      "        0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,\n",
      "        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,\n",
      "        1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,\n",
      "        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,\n",
      "        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,\n",
      "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,\n",
      "        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,\n",
      "        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,\n",
      "        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,\n",
      "        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,\n",
      "        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,\n",
      "        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,\n",
      "        0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n",
      "        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
      "        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,\n",
      "        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n",
      "        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,\n",
      "        0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,\n",
      "        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,\n",
      "        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,\n",
      "        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,\n",
      "        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,\n",
      "        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,\n",
      "        0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,\n",
      "        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,\n",
      "        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1], dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xs/6yx8fshd7_v5fg9sl09cfpdh0000gn/T/ipykernel_16955/2208809294.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_output_compare = torch.tensor(expected_result, dtype=torch.int)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7609999775886536"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"matrices_test.csv\")\n",
    "just_input_test = test_df.drop('last', axis=1)\n",
    "input_data_test = np.array(just_input_test.values.tolist())\n",
    "desired_output = torch.tensor(test_df['last'].tolist(), dtype=torch.float32).long()\n",
    "\n",
    "test_model(trained_model, input_data_test, desired_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
