{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning: this doesn't work. This was from Donald's code that I just changed a few things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(string): \n",
    "    a = np.array([[1, 1, 2],[0, 1, 1],[0,-3,-2]])\n",
    "    b = np.array([[-2, 0, -1],[-5, 1, -1],[3,0,1]])\n",
    "\n",
    "    maxMats = 50\n",
    "    dataPoints = 1000\n",
    "    prevMat = -1\n",
    "    consecutiveA = 0\n",
    "    consecutiveB = 0\n",
    "    mat = np.array([[1,0,0],[0,1,0],[0,0,1]])\n",
    "    \n",
    "\n",
    "\n",
    "    for j in range(dataPoints):\n",
    "        mat = np.array([[1,0,0],[0,1,0],[0,0,1]])\n",
    "        nextMat = random.randint(0,1)\n",
    "        if nextMat == 0: \n",
    "            mat = np.matmul(a, mat)\n",
    "            consecutiveA = consecutiveA + 1 \n",
    "            consecutiveB = 0 \n",
    "            prevMat = 0\n",
    "        elif nextMat == 1:\n",
    "            mat = np.matmul(b, mat)\n",
    "            consecutiveB = consecutiveB + 1\n",
    "            consecutiveA = 0\n",
    "            prevMat = 1\n",
    "        numOfMatrices = random.randint(1, maxMats)\n",
    "        for i in range(numOfMatrices):\n",
    "            nextMat = random.randint(0,1)\n",
    "            if nextMat == 0 and consecutiveA == 2: \n",
    "                map = np.matmul(b, mat)\n",
    "                consecutiveB = consecutiveB + 1\n",
    "                consecutiveA = 0\n",
    "                prevMat = 1\n",
    "            if nextMat == 1 and consecutiveB == 2:\n",
    "                mat = np.matmul(a, mat)\n",
    "                consecutiveA = consecutiveA + 1 \n",
    "                consecutiveB = 0 \n",
    "                prevMat = 0\n",
    "            elif (nextMat == 0):\n",
    "                mat = np.matmul(a, mat)\n",
    "                consecutiveA = consecutiveA + 1 \n",
    "                consecutiveB = 0 \n",
    "                prevMat = 0\n",
    "            elif (nextMat == 1):\n",
    "                mat = np.matmul(b, mat)\n",
    "                consecutiveB = consecutiveB + 1\n",
    "                consecutiveA = 0\n",
    "                prevMat = 1\n",
    "            \n",
    "        if j == 0: \n",
    "            df = pd.DataFrame([[mat[0][0],mat[0][1],mat[0][2], mat[1][0],mat[1][1],mat[1][2],mat[2][0],mat[2][1],mat[2][2], prevMat]], columns = ['00', '01', '02','10','11','12', '20', '21','22',\"last\"])\n",
    "        else: \n",
    "            df = pd.concat([df, pd.DataFrame([[mat[0][0],mat[0][1],mat[0][2], mat[1][0],mat[1][1],mat[1][2],mat[2][0],mat[2][1],mat[2][2], prevMat]],columns = ['00', '01', '02','10','11','12', '20', '21','22',\"last\"])])\n",
    "# mat[0][0] = mat[0][0]  % 7\n",
    "# mat [0][1] = mat[0][1]  % 7\n",
    "# mat[0][2] = mat[0][2]  % 7\n",
    "# mat [1][0] = mat[1][0]  % 7\n",
    "# mat [1][1] = mat[1][1]  % 7\n",
    "# mat [1][1] = mat[1][1]  % 7\n",
    "    df.to_csv(string, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code adapted from ChatGPT\n",
    "\n",
    "# Define your neural network model\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "\n",
    "        self.step1 = nn.Linear(9, 128, bias=True)\n",
    "        self.step2 = nn.ReLU()\n",
    "        self.step3 = nn.Linear(128, 64, bias=True)\n",
    "        self.step4 = nn.ReLU()\n",
    "        self.step5 = nn.Linear(64, 16, bias=True)\n",
    "        self.step6 = nn.ReLU()\n",
    "        self.step7 = nn.Linear(16, 2, bias=True)\n",
    "\n",
    "        # multi-class classification adapted from ChatGPT\n",
    "        self.step8 = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # RUN IT ON A GPU if it exists\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.to(\"cuda\")\n",
    "\n",
    "        x = self.step1(x)\n",
    "        x = self.step2(x)\n",
    "        x = self.step3(x)\n",
    "        x = self.step4(x)\n",
    "        x = self.step5(x)\n",
    "        x = self.step6(x)\n",
    "        x = self.step7(x)\n",
    "        x = self.step8(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def train_model(inputs, desired_outputs, num_epochs=100, learning_rate=0.01, viz_accuracy=False):\n",
    "    # Convert inputs and desired_outputs to PyTorch tensors\n",
    "    inputs = torch.tensor(inputs, dtype=torch.float32)\n",
    "    desired_outputs = torch.tensor(desired_outputs, dtype=torch.float32)\n",
    "    \n",
    "    # Create a DataLoader to handle batching (if needed)\n",
    "    dataset = TensorDataset(inputs, desired_outputs)\n",
    "    dataloader = DataLoader(dataset, batch_size=1000, shuffle=True)  # Adjust batch_size as needed\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = SimpleModel()\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()  # Mean Squared Error loss\n",
    "    # optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "    # criterion = nn.MSELoss()\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.995)\n",
    "    losses = []\n",
    "\n",
    "    inputs = inputs.float()\n",
    "    desired_outputs = desired_outputs.long()\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for batch_inputs, batch_desired_outputs in dataloader:\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            outputs = model(batch_inputs)  # Forward pass\n",
    "\n",
    "            batch_desired_outputs = batch_desired_outputs.long()\n",
    "\n",
    "            loss = criterion(outputs, batch_desired_outputs)  # Compute the loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update the model's parameters\n",
    "            cur_item = loss.item()\n",
    "            total_loss += cur_item\n",
    "\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Print the average loss for this epoch\n",
    "        print(f\"total loss: {total_loss}\")\n",
    "        average_loss = total_loss / len(dataloader)\n",
    "        if viz_accuracy:\n",
    "            losses.append(average_loss)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.4f}')\n",
    "    \n",
    "    if viz_accuracy:\n",
    "        plt.scatter(x=range(1, len(losses)+1), y=losses)\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Average loss\")\n",
    "        plt.show()\n",
    "\n",
    "    # Return the trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>last</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16412</td>\n",
       "      <td>-4360</td>\n",
       "      <td>2087</td>\n",
       "      <td>84997</td>\n",
       "      <td>-22605</td>\n",
       "      <td>10820</td>\n",
       "      <td>-402030</td>\n",
       "      <td>106929</td>\n",
       "      <td>-51182</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-59</td>\n",
       "      <td>-30</td>\n",
       "      <td>-11</td>\n",
       "      <td>-60</td>\n",
       "      <td>-29</td>\n",
       "      <td>-11</td>\n",
       "      <td>54</td>\n",
       "      <td>27</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2502</td>\n",
       "      <td>-1240</td>\n",
       "      <td>-4111</td>\n",
       "      <td>-2462</td>\n",
       "      <td>-1219</td>\n",
       "      <td>-4044</td>\n",
       "      <td>2265</td>\n",
       "      <td>1122</td>\n",
       "      <td>3721</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37</td>\n",
       "      <td>14</td>\n",
       "      <td>56</td>\n",
       "      <td>42</td>\n",
       "      <td>19</td>\n",
       "      <td>67</td>\n",
       "      <td>-36</td>\n",
       "      <td>-15</td>\n",
       "      <td>-56</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>-283378</td>\n",
       "      <td>442741</td>\n",
       "      <td>77288</td>\n",
       "      <td>-805349</td>\n",
       "      <td>1258246</td>\n",
       "      <td>219648</td>\n",
       "      <td>541908</td>\n",
       "      <td>-846678</td>\n",
       "      <td>-147803</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>127</td>\n",
       "      <td>117</td>\n",
       "      <td>267</td>\n",
       "      <td>66</td>\n",
       "      <td>61</td>\n",
       "      <td>139</td>\n",
       "      <td>-126</td>\n",
       "      <td>-117</td>\n",
       "      <td>-266</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1222</td>\n",
       "      <td>-3838</td>\n",
       "      <td>-779</td>\n",
       "      <td>907</td>\n",
       "      <td>-2877</td>\n",
       "      <td>-593</td>\n",
       "      <td>-1041</td>\n",
       "      <td>3276</td>\n",
       "      <td>667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>2379</td>\n",
       "      <td>-646</td>\n",
       "      <td>284</td>\n",
       "      <td>2341</td>\n",
       "      <td>-636</td>\n",
       "      <td>279</td>\n",
       "      <td>-2154</td>\n",
       "      <td>585</td>\n",
       "      <td>-257</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>15272538</td>\n",
       "      <td>5046863</td>\n",
       "      <td>22307690</td>\n",
       "      <td>6981001</td>\n",
       "      <td>2306896</td>\n",
       "      <td>10196734</td>\n",
       "      <td>-11924805</td>\n",
       "      <td>-3940593</td>\n",
       "      <td>-17417855</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           00       01        02       10       11        12        20  \\\n",
       "0       16412    -4360      2087    84997   -22605     10820   -402030   \n",
       "1         -59      -30       -11      -60      -29       -11        54   \n",
       "2       -2502    -1240     -4111    -2462    -1219     -4044      2265   \n",
       "3          37       14        56       42       19        67       -36   \n",
       "4          -1       -1        -2        0       -1        -1         6   \n",
       "..        ...      ...       ...      ...      ...       ...       ...   \n",
       "995   -283378   442741     77288  -805349  1258246    219648    541908   \n",
       "996       127      117       267       66       61       139      -126   \n",
       "997      1222    -3838      -779      907    -2877      -593     -1041   \n",
       "998      2379     -646       284     2341     -636       279     -2154   \n",
       "999  15272538  5046863  22307690  6981001  2306896  10196734 -11924805   \n",
       "\n",
       "          21        22  last  \n",
       "0     106929    -51182     0  \n",
       "1         27        10     1  \n",
       "2       1122      3721     1  \n",
       "3        -15       -56     1  \n",
       "4          6        13     0  \n",
       "..       ...       ...   ...  \n",
       "995  -846678   -147803     0  \n",
       "996     -117      -266     0  \n",
       "997     3276       667     1  \n",
       "998      585      -257     1  \n",
       "999 -3940593 -17417855     0  \n",
       "\n",
       "[1000 rows x 10 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate('matrices_train.csv')\n",
    "generate('matrices_test.csv')\n",
    "df = pd.read_csv(\"matrices_train.csv\")\n",
    "just_input = df.drop('last', axis=1)\n",
    "\n",
    "# adapted from https://stackoverflow.com/questions/43898035/pandas-combine-column-values-into-a-list-in-a-new-column\n",
    "input_data = np.array(just_input.values.tolist())\n",
    "desired_output = torch.tensor(df['last'].tolist(), dtype=torch.float32).long()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xs/6yx8fshd7_v5fg9sl09cfpdh0000gn/T/ipykernel_16955/363369511.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  desired_outputs = torch.tensor(desired_outputs, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total loss: 0.7322138547897339\n",
      "Epoch [1/500], Loss: 0.7322\n",
      "total loss: 0.7695125937461853\n",
      "Epoch [2/500], Loss: 0.7695\n",
      "total loss: 0.6995541453361511\n",
      "Epoch [3/500], Loss: 0.6996\n",
      "total loss: 0.7158668041229248\n",
      "Epoch [4/500], Loss: 0.7159\n",
      "total loss: 0.7108623385429382\n",
      "Epoch [5/500], Loss: 0.7109\n",
      "total loss: 0.6687511801719666\n",
      "Epoch [6/500], Loss: 0.6688\n",
      "total loss: 0.7852352261543274\n",
      "Epoch [7/500], Loss: 0.7852\n",
      "total loss: 0.8124547004699707\n",
      "Epoch [8/500], Loss: 0.8125\n",
      "total loss: 0.8140930533409119\n",
      "Epoch [9/500], Loss: 0.8141\n",
      "total loss: 0.8073356747627258\n",
      "Epoch [10/500], Loss: 0.8073\n",
      "total loss: 0.804980456829071\n",
      "Epoch [11/500], Loss: 0.8050\n",
      "total loss: 0.7936748266220093\n",
      "Epoch [12/500], Loss: 0.7937\n",
      "total loss: 0.7952510118484497\n",
      "Epoch [13/500], Loss: 0.7953\n",
      "total loss: 0.792862057685852\n",
      "Epoch [14/500], Loss: 0.7929\n",
      "total loss: 0.743161678314209\n",
      "Epoch [15/500], Loss: 0.7432\n",
      "total loss: 0.7201625108718872\n",
      "Epoch [16/500], Loss: 0.7202\n",
      "total loss: 0.6983873844146729\n",
      "Epoch [17/500], Loss: 0.6984\n",
      "total loss: 0.6901479959487915\n",
      "Epoch [18/500], Loss: 0.6901\n",
      "total loss: 0.6833825707435608\n",
      "Epoch [19/500], Loss: 0.6834\n",
      "total loss: 0.6972296833992004\n",
      "Epoch [20/500], Loss: 0.6972\n",
      "total loss: 0.6797225475311279\n",
      "Epoch [21/500], Loss: 0.6797\n",
      "total loss: 0.6716015338897705\n",
      "Epoch [22/500], Loss: 0.6716\n",
      "total loss: 0.6577970385551453\n",
      "Epoch [23/500], Loss: 0.6578\n",
      "total loss: 0.6407871842384338\n",
      "Epoch [24/500], Loss: 0.6408\n",
      "total loss: 0.6308894157409668\n",
      "Epoch [25/500], Loss: 0.6309\n",
      "total loss: 0.645964503288269\n",
      "Epoch [26/500], Loss: 0.6460\n",
      "total loss: 0.657096803188324\n",
      "Epoch [27/500], Loss: 0.6571\n",
      "total loss: 0.6609724164009094\n",
      "Epoch [28/500], Loss: 0.6610\n",
      "total loss: 0.6653357148170471\n",
      "Epoch [29/500], Loss: 0.6653\n",
      "total loss: 0.6651045083999634\n",
      "Epoch [30/500], Loss: 0.6651\n",
      "total loss: 0.6602052450180054\n",
      "Epoch [31/500], Loss: 0.6602\n",
      "total loss: 0.6514689922332764\n",
      "Epoch [32/500], Loss: 0.6515\n",
      "total loss: 0.6615053415298462\n",
      "Epoch [33/500], Loss: 0.6615\n",
      "total loss: 0.6756258010864258\n",
      "Epoch [34/500], Loss: 0.6756\n",
      "total loss: 0.6826149821281433\n",
      "Epoch [35/500], Loss: 0.6826\n",
      "total loss: 0.6852641105651855\n",
      "Epoch [36/500], Loss: 0.6853\n",
      "total loss: 0.6827858686447144\n",
      "Epoch [37/500], Loss: 0.6828\n",
      "total loss: 0.6836094260215759\n",
      "Epoch [38/500], Loss: 0.6836\n",
      "total loss: 0.6839054226875305\n",
      "Epoch [39/500], Loss: 0.6839\n",
      "total loss: 0.6858596205711365\n",
      "Epoch [40/500], Loss: 0.6859\n",
      "total loss: 0.683283269405365\n",
      "Epoch [41/500], Loss: 0.6833\n",
      "total loss: 0.6872758269309998\n",
      "Epoch [42/500], Loss: 0.6873\n",
      "total loss: 0.6936945915222168\n",
      "Epoch [43/500], Loss: 0.6937\n",
      "total loss: 0.7049778699874878\n",
      "Epoch [44/500], Loss: 0.7050\n",
      "total loss: 0.6900871992111206\n",
      "Epoch [45/500], Loss: 0.6901\n",
      "total loss: 0.6870266795158386\n",
      "Epoch [46/500], Loss: 0.6870\n",
      "total loss: 0.6829781532287598\n",
      "Epoch [47/500], Loss: 0.6830\n",
      "total loss: 0.6816338896751404\n",
      "Epoch [48/500], Loss: 0.6816\n",
      "total loss: 0.6812055706977844\n",
      "Epoch [49/500], Loss: 0.6812\n",
      "total loss: 0.6785081028938293\n",
      "Epoch [50/500], Loss: 0.6785\n",
      "total loss: 0.6739094257354736\n",
      "Epoch [51/500], Loss: 0.6739\n",
      "total loss: 0.6703392267227173\n",
      "Epoch [52/500], Loss: 0.6703\n",
      "total loss: 0.670373797416687\n",
      "Epoch [53/500], Loss: 0.6704\n",
      "total loss: 0.668062150478363\n",
      "Epoch [54/500], Loss: 0.6681\n",
      "total loss: 0.6657661199569702\n",
      "Epoch [55/500], Loss: 0.6658\n",
      "total loss: 0.6655876040458679\n",
      "Epoch [56/500], Loss: 0.6656\n",
      "total loss: 0.6664320230484009\n",
      "Epoch [57/500], Loss: 0.6664\n",
      "total loss: 0.6662971377372742\n",
      "Epoch [58/500], Loss: 0.6663\n",
      "total loss: 0.665162205696106\n",
      "Epoch [59/500], Loss: 0.6652\n",
      "total loss: 0.6650470495223999\n",
      "Epoch [60/500], Loss: 0.6650\n",
      "total loss: 0.6649546027183533\n",
      "Epoch [61/500], Loss: 0.6650\n",
      "total loss: 0.6648519039154053\n",
      "Epoch [62/500], Loss: 0.6649\n",
      "total loss: 0.663765549659729\n",
      "Epoch [63/500], Loss: 0.6638\n",
      "total loss: 0.6636825799942017\n",
      "Epoch [64/500], Loss: 0.6637\n",
      "total loss: 0.6635801792144775\n",
      "Epoch [65/500], Loss: 0.6636\n",
      "total loss: 0.6634050011634827\n",
      "Epoch [66/500], Loss: 0.6634\n",
      "total loss: 0.6638281345367432\n",
      "Epoch [67/500], Loss: 0.6638\n",
      "total loss: 0.6626205444335938\n",
      "Epoch [68/500], Loss: 0.6626\n",
      "total loss: 0.6614317297935486\n",
      "Epoch [69/500], Loss: 0.6614\n",
      "total loss: 0.6624199748039246\n",
      "Epoch [70/500], Loss: 0.6624\n",
      "total loss: 0.6623883247375488\n",
      "Epoch [71/500], Loss: 0.6624\n",
      "total loss: 0.6593510508537292\n",
      "Epoch [72/500], Loss: 0.6594\n",
      "total loss: 0.6614678502082825\n",
      "Epoch [73/500], Loss: 0.6615\n",
      "total loss: 0.6607667207717896\n",
      "Epoch [74/500], Loss: 0.6608\n",
      "total loss: 0.6575233936309814\n",
      "Epoch [75/500], Loss: 0.6575\n",
      "total loss: 0.65217524766922\n",
      "Epoch [76/500], Loss: 0.6522\n",
      "total loss: 0.6434056162834167\n",
      "Epoch [77/500], Loss: 0.6434\n",
      "total loss: 0.6291550397872925\n",
      "Epoch [78/500], Loss: 0.6292\n",
      "total loss: 0.622204601764679\n",
      "Epoch [79/500], Loss: 0.6222\n",
      "total loss: 0.6297832727432251\n",
      "Epoch [80/500], Loss: 0.6298\n",
      "total loss: 0.6282115578651428\n",
      "Epoch [81/500], Loss: 0.6282\n",
      "total loss: 0.6053734421730042\n",
      "Epoch [82/500], Loss: 0.6054\n",
      "total loss: 0.6029680371284485\n",
      "Epoch [83/500], Loss: 0.6030\n",
      "total loss: 0.6034754514694214\n",
      "Epoch [84/500], Loss: 0.6035\n",
      "total loss: 0.6057218909263611\n",
      "Epoch [85/500], Loss: 0.6057\n",
      "total loss: 0.6323899030685425\n",
      "Epoch [86/500], Loss: 0.6324\n",
      "total loss: 0.6334062218666077\n",
      "Epoch [87/500], Loss: 0.6334\n",
      "total loss: 0.6372668743133545\n",
      "Epoch [88/500], Loss: 0.6373\n",
      "total loss: 0.6375653743743896\n",
      "Epoch [89/500], Loss: 0.6376\n",
      "total loss: 0.6366202235221863\n",
      "Epoch [90/500], Loss: 0.6366\n",
      "total loss: 0.6396452784538269\n",
      "Epoch [91/500], Loss: 0.6396\n",
      "total loss: 0.6354469060897827\n",
      "Epoch [92/500], Loss: 0.6354\n",
      "total loss: 0.636544406414032\n",
      "Epoch [93/500], Loss: 0.6365\n",
      "total loss: 0.6343052983283997\n",
      "Epoch [94/500], Loss: 0.6343\n",
      "total loss: 0.6313449144363403\n",
      "Epoch [95/500], Loss: 0.6313\n",
      "total loss: 0.629258930683136\n",
      "Epoch [96/500], Loss: 0.6293\n",
      "total loss: 0.6281470656394958\n",
      "Epoch [97/500], Loss: 0.6281\n",
      "total loss: 0.6248027086257935\n",
      "Epoch [98/500], Loss: 0.6248\n",
      "total loss: 0.6239461302757263\n",
      "Epoch [99/500], Loss: 0.6239\n",
      "total loss: 0.6208541393280029\n",
      "Epoch [100/500], Loss: 0.6209\n",
      "total loss: 0.6154731512069702\n",
      "Epoch [101/500], Loss: 0.6155\n",
      "total loss: 0.603329062461853\n",
      "Epoch [102/500], Loss: 0.6033\n",
      "total loss: 0.5946401953697205\n",
      "Epoch [103/500], Loss: 0.5946\n",
      "total loss: 0.5937233567237854\n",
      "Epoch [104/500], Loss: 0.5937\n",
      "total loss: 0.5932260751724243\n",
      "Epoch [105/500], Loss: 0.5932\n",
      "total loss: 0.5980152487754822\n",
      "Epoch [106/500], Loss: 0.5980\n",
      "total loss: 0.5952780246734619\n",
      "Epoch [107/500], Loss: 0.5953\n",
      "total loss: 0.5764606595039368\n",
      "Epoch [108/500], Loss: 0.5765\n",
      "total loss: 0.5720720291137695\n",
      "Epoch [109/500], Loss: 0.5721\n",
      "total loss: 0.572390615940094\n",
      "Epoch [110/500], Loss: 0.5724\n",
      "total loss: 0.5739379525184631\n",
      "Epoch [111/500], Loss: 0.5739\n",
      "total loss: 0.5757291913032532\n",
      "Epoch [112/500], Loss: 0.5757\n",
      "total loss: 0.5743772983551025\n",
      "Epoch [113/500], Loss: 0.5744\n",
      "total loss: 0.5692983269691467\n",
      "Epoch [114/500], Loss: 0.5693\n",
      "total loss: 0.5645084381103516\n",
      "Epoch [115/500], Loss: 0.5645\n",
      "total loss: 0.5625327825546265\n",
      "Epoch [116/500], Loss: 0.5625\n",
      "total loss: 0.557418942451477\n",
      "Epoch [117/500], Loss: 0.5574\n",
      "total loss: 0.5512903332710266\n",
      "Epoch [118/500], Loss: 0.5513\n",
      "total loss: 0.5565143823623657\n",
      "Epoch [119/500], Loss: 0.5565\n",
      "total loss: 0.5898823738098145\n",
      "Epoch [120/500], Loss: 0.5899\n",
      "total loss: 0.6048312783241272\n",
      "Epoch [121/500], Loss: 0.6048\n",
      "total loss: 0.6170977354049683\n",
      "Epoch [122/500], Loss: 0.6171\n",
      "total loss: 0.6279388666152954\n",
      "Epoch [123/500], Loss: 0.6279\n",
      "total loss: 0.6336050033569336\n",
      "Epoch [124/500], Loss: 0.6336\n",
      "total loss: 0.6293469667434692\n",
      "Epoch [125/500], Loss: 0.6293\n",
      "total loss: 0.6263815760612488\n",
      "Epoch [126/500], Loss: 0.6264\n",
      "total loss: 0.6067497134208679\n",
      "Epoch [127/500], Loss: 0.6067\n",
      "total loss: 0.5898038148880005\n",
      "Epoch [128/500], Loss: 0.5898\n",
      "total loss: 0.5905681252479553\n",
      "Epoch [129/500], Loss: 0.5906\n",
      "total loss: 0.5857539772987366\n",
      "Epoch [130/500], Loss: 0.5858\n",
      "total loss: 0.5863763689994812\n",
      "Epoch [131/500], Loss: 0.5864\n",
      "total loss: 0.5826510190963745\n",
      "Epoch [132/500], Loss: 0.5827\n",
      "total loss: 0.5833151936531067\n",
      "Epoch [133/500], Loss: 0.5833\n",
      "total loss: 0.5801220536231995\n",
      "Epoch [134/500], Loss: 0.5801\n",
      "total loss: 0.576254665851593\n",
      "Epoch [135/500], Loss: 0.5763\n",
      "total loss: 0.5736344456672668\n",
      "Epoch [136/500], Loss: 0.5736\n",
      "total loss: 0.5736521482467651\n",
      "Epoch [137/500], Loss: 0.5737\n",
      "total loss: 0.5662948489189148\n",
      "Epoch [138/500], Loss: 0.5663\n",
      "total loss: 0.5647428035736084\n",
      "Epoch [139/500], Loss: 0.5647\n",
      "total loss: 0.565675675868988\n",
      "Epoch [140/500], Loss: 0.5657\n",
      "total loss: 0.5653550624847412\n",
      "Epoch [141/500], Loss: 0.5654\n",
      "total loss: 0.5622309446334839\n",
      "Epoch [142/500], Loss: 0.5622\n",
      "total loss: 0.5613320469856262\n",
      "Epoch [143/500], Loss: 0.5613\n",
      "total loss: 0.5617809295654297\n",
      "Epoch [144/500], Loss: 0.5618\n",
      "total loss: 0.5608437061309814\n",
      "Epoch [145/500], Loss: 0.5608\n",
      "total loss: 0.5586881041526794\n",
      "Epoch [146/500], Loss: 0.5587\n",
      "total loss: 0.5584720373153687\n",
      "Epoch [147/500], Loss: 0.5585\n",
      "total loss: 0.5582548975944519\n",
      "Epoch [148/500], Loss: 0.5583\n",
      "total loss: 0.5570337772369385\n",
      "Epoch [149/500], Loss: 0.5570\n",
      "total loss: 0.5585351586341858\n",
      "Epoch [150/500], Loss: 0.5585\n",
      "total loss: 0.5570798516273499\n",
      "Epoch [151/500], Loss: 0.5571\n",
      "total loss: 0.5581486225128174\n",
      "Epoch [152/500], Loss: 0.5581\n",
      "total loss: 0.5560885071754456\n",
      "Epoch [153/500], Loss: 0.5561\n",
      "total loss: 0.5581055283546448\n",
      "Epoch [154/500], Loss: 0.5581\n",
      "total loss: 0.5570563077926636\n",
      "Epoch [155/500], Loss: 0.5571\n",
      "total loss: 0.5620627403259277\n",
      "Epoch [156/500], Loss: 0.5621\n",
      "total loss: 0.5683391690254211\n",
      "Epoch [157/500], Loss: 0.5683\n",
      "total loss: 0.5558556318283081\n",
      "Epoch [158/500], Loss: 0.5559\n",
      "total loss: 0.5545657873153687\n",
      "Epoch [159/500], Loss: 0.5546\n",
      "total loss: 0.5573247671127319\n",
      "Epoch [160/500], Loss: 0.5573\n",
      "total loss: 0.5574832558631897\n",
      "Epoch [161/500], Loss: 0.5575\n",
      "total loss: 0.5550917387008667\n",
      "Epoch [162/500], Loss: 0.5551\n",
      "total loss: 0.5563871264457703\n",
      "Epoch [163/500], Loss: 0.5564\n",
      "total loss: 0.5538920164108276\n",
      "Epoch [164/500], Loss: 0.5539\n",
      "total loss: 0.5524951815605164\n",
      "Epoch [165/500], Loss: 0.5525\n",
      "total loss: 0.5523747801780701\n",
      "Epoch [166/500], Loss: 0.5524\n",
      "total loss: 0.5542876124382019\n",
      "Epoch [167/500], Loss: 0.5543\n",
      "total loss: 0.5532015562057495\n",
      "Epoch [168/500], Loss: 0.5532\n",
      "total loss: 0.5502612590789795\n",
      "Epoch [169/500], Loss: 0.5503\n",
      "total loss: 0.5489813089370728\n",
      "Epoch [170/500], Loss: 0.5490\n",
      "total loss: 0.5473607778549194\n",
      "Epoch [171/500], Loss: 0.5474\n",
      "total loss: 0.542672872543335\n",
      "Epoch [172/500], Loss: 0.5427\n",
      "total loss: 0.5426990985870361\n",
      "Epoch [173/500], Loss: 0.5427\n",
      "total loss: 0.5417157411575317\n",
      "Epoch [174/500], Loss: 0.5417\n",
      "total loss: 0.5422132611274719\n",
      "Epoch [175/500], Loss: 0.5422\n",
      "total loss: 0.5412846207618713\n",
      "Epoch [176/500], Loss: 0.5413\n",
      "total loss: 0.5408666729927063\n",
      "Epoch [177/500], Loss: 0.5409\n",
      "total loss: 0.5416107177734375\n",
      "Epoch [178/500], Loss: 0.5416\n",
      "total loss: 0.5414435863494873\n",
      "Epoch [179/500], Loss: 0.5414\n",
      "total loss: 0.5434024333953857\n",
      "Epoch [180/500], Loss: 0.5434\n",
      "total loss: 0.5433484315872192\n",
      "Epoch [181/500], Loss: 0.5433\n",
      "total loss: 0.5432946681976318\n",
      "Epoch [182/500], Loss: 0.5433\n",
      "total loss: 0.5425150394439697\n",
      "Epoch [183/500], Loss: 0.5425\n",
      "total loss: 0.5430094003677368\n",
      "Epoch [184/500], Loss: 0.5430\n",
      "total loss: 0.5433821082115173\n",
      "Epoch [185/500], Loss: 0.5434\n",
      "total loss: 0.5436150431632996\n",
      "Epoch [186/500], Loss: 0.5436\n",
      "total loss: 0.543764591217041\n",
      "Epoch [187/500], Loss: 0.5438\n",
      "total loss: 0.5436227917671204\n",
      "Epoch [188/500], Loss: 0.5436\n",
      "total loss: 0.5431165099143982\n",
      "Epoch [189/500], Loss: 0.5431\n",
      "total loss: 0.5425412654876709\n",
      "Epoch [190/500], Loss: 0.5425\n",
      "total loss: 0.5422402620315552\n",
      "Epoch [191/500], Loss: 0.5422\n",
      "total loss: 0.5421056747436523\n",
      "Epoch [192/500], Loss: 0.5421\n",
      "total loss: 0.542885959148407\n",
      "Epoch [193/500], Loss: 0.5429\n",
      "total loss: 0.5420224666595459\n",
      "Epoch [194/500], Loss: 0.5420\n",
      "total loss: 0.5420366525650024\n",
      "Epoch [195/500], Loss: 0.5420\n",
      "total loss: 0.5420441627502441\n",
      "Epoch [196/500], Loss: 0.5420\n",
      "total loss: 0.5420417189598083\n",
      "Epoch [197/500], Loss: 0.5420\n",
      "total loss: 0.5420286059379578\n",
      "Epoch [198/500], Loss: 0.5420\n",
      "total loss: 0.5420039892196655\n",
      "Epoch [199/500], Loss: 0.5420\n",
      "total loss: 0.5419637560844421\n",
      "Epoch [200/500], Loss: 0.5420\n",
      "total loss: 0.5419056415557861\n",
      "Epoch [201/500], Loss: 0.5419\n",
      "total loss: 0.541833758354187\n",
      "Epoch [202/500], Loss: 0.5418\n",
      "total loss: 0.5417565703392029\n",
      "Epoch [203/500], Loss: 0.5418\n",
      "total loss: 0.5416797399520874\n",
      "Epoch [204/500], Loss: 0.5417\n",
      "total loss: 0.5416062474250793\n",
      "Epoch [205/500], Loss: 0.5416\n",
      "total loss: 0.5415364503860474\n",
      "Epoch [206/500], Loss: 0.5415\n",
      "total loss: 0.5414798855781555\n",
      "Epoch [207/500], Loss: 0.5415\n",
      "total loss: 0.5414202809333801\n",
      "Epoch [208/500], Loss: 0.5414\n",
      "total loss: 0.5413593053817749\n",
      "Epoch [209/500], Loss: 0.5414\n",
      "total loss: 0.5413046479225159\n",
      "Epoch [210/500], Loss: 0.5413\n",
      "total loss: 0.5412567853927612\n",
      "Epoch [211/500], Loss: 0.5413\n",
      "total loss: 0.5412108898162842\n",
      "Epoch [212/500], Loss: 0.5412\n",
      "total loss: 0.5411679744720459\n",
      "Epoch [213/500], Loss: 0.5412\n",
      "total loss: 0.5411273241043091\n",
      "Epoch [214/500], Loss: 0.5411\n",
      "total loss: 0.5412196516990662\n",
      "Epoch [215/500], Loss: 0.5412\n",
      "total loss: 0.541092038154602\n",
      "Epoch [216/500], Loss: 0.5411\n",
      "total loss: 0.5401858687400818\n",
      "Epoch [217/500], Loss: 0.5402\n",
      "total loss: 0.5405625104904175\n",
      "Epoch [218/500], Loss: 0.5406\n",
      "total loss: 0.5412426590919495\n",
      "Epoch [219/500], Loss: 0.5412\n",
      "total loss: 0.5408146381378174\n",
      "Epoch [220/500], Loss: 0.5408\n",
      "total loss: 0.54024738073349\n",
      "Epoch [221/500], Loss: 0.5402\n",
      "total loss: 0.5400193333625793\n",
      "Epoch [222/500], Loss: 0.5400\n",
      "total loss: 0.5399380326271057\n",
      "Epoch [223/500], Loss: 0.5399\n",
      "total loss: 0.5398880839347839\n",
      "Epoch [224/500], Loss: 0.5399\n",
      "total loss: 0.5398535132408142\n",
      "Epoch [225/500], Loss: 0.5399\n",
      "total loss: 0.5398272275924683\n",
      "Epoch [226/500], Loss: 0.5398\n",
      "total loss: 0.5399236083030701\n",
      "Epoch [227/500], Loss: 0.5399\n",
      "total loss: 0.5407953858375549\n",
      "Epoch [228/500], Loss: 0.5408\n",
      "total loss: 0.5407931804656982\n",
      "Epoch [229/500], Loss: 0.5408\n",
      "total loss: 0.5408003926277161\n",
      "Epoch [230/500], Loss: 0.5408\n",
      "total loss: 0.5407991409301758\n",
      "Epoch [231/500], Loss: 0.5408\n",
      "total loss: 0.5378127694129944\n",
      "Epoch [232/500], Loss: 0.5378\n",
      "total loss: 0.5378318428993225\n",
      "Epoch [233/500], Loss: 0.5378\n",
      "total loss: 0.5388540029525757\n",
      "Epoch [234/500], Loss: 0.5389\n",
      "total loss: 0.5379593968391418\n",
      "Epoch [235/500], Loss: 0.5380\n",
      "total loss: 0.540789783000946\n",
      "Epoch [236/500], Loss: 0.5408\n",
      "total loss: 0.5416855812072754\n",
      "Epoch [237/500], Loss: 0.5417\n",
      "total loss: 0.539071798324585\n",
      "Epoch [238/500], Loss: 0.5391\n",
      "total loss: 0.543665885925293\n",
      "Epoch [239/500], Loss: 0.5437\n",
      "total loss: 0.5399535298347473\n",
      "Epoch [240/500], Loss: 0.5400\n",
      "total loss: 0.5414443016052246\n",
      "Epoch [241/500], Loss: 0.5414\n",
      "total loss: 0.5429407358169556\n",
      "Epoch [242/500], Loss: 0.5429\n",
      "total loss: 0.5424861907958984\n",
      "Epoch [243/500], Loss: 0.5425\n",
      "total loss: 0.5447251200675964\n",
      "Epoch [244/500], Loss: 0.5447\n",
      "total loss: 0.5447344779968262\n",
      "Epoch [245/500], Loss: 0.5447\n",
      "total loss: 0.5451313257217407\n",
      "Epoch [246/500], Loss: 0.5451\n",
      "total loss: 0.5444692373275757\n",
      "Epoch [247/500], Loss: 0.5445\n",
      "total loss: 0.5436874628067017\n",
      "Epoch [248/500], Loss: 0.5437\n",
      "total loss: 0.5439520478248596\n",
      "Epoch [249/500], Loss: 0.5440\n",
      "total loss: 0.543398380279541\n",
      "Epoch [250/500], Loss: 0.5434\n",
      "total loss: 0.5432547330856323\n",
      "Epoch [251/500], Loss: 0.5433\n",
      "total loss: 0.5421302914619446\n",
      "Epoch [252/500], Loss: 0.5421\n",
      "total loss: 0.5410155057907104\n",
      "Epoch [253/500], Loss: 0.5410\n",
      "total loss: 0.5409054160118103\n",
      "Epoch [254/500], Loss: 0.5409\n",
      "total loss: 0.5407910943031311\n",
      "Epoch [255/500], Loss: 0.5408\n",
      "total loss: 0.5406644344329834\n",
      "Epoch [256/500], Loss: 0.5407\n",
      "total loss: 0.5397941470146179\n",
      "Epoch [257/500], Loss: 0.5398\n",
      "total loss: 0.5413445830345154\n",
      "Epoch [258/500], Loss: 0.5413\n",
      "total loss: 0.5422351956367493\n",
      "Epoch [259/500], Loss: 0.5422\n",
      "total loss: 0.5432186722755432\n",
      "Epoch [260/500], Loss: 0.5432\n",
      "total loss: 0.5432404279708862\n",
      "Epoch [261/500], Loss: 0.5432\n",
      "total loss: 0.5432842373847961\n",
      "Epoch [262/500], Loss: 0.5433\n",
      "total loss: 0.542374849319458\n",
      "Epoch [263/500], Loss: 0.5424\n",
      "total loss: 0.5421795845031738\n",
      "Epoch [264/500], Loss: 0.5422\n",
      "total loss: 0.542812168598175\n",
      "Epoch [265/500], Loss: 0.5428\n",
      "total loss: 0.5426877737045288\n",
      "Epoch [266/500], Loss: 0.5427\n",
      "total loss: 0.5426589250564575\n",
      "Epoch [267/500], Loss: 0.5427\n",
      "total loss: 0.5413762331008911\n",
      "Epoch [268/500], Loss: 0.5414\n",
      "total loss: 0.5412230491638184\n",
      "Epoch [269/500], Loss: 0.5412\n",
      "total loss: 0.5421451330184937\n",
      "Epoch [270/500], Loss: 0.5421\n",
      "total loss: 0.5411319732666016\n",
      "Epoch [271/500], Loss: 0.5411\n",
      "total loss: 0.5421322584152222\n",
      "Epoch [272/500], Loss: 0.5421\n",
      "total loss: 0.541157066822052\n",
      "Epoch [273/500], Loss: 0.5412\n",
      "total loss: 0.542160153388977\n",
      "Epoch [274/500], Loss: 0.5422\n",
      "total loss: 0.5423498749732971\n",
      "Epoch [275/500], Loss: 0.5423\n",
      "total loss: 0.542056143283844\n",
      "Epoch [276/500], Loss: 0.5421\n",
      "total loss: 0.5400715470314026\n",
      "Epoch [277/500], Loss: 0.5401\n",
      "total loss: 0.5400741100311279\n",
      "Epoch [278/500], Loss: 0.5401\n",
      "total loss: 0.5401023626327515\n",
      "Epoch [279/500], Loss: 0.5401\n",
      "total loss: 0.5401548147201538\n",
      "Epoch [280/500], Loss: 0.5402\n",
      "total loss: 0.540219247341156\n",
      "Epoch [281/500], Loss: 0.5402\n",
      "total loss: 0.5412952899932861\n",
      "Epoch [282/500], Loss: 0.5413\n",
      "total loss: 0.5423658490180969\n",
      "Epoch [283/500], Loss: 0.5424\n",
      "total loss: 0.5424194931983948\n",
      "Epoch [284/500], Loss: 0.5424\n",
      "total loss: 0.5424403548240662\n",
      "Epoch [285/500], Loss: 0.5424\n",
      "total loss: 0.5424140691757202\n",
      "Epoch [286/500], Loss: 0.5424\n",
      "total loss: 0.5423461198806763\n",
      "Epoch [287/500], Loss: 0.5423\n",
      "total loss: 0.5422549843788147\n",
      "Epoch [288/500], Loss: 0.5423\n",
      "total loss: 0.5421586036682129\n",
      "Epoch [289/500], Loss: 0.5422\n",
      "total loss: 0.5420676469802856\n",
      "Epoch [290/500], Loss: 0.5421\n",
      "total loss: 0.5419865846633911\n",
      "Epoch [291/500], Loss: 0.5420\n",
      "total loss: 0.5424975752830505\n",
      "Epoch [292/500], Loss: 0.5425\n",
      "total loss: 0.5431240797042847\n",
      "Epoch [293/500], Loss: 0.5431\n",
      "total loss: 0.5440016984939575\n",
      "Epoch [294/500], Loss: 0.5440\n",
      "total loss: 0.5449024438858032\n",
      "Epoch [295/500], Loss: 0.5449\n",
      "total loss: 0.5486196875572205\n",
      "Epoch [296/500], Loss: 0.5486\n",
      "total loss: 0.5529047846794128\n",
      "Epoch [297/500], Loss: 0.5529\n",
      "total loss: 0.5540183186531067\n",
      "Epoch [298/500], Loss: 0.5540\n",
      "total loss: 0.5609822869300842\n",
      "Epoch [299/500], Loss: 0.5610\n",
      "total loss: 0.5650840997695923\n",
      "Epoch [300/500], Loss: 0.5651\n",
      "total loss: 0.5694025158882141\n",
      "Epoch [301/500], Loss: 0.5694\n",
      "total loss: 0.5703310966491699\n",
      "Epoch [302/500], Loss: 0.5703\n",
      "total loss: 0.5726078152656555\n",
      "Epoch [303/500], Loss: 0.5726\n",
      "total loss: 0.5726770162582397\n",
      "Epoch [304/500], Loss: 0.5727\n",
      "total loss: 0.5721527934074402\n",
      "Epoch [305/500], Loss: 0.5722\n",
      "total loss: 0.570784866809845\n",
      "Epoch [306/500], Loss: 0.5708\n",
      "total loss: 0.5723608136177063\n",
      "Epoch [307/500], Loss: 0.5724\n",
      "total loss: 0.5704237818717957\n",
      "Epoch [308/500], Loss: 0.5704\n",
      "total loss: 0.5681653618812561\n",
      "Epoch [309/500], Loss: 0.5682\n",
      "total loss: 0.5673214793205261\n",
      "Epoch [310/500], Loss: 0.5673\n",
      "total loss: 0.5673030018806458\n",
      "Epoch [311/500], Loss: 0.5673\n",
      "total loss: 0.5691609978675842\n",
      "Epoch [312/500], Loss: 0.5692\n",
      "total loss: 0.5675878524780273\n",
      "Epoch [313/500], Loss: 0.5676\n",
      "total loss: 0.5657625794410706\n",
      "Epoch [314/500], Loss: 0.5658\n",
      "total loss: 0.5644699931144714\n",
      "Epoch [315/500], Loss: 0.5645\n",
      "total loss: 0.5631536245346069\n",
      "Epoch [316/500], Loss: 0.5632\n",
      "total loss: 0.5619256496429443\n",
      "Epoch [317/500], Loss: 0.5619\n",
      "total loss: 0.56133633852005\n",
      "Epoch [318/500], Loss: 0.5613\n",
      "total loss: 0.5598731637001038\n",
      "Epoch [319/500], Loss: 0.5599\n",
      "total loss: 0.5597425103187561\n",
      "Epoch [320/500], Loss: 0.5597\n",
      "total loss: 0.5586146116256714\n",
      "Epoch [321/500], Loss: 0.5586\n",
      "total loss: 0.5584843158721924\n",
      "Epoch [322/500], Loss: 0.5585\n",
      "total loss: 0.5583580136299133\n",
      "Epoch [323/500], Loss: 0.5584\n",
      "total loss: 0.5582387447357178\n",
      "Epoch [324/500], Loss: 0.5582\n",
      "total loss: 0.5581421256065369\n",
      "Epoch [325/500], Loss: 0.5581\n",
      "total loss: 0.5580496191978455\n",
      "Epoch [326/500], Loss: 0.5580\n",
      "total loss: 0.5579542517662048\n",
      "Epoch [327/500], Loss: 0.5580\n",
      "total loss: 0.5588526725769043\n",
      "Epoch [328/500], Loss: 0.5589\n",
      "total loss: 0.5567451119422913\n",
      "Epoch [329/500], Loss: 0.5567\n",
      "total loss: 0.5566204190254211\n",
      "Epoch [330/500], Loss: 0.5566\n",
      "total loss: 0.5563973784446716\n",
      "Epoch [331/500], Loss: 0.5564\n",
      "total loss: 0.5556488633155823\n",
      "Epoch [332/500], Loss: 0.5556\n",
      "total loss: 0.5552369952201843\n",
      "Epoch [333/500], Loss: 0.5552\n",
      "total loss: 0.5541791319847107\n",
      "Epoch [334/500], Loss: 0.5542\n",
      "total loss: 0.5529995560646057\n",
      "Epoch [335/500], Loss: 0.5530\n",
      "total loss: 0.5537965893745422\n",
      "Epoch [336/500], Loss: 0.5538\n",
      "total loss: 0.5534904599189758\n",
      "Epoch [337/500], Loss: 0.5535\n",
      "total loss: 0.552065372467041\n",
      "Epoch [338/500], Loss: 0.5521\n",
      "total loss: 0.5524722933769226\n",
      "Epoch [339/500], Loss: 0.5525\n",
      "total loss: 0.5517240166664124\n",
      "Epoch [340/500], Loss: 0.5517\n",
      "total loss: 0.5508905053138733\n",
      "Epoch [341/500], Loss: 0.5509\n",
      "total loss: 0.5481398701667786\n",
      "Epoch [342/500], Loss: 0.5481\n",
      "total loss: 0.5445038080215454\n",
      "Epoch [343/500], Loss: 0.5445\n",
      "total loss: 0.5435319542884827\n",
      "Epoch [344/500], Loss: 0.5435\n",
      "total loss: 0.5405975580215454\n",
      "Epoch [345/500], Loss: 0.5406\n",
      "total loss: 0.5406110882759094\n",
      "Epoch [346/500], Loss: 0.5406\n",
      "total loss: 0.5405784249305725\n",
      "Epoch [347/500], Loss: 0.5406\n",
      "total loss: 0.5404608845710754\n",
      "Epoch [348/500], Loss: 0.5405\n",
      "total loss: 0.5412561893463135\n",
      "Epoch [349/500], Loss: 0.5413\n",
      "total loss: 0.5407925844192505\n",
      "Epoch [350/500], Loss: 0.5408\n",
      "total loss: 0.5398054122924805\n",
      "Epoch [351/500], Loss: 0.5398\n",
      "total loss: 0.53969806432724\n",
      "Epoch [352/500], Loss: 0.5397\n",
      "total loss: 0.5395963191986084\n",
      "Epoch [353/500], Loss: 0.5396\n",
      "total loss: 0.5406339764595032\n",
      "Epoch [354/500], Loss: 0.5406\n",
      "total loss: 0.5406348705291748\n",
      "Epoch [355/500], Loss: 0.5406\n",
      "total loss: 0.540632426738739\n",
      "Epoch [356/500], Loss: 0.5406\n",
      "total loss: 0.5406263470649719\n",
      "Epoch [357/500], Loss: 0.5406\n",
      "total loss: 0.5406429171562195\n",
      "Epoch [358/500], Loss: 0.5406\n",
      "total loss: 0.541098415851593\n",
      "Epoch [359/500], Loss: 0.5411\n",
      "total loss: 0.5405467748641968\n",
      "Epoch [360/500], Loss: 0.5405\n",
      "total loss: 0.5405058860778809\n",
      "Epoch [361/500], Loss: 0.5405\n",
      "total loss: 0.5404742360115051\n",
      "Epoch [362/500], Loss: 0.5405\n",
      "total loss: 0.5404506325721741\n",
      "Epoch [363/500], Loss: 0.5405\n",
      "total loss: 0.5404337048530579\n",
      "Epoch [364/500], Loss: 0.5404\n",
      "total loss: 0.5394240021705627\n",
      "Epoch [365/500], Loss: 0.5394\n",
      "total loss: 0.5398468375205994\n",
      "Epoch [366/500], Loss: 0.5398\n",
      "total loss: 0.5394140481948853\n",
      "Epoch [367/500], Loss: 0.5394\n",
      "total loss: 0.5393897294998169\n",
      "Epoch [368/500], Loss: 0.5394\n",
      "total loss: 0.5393787026405334\n",
      "Epoch [369/500], Loss: 0.5394\n",
      "total loss: 0.539369523525238\n",
      "Epoch [370/500], Loss: 0.5394\n",
      "total loss: 0.5393617749214172\n",
      "Epoch [371/500], Loss: 0.5394\n",
      "total loss: 0.5393528938293457\n",
      "Epoch [372/500], Loss: 0.5394\n",
      "total loss: 0.540341854095459\n",
      "Epoch [373/500], Loss: 0.5403\n",
      "total loss: 0.5401051044464111\n",
      "Epoch [374/500], Loss: 0.5401\n",
      "total loss: 0.5393328666687012\n",
      "Epoch [375/500], Loss: 0.5393\n",
      "total loss: 0.5393357872962952\n",
      "Epoch [376/500], Loss: 0.5393\n",
      "total loss: 0.5393408536911011\n",
      "Epoch [377/500], Loss: 0.5393\n",
      "total loss: 0.5393479466438293\n",
      "Epoch [378/500], Loss: 0.5393\n",
      "total loss: 0.5393559336662292\n",
      "Epoch [379/500], Loss: 0.5394\n",
      "total loss: 0.5393654108047485\n",
      "Epoch [380/500], Loss: 0.5394\n",
      "total loss: 0.539374589920044\n",
      "Epoch [381/500], Loss: 0.5394\n",
      "total loss: 0.5393831729888916\n",
      "Epoch [382/500], Loss: 0.5394\n",
      "total loss: 0.5393905639648438\n",
      "Epoch [383/500], Loss: 0.5394\n",
      "total loss: 0.5393962264060974\n",
      "Epoch [384/500], Loss: 0.5394\n",
      "total loss: 0.5393998026847839\n",
      "Epoch [385/500], Loss: 0.5394\n",
      "total loss: 0.5394012331962585\n",
      "Epoch [386/500], Loss: 0.5394\n",
      "total loss: 0.5394006967544556\n",
      "Epoch [387/500], Loss: 0.5394\n",
      "total loss: 0.5393982529640198\n",
      "Epoch [388/500], Loss: 0.5394\n",
      "total loss: 0.5393940210342407\n",
      "Epoch [389/500], Loss: 0.5394\n",
      "total loss: 0.5393882989883423\n",
      "Epoch [390/500], Loss: 0.5394\n",
      "total loss: 0.5393812656402588\n",
      "Epoch [391/500], Loss: 0.5394\n",
      "total loss: 0.5393733978271484\n",
      "Epoch [392/500], Loss: 0.5394\n",
      "total loss: 0.5393649935722351\n",
      "Epoch [393/500], Loss: 0.5394\n",
      "total loss: 0.5393561720848083\n",
      "Epoch [394/500], Loss: 0.5394\n",
      "total loss: 0.5393474102020264\n",
      "Epoch [395/500], Loss: 0.5393\n",
      "total loss: 0.5393385291099548\n",
      "Epoch [396/500], Loss: 0.5393\n",
      "total loss: 0.5393298864364624\n",
      "Epoch [397/500], Loss: 0.5393\n",
      "total loss: 0.5393214821815491\n",
      "Epoch [398/500], Loss: 0.5393\n",
      "total loss: 0.5393133759498596\n",
      "Epoch [399/500], Loss: 0.5393\n",
      "total loss: 0.5393052697181702\n",
      "Epoch [400/500], Loss: 0.5393\n",
      "total loss: 0.5392974019050598\n",
      "Epoch [401/500], Loss: 0.5393\n",
      "total loss: 0.5392897725105286\n",
      "Epoch [402/500], Loss: 0.5393\n",
      "total loss: 0.5392823219299316\n",
      "Epoch [403/500], Loss: 0.5393\n",
      "total loss: 0.5392749905586243\n",
      "Epoch [404/500], Loss: 0.5393\n",
      "total loss: 0.5392677783966064\n",
      "Epoch [405/500], Loss: 0.5393\n",
      "total loss: 0.5392606258392334\n",
      "Epoch [406/500], Loss: 0.5393\n",
      "total loss: 0.5392537713050842\n",
      "Epoch [407/500], Loss: 0.5393\n",
      "total loss: 0.5392469167709351\n",
      "Epoch [408/500], Loss: 0.5392\n",
      "total loss: 0.5392401814460754\n",
      "Epoch [409/500], Loss: 0.5392\n",
      "total loss: 0.5392341017723083\n",
      "Epoch [410/500], Loss: 0.5392\n",
      "total loss: 0.5392287373542786\n",
      "Epoch [411/500], Loss: 0.5392\n",
      "total loss: 0.5392236113548279\n",
      "Epoch [412/500], Loss: 0.5392\n",
      "total loss: 0.5392186045646667\n",
      "Epoch [413/500], Loss: 0.5392\n",
      "total loss: 0.5392137169837952\n",
      "Epoch [414/500], Loss: 0.5392\n",
      "total loss: 0.5392090678215027\n",
      "Epoch [415/500], Loss: 0.5392\n",
      "total loss: 0.539204478263855\n",
      "Epoch [416/500], Loss: 0.5392\n",
      "total loss: 0.5392000675201416\n",
      "Epoch [417/500], Loss: 0.5392\n",
      "total loss: 0.5391958951950073\n",
      "Epoch [418/500], Loss: 0.5392\n",
      "total loss: 0.5391919612884521\n",
      "Epoch [419/500], Loss: 0.5392\n",
      "total loss: 0.5391879677772522\n",
      "Epoch [420/500], Loss: 0.5392\n",
      "total loss: 0.5391841530799866\n",
      "Epoch [421/500], Loss: 0.5392\n",
      "total loss: 0.5391804575920105\n",
      "Epoch [422/500], Loss: 0.5392\n",
      "total loss: 0.5391773581504822\n",
      "Epoch [423/500], Loss: 0.5392\n",
      "total loss: 0.5391743183135986\n",
      "Epoch [424/500], Loss: 0.5392\n",
      "total loss: 0.5391713380813599\n",
      "Epoch [425/500], Loss: 0.5392\n",
      "total loss: 0.5391684174537659\n",
      "Epoch [426/500], Loss: 0.5392\n",
      "total loss: 0.5391655564308167\n",
      "Epoch [427/500], Loss: 0.5392\n",
      "total loss: 0.5391626358032227\n",
      "Epoch [428/500], Loss: 0.5392\n",
      "total loss: 0.5391598343849182\n",
      "Epoch [429/500], Loss: 0.5392\n",
      "total loss: 0.5391570329666138\n",
      "Epoch [430/500], Loss: 0.5392\n",
      "total loss: 0.5391544103622437\n",
      "Epoch [431/500], Loss: 0.5392\n",
      "total loss: 0.539151668548584\n",
      "Epoch [432/500], Loss: 0.5392\n",
      "total loss: 0.5391490459442139\n",
      "Epoch [433/500], Loss: 0.5391\n",
      "total loss: 0.5391465425491333\n",
      "Epoch [434/500], Loss: 0.5391\n",
      "total loss: 0.5391440391540527\n",
      "Epoch [435/500], Loss: 0.5391\n",
      "total loss: 0.5391415357589722\n",
      "Epoch [436/500], Loss: 0.5391\n",
      "total loss: 0.5391390919685364\n",
      "Epoch [437/500], Loss: 0.5391\n",
      "total loss: 0.5391367077827454\n",
      "Epoch [438/500], Loss: 0.5391\n",
      "total loss: 0.5391343832015991\n",
      "Epoch [439/500], Loss: 0.5391\n",
      "total loss: 0.5391320586204529\n",
      "Epoch [440/500], Loss: 0.5391\n",
      "total loss: 0.5391298532485962\n",
      "Epoch [441/500], Loss: 0.5391\n",
      "total loss: 0.5391276478767395\n",
      "Epoch [442/500], Loss: 0.5391\n",
      "total loss: 0.5391254425048828\n",
      "Epoch [443/500], Loss: 0.5391\n",
      "total loss: 0.5391233563423157\n",
      "Epoch [444/500], Loss: 0.5391\n",
      "total loss: 0.539121150970459\n",
      "Epoch [445/500], Loss: 0.5391\n",
      "total loss: 0.5391190648078918\n",
      "Epoch [446/500], Loss: 0.5391\n",
      "total loss: 0.5391170382499695\n",
      "Epoch [447/500], Loss: 0.5391\n",
      "total loss: 0.5391150116920471\n",
      "Epoch [448/500], Loss: 0.5391\n",
      "total loss: 0.5391131043434143\n",
      "Epoch [449/500], Loss: 0.5391\n",
      "total loss: 0.5391111373901367\n",
      "Epoch [450/500], Loss: 0.5391\n",
      "total loss: 0.5391091704368591\n",
      "Epoch [451/500], Loss: 0.5391\n",
      "total loss: 0.5391073226928711\n",
      "Epoch [452/500], Loss: 0.5391\n",
      "total loss: 0.5391054749488831\n",
      "Epoch [453/500], Loss: 0.5391\n",
      "total loss: 0.539103627204895\n",
      "Epoch [454/500], Loss: 0.5391\n",
      "total loss: 0.5391018390655518\n",
      "Epoch [455/500], Loss: 0.5391\n",
      "total loss: 0.5391000509262085\n",
      "Epoch [456/500], Loss: 0.5391\n",
      "total loss: 0.5390982627868652\n",
      "Epoch [457/500], Loss: 0.5391\n",
      "total loss: 0.5390965342521667\n",
      "Epoch [458/500], Loss: 0.5391\n",
      "total loss: 0.539094865322113\n",
      "Epoch [459/500], Loss: 0.5391\n",
      "total loss: 0.5390931367874146\n",
      "Epoch [460/500], Loss: 0.5391\n",
      "total loss: 0.5390914678573608\n",
      "Epoch [461/500], Loss: 0.5391\n",
      "total loss: 0.5390898585319519\n",
      "Epoch [462/500], Loss: 0.5391\n",
      "total loss: 0.5390881896018982\n",
      "Epoch [463/500], Loss: 0.5391\n",
      "total loss: 0.5390865802764893\n",
      "Epoch [464/500], Loss: 0.5391\n",
      "total loss: 0.5390850305557251\n",
      "Epoch [465/500], Loss: 0.5391\n",
      "total loss: 0.5390834212303162\n",
      "Epoch [466/500], Loss: 0.5391\n",
      "total loss: 0.5390819311141968\n",
      "Epoch [467/500], Loss: 0.5391\n",
      "total loss: 0.5390803813934326\n",
      "Epoch [468/500], Loss: 0.5391\n",
      "total loss: 0.5390788316726685\n",
      "Epoch [469/500], Loss: 0.5391\n",
      "total loss: 0.5390774011611938\n",
      "Epoch [470/500], Loss: 0.5391\n",
      "total loss: 0.5390758514404297\n",
      "Epoch [471/500], Loss: 0.5391\n",
      "total loss: 0.5390744209289551\n",
      "Epoch [472/500], Loss: 0.5391\n",
      "total loss: 0.5390729904174805\n",
      "Epoch [473/500], Loss: 0.5391\n",
      "total loss: 0.5390716195106506\n",
      "Epoch [474/500], Loss: 0.5391\n",
      "total loss: 0.539070188999176\n",
      "Epoch [475/500], Loss: 0.5391\n",
      "total loss: 0.5390687584877014\n",
      "Epoch [476/500], Loss: 0.5391\n",
      "total loss: 0.5390674471855164\n",
      "Epoch [477/500], Loss: 0.5391\n",
      "total loss: 0.5390660166740417\n",
      "Epoch [478/500], Loss: 0.5391\n",
      "total loss: 0.5390646457672119\n",
      "Epoch [479/500], Loss: 0.5391\n",
      "total loss: 0.5390633344650269\n",
      "Epoch [480/500], Loss: 0.5391\n",
      "total loss: 0.5390620231628418\n",
      "Epoch [481/500], Loss: 0.5391\n",
      "total loss: 0.5390607118606567\n",
      "Epoch [482/500], Loss: 0.5391\n",
      "total loss: 0.5390594601631165\n",
      "Epoch [483/500], Loss: 0.5391\n",
      "total loss: 0.5390580892562866\n",
      "Epoch [484/500], Loss: 0.5391\n",
      "total loss: 0.5390568971633911\n",
      "Epoch [485/500], Loss: 0.5391\n",
      "total loss: 0.5390556454658508\n",
      "Epoch [486/500], Loss: 0.5391\n",
      "total loss: 0.5390544533729553\n",
      "Epoch [487/500], Loss: 0.5391\n",
      "total loss: 0.539053201675415\n",
      "Epoch [488/500], Loss: 0.5391\n",
      "total loss: 0.5390520691871643\n",
      "Epoch [489/500], Loss: 0.5391\n",
      "total loss: 0.5390506982803345\n",
      "Epoch [490/500], Loss: 0.5391\n",
      "total loss: 0.5390496253967285\n",
      "Epoch [491/500], Loss: 0.5390\n",
      "total loss: 0.5390483736991882\n",
      "Epoch [492/500], Loss: 0.5390\n",
      "total loss: 0.5390472412109375\n",
      "Epoch [493/500], Loss: 0.5390\n",
      "total loss: 0.5390461087226868\n",
      "Epoch [494/500], Loss: 0.5390\n",
      "total loss: 0.539044976234436\n",
      "Epoch [495/500], Loss: 0.5390\n",
      "total loss: 0.5390438437461853\n",
      "Epoch [496/500], Loss: 0.5390\n",
      "total loss: 0.5390427112579346\n",
      "Epoch [497/500], Loss: 0.5390\n",
      "total loss: 0.5390416383743286\n",
      "Epoch [498/500], Loss: 0.5390\n",
      "total loss: 0.5390405058860779\n",
      "Epoch [499/500], Loss: 0.5390\n",
      "total loss: 0.5390394926071167\n",
      "Epoch [500/500], Loss: 0.5390\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Train the model\n",
    "trained_model = train_model(input_data, desired_output, 500, viz_accuracy=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(trained_model, new_data, expected_result):\n",
    "    test_input = torch.tensor(new_data, dtype=torch.float32)\n",
    "\n",
    "    test_output = trained_model(test_input)\n",
    "    print(test_output)\n",
    "\n",
    "    predicted_classes = torch.argmax(test_output, dim=1)\n",
    "    print(predicted_classes)\n",
    "\n",
    "    test_output_compare = torch.tensor(expected_result, dtype=torch.int)\n",
    "    print(test_output_compare)\n",
    "\n",
    "    return float(sum(test_output_compare == predicted_classes)/len(test_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000e+00, 1.0000e+00],\n",
      "        [1.0000e+00, 4.4735e-09],\n",
      "        [1.0000e+00, 0.0000e+00],\n",
      "        ...,\n",
      "        [1.0000e+00, 0.0000e+00],\n",
      "        [1.0000e+00, 8.3034e-07],\n",
      "        [1.0000e+00, 0.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,\n",
      "        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,\n",
      "        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,\n",
      "        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
      "        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,\n",
      "        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n",
      "        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,\n",
      "        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,\n",
      "        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,\n",
      "        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,\n",
      "        1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,\n",
      "        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,\n",
      "        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,\n",
      "        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,\n",
      "        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,\n",
      "        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
      "        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
      "        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,\n",
      "        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,\n",
      "        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,\n",
      "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
      "        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,\n",
      "        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,\n",
      "        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,\n",
      "        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,\n",
      "        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n",
      "        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,\n",
      "        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
      "        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,\n",
      "        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
      "        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
      "        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,\n",
      "        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,\n",
      "        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
      "        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,\n",
      "        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,\n",
      "        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,\n",
      "        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,\n",
      "        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,\n",
      "        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,\n",
      "        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
      "        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,\n",
      "        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "        0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
      "        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,\n",
      "        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
      "        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
      "        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n",
      "        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,\n",
      "        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,\n",
      "        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
      "        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1], dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xs/6yx8fshd7_v5fg9sl09cfpdh0000gn/T/ipykernel_16955/2208809294.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_output_compare = torch.tensor(expected_result, dtype=torch.int)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7689999938011169"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"matrices_test.csv\")\n",
    "just_input_test = test_df.drop('last', axis=1)\n",
    "input_data_test = np.array(just_input_test.values.tolist())\n",
    "desired_output = torch.tensor(test_df['last'].tolist(), dtype=torch.float32).long()\n",
    "\n",
    "test_model(trained_model, input_data_test, desired_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Model's results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3  3  2]\n",
      " [-1 -4  1]\n",
      " [-1 12 -8]]\n",
      "tensor([ 3.,  3.,  2., -1., -4.,  1., -1., 12., -8.])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/sophiacohen/MXM-AI-Fall-2023/SL3_Z/Neural_Nets/two-mat-consecutives.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sophiacohen/MXM-AI-Fall-2023/SL3_Z/Neural_Nets/two-mat-consecutives.ipynb#X14sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m iteration \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sophiacohen/MXM-AI-Fall-2023/SL3_Z/Neural_Nets/two-mat-consecutives.ipynb#X14sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mwhile\u001b[39;00m iteration \u001b[39m<\u001b[39m \u001b[39m100\u001b[39m: \n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sophiacohen/MXM-AI-Fall-2023/SL3_Z/Neural_Nets/two-mat-consecutives.ipynb#X14sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     prediction \u001b[39m=\u001b[39m trained_model(input_tensor)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/Users/sophiacohen/MXM-AI-Fall-2023/SL3_Z/Neural_Nets/two-mat-consecutives.ipynb Cell 10\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sophiacohen/MXM-AI-Fall-2023/SL3_Z/Neural_Nets/two-mat-consecutives.ipynb#X14sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep6(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sophiacohen/MXM-AI-Fall-2023/SL3_Z/Neural_Nets/two-mat-consecutives.ipynb#X14sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep7(x)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sophiacohen/MXM-AI-Fall-2023/SL3_Z/Neural_Nets/two-mat-consecutives.ipynb#X14sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep8(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sophiacohen/MXM-AI-Fall-2023/SL3_Z/Neural_Nets/two-mat-consecutives.ipynb#X14sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/activation.py:1514\u001b[0m, in \u001b[0;36mSoftmax.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m   1513\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1514\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49msoftmax(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdim, _stacklevel\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/functional.py:1856\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1854\u001b[0m     dim \u001b[39m=\u001b[39m _get_softmax_dim(\u001b[39m\"\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim(), _stacklevel)\n\u001b[1;32m   1855\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1856\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49msoftmax(dim)\n\u001b[1;32m   1857\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1858\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msoftmax(dim, dtype\u001b[39m=\u001b[39mdtype)\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "# Generate random matrix in SL3Z \n",
    "\n",
    "x = np.array([[1,0,1],[0,-1,-1],[0,1,0]])\n",
    "y = np.array([[0,1,0],[0,0,1],[1,0,0]])\n",
    "z = np.array([[0,1,0],[1,0,0],[-1,-1,-1]])\n",
    "mat = ([[1,0,0],[0,1,0],[0,0,1]])\n",
    "\n",
    "for i in range(200):\n",
    "    coin = random.randint(0,2)\n",
    "    if coin == 0: \n",
    "        mat = np.matmul(mat, x)\n",
    "    elif coin == 1: \n",
    "        mat = np.matmul(mat, y)\n",
    "    elif coin == 2: \n",
    "        mat = np.matmul(mat, z)\n",
    "\n",
    "input_array = np.array([mat[0][0],mat[0][1], mat[0][2], mat[1][0],mat[1][1],mat[1][2],mat[2][0],mat[2][1], mat[2][2]] )\n",
    "input_tensor = torch.tensor(input_array, dtype=torch.float32)\n",
    "print(mat)\n",
    "print(input_tensor)\n",
    "iteration = 0\n",
    "while iteration < 100: \n",
    "    prediction = trained_model(input_tensor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
