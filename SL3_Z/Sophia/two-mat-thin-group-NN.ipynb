{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(string): \n",
    "    a = np.array([[1, 1, 2],[0, 1, 1],[0,-3,-2]])\n",
    "    b = np.array([[-2, 0, -1],[-5, 1, -1],[3,0,1]])\n",
    "\n",
    "    maxMats = 50\n",
    "    dataPoints = 10000\n",
    "    prevMat = -1\n",
    "\n",
    "\n",
    "    for j in range(dataPoints):\n",
    "        nextMat = random.randint(0,1)\n",
    "        if nextMat == 0: \n",
    "            mat = a\n",
    "            prevMat = 0\n",
    "        elif nextMat == 1:\n",
    "            mat = b\n",
    "            prevMat = 1\n",
    "        numOfMatrices = random.randint(1, maxMats)\n",
    "        for i in range(numOfMatrices): \n",
    "            nextMat = random.randint(0,1)\n",
    "            if (nextMat == 0):\n",
    "                mat = np.matmul(a, mat)\n",
    "                prevMat = nextMat\n",
    "            elif (nextMat == 1):\n",
    "                mat = np.matmul(b, mat)\n",
    "                prevMat = nextMat\n",
    "        if j == 0: \n",
    "            df = pd.DataFrame([[mat[0][0],mat[0][1],mat[0][2], mat[1][0],mat[1][1],mat[1][2],mat[2][0],mat[2][1],mat[2][2], prevMat]], columns = ['00', '01', '02','10','11','12', '20', '21','22',\"last\"])\n",
    "        else: \n",
    "            df = pd.concat([df, pd.DataFrame([[mat[0][0],mat[0][1],mat[0][2], mat[1][0],mat[1][1],mat[1][2],mat[2][0],mat[2][1],mat[2][2], prevMat]],columns = ['00', '01', '02','10','11','12', '20', '21','22',\"last\"])])\n",
    "# mat[0][0] = mat[0][0]  % 7\n",
    "# mat [0][1] = mat[0][1]  % 7\n",
    "# mat[0][2] = mat[0][2]  % 7\n",
    "# mat [1][0] = mat[1][0]  % 7\n",
    "# mat [1][1] = mat[1][1]  % 7\n",
    "# mat [1][1] = mat[1][1]  % 7\n",
    "    df.to_csv(string, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code adapted from ChatGPT\n",
    "\n",
    "# Define your neural network model\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "\n",
    "        self.step1 = nn.Linear(9, 128, bias=True)\n",
    "        self.step2 = nn.ReLU()\n",
    "        self.step3 = nn.Linear(128, 64, bias=True)\n",
    "        self.step4 = nn.ReLU()\n",
    "        self.step5 = nn.Linear(64, 16, bias=True)\n",
    "        self.step6 = nn.ReLU()\n",
    "        self.step7 = nn.Linear(16, 1, bias=True)\n",
    "        self.step8 = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # RUN IT ON A GPU if it exists\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.to(\"cuda\")\n",
    "\n",
    "        x = self.step1(x)\n",
    "        x = self.step2(x)\n",
    "        x = self.step3(x)\n",
    "        x = self.step4(x)\n",
    "        x = self.step5(x)\n",
    "        x = self.step6(x)\n",
    "        x = self.step7(x)\n",
    "        x = self.step8(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def train_model(inputs, desired_outputs, num_epochs=100, learning_rate=0.02, momentum=0.9, viz_accuracy=False):\n",
    "    # Convert inputs and desired_outputs to PyTorch tensors\n",
    "    inputs = torch.tensor(inputs, dtype=torch.float32)\n",
    "    desired_outputs = torch.tensor(desired_outputs, dtype=torch.float32)\n",
    "    \n",
    "    # Create a DataLoader to handle batching (if needed)\n",
    "    dataset = TensorDataset(inputs, desired_outputs)\n",
    "    dataloader = DataLoader(dataset, batch_size=1000, shuffle=True)  # Adjust batch_size as needed\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = SimpleModel()\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.MSELoss()  # Mean Squared Error loss\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "    losses = []\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for batch_inputs, batch_desired_outputs in dataloader:\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            outputs = model(batch_inputs)  # Forward pass\n",
    "            loss = criterion(outputs, batch_desired_outputs)  # Compute the loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update the model's parameters\n",
    "            cur_item = loss.item()\n",
    "            total_loss += cur_item\n",
    "        \n",
    "        # Print the average loss for this epoch\n",
    "        print(f\"total loss: {total_loss}\")\n",
    "        average_loss = total_loss / len(dataloader)\n",
    "        if viz_accuracy:\n",
    "            losses.append(average_loss)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.4f}')\n",
    "    \n",
    "    if viz_accuracy:\n",
    "        plt.scatter(x=range(1, len(losses)+1), y=losses)\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Average loss\")\n",
    "        plt.show()\n",
    "\n",
    "    # Return the trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate('matrices_train.csv')\n",
    "generate('matrices_test.csv')\n",
    "df = pd.read_csv(\"matrices_train.csv\")\n",
    "just_input = df.drop('last', axis=1)\n",
    "\n",
    "# adapted from https://stackoverflow.com/questions/43898035/pandas-combine-column-values-into-a-list-in-a-new-column\n",
    "input_data = np.array(just_input.values.tolist())\n",
    "desired_output = torch.tensor(df['last'].tolist(), dtype=torch.float32).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xs/6yx8fshd7_v5fg9sl09cfpdh0000gn/T/ipykernel_77585/3187993261.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  desired_outputs = torch.tensor(desired_outputs, dtype=torch.float32)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1000])) that is different to the input size (torch.Size([1000, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total loss: 3.66562220454216\n",
      "Epoch [1/500], Loss: 0.3666\n",
      "total loss: 3.6760697960853577\n",
      "Epoch [2/500], Loss: 0.3676\n",
      "total loss: 3.392651081085205\n",
      "Epoch [3/500], Loss: 0.3393\n",
      "total loss: 2.7086480259895325\n",
      "Epoch [4/500], Loss: 0.2709\n",
      "total loss: 2.5005653649568558\n",
      "Epoch [5/500], Loss: 0.2501\n",
      "total loss: 2.5002222806215286\n",
      "Epoch [6/500], Loss: 0.2500\n",
      "total loss: 2.5000112801790237\n",
      "Epoch [7/500], Loss: 0.2500\n",
      "total loss: 2.4998519718647003\n",
      "Epoch [8/500], Loss: 0.2500\n",
      "total loss: 2.499888926744461\n",
      "Epoch [9/500], Loss: 0.2500\n",
      "total loss: 2.499765083193779\n",
      "Epoch [10/500], Loss: 0.2500\n",
      "total loss: 2.49977245926857\n",
      "Epoch [11/500], Loss: 0.2500\n",
      "total loss: 2.499749854207039\n",
      "Epoch [12/500], Loss: 0.2500\n",
      "total loss: 2.4997647404670715\n",
      "Epoch [13/500], Loss: 0.2500\n",
      "total loss: 2.4997446537017822\n",
      "Epoch [14/500], Loss: 0.2500\n",
      "total loss: 2.499747484922409\n",
      "Epoch [15/500], Loss: 0.2500\n",
      "total loss: 2.4997736364603043\n",
      "Epoch [16/500], Loss: 0.2500\n",
      "total loss: 2.4997455775737762\n",
      "Epoch [17/500], Loss: 0.2500\n",
      "total loss: 2.4997431188821793\n",
      "Epoch [18/500], Loss: 0.2500\n",
      "total loss: 2.4997478723526\n",
      "Epoch [19/500], Loss: 0.2500\n",
      "total loss: 2.4997488856315613\n",
      "Epoch [20/500], Loss: 0.2500\n",
      "total loss: 2.499751254916191\n",
      "Epoch [21/500], Loss: 0.2500\n",
      "total loss: 2.499750182032585\n",
      "Epoch [22/500], Loss: 0.2500\n",
      "total loss: 2.4997474402189255\n",
      "Epoch [23/500], Loss: 0.2500\n",
      "total loss: 2.499748945236206\n",
      "Epoch [24/500], Loss: 0.2500\n",
      "total loss: 2.4997449666261673\n",
      "Epoch [25/500], Loss: 0.2500\n",
      "total loss: 2.499744191765785\n",
      "Epoch [26/500], Loss: 0.2500\n",
      "total loss: 2.499768078327179\n",
      "Epoch [27/500], Loss: 0.2500\n",
      "total loss: 2.4997548609972\n",
      "Epoch [28/500], Loss: 0.2500\n",
      "total loss: 2.4997566044330597\n",
      "Epoch [29/500], Loss: 0.2500\n",
      "total loss: 2.499744474887848\n",
      "Epoch [30/500], Loss: 0.2500\n",
      "total loss: 2.499745652079582\n",
      "Epoch [31/500], Loss: 0.2500\n",
      "total loss: 2.4997564554214478\n",
      "Epoch [32/500], Loss: 0.2500\n",
      "total loss: 2.4997517615556717\n",
      "Epoch [33/500], Loss: 0.2500\n",
      "total loss: 2.4997546523809433\n",
      "Epoch [34/500], Loss: 0.2500\n",
      "total loss: 2.499748542904854\n",
      "Epoch [35/500], Loss: 0.2500\n",
      "total loss: 2.4997538328170776\n",
      "Epoch [36/500], Loss: 0.2500\n",
      "total loss: 2.4997565299272537\n",
      "Epoch [37/500], Loss: 0.2500\n",
      "total loss: 2.499748155474663\n",
      "Epoch [38/500], Loss: 0.2500\n",
      "total loss: 2.4997797906398773\n",
      "Epoch [39/500], Loss: 0.2500\n",
      "total loss: 2.499749258160591\n",
      "Epoch [40/500], Loss: 0.2500\n",
      "total loss: 2.4997483044862747\n",
      "Epoch [41/500], Loss: 0.2500\n",
      "total loss: 2.4997493475675583\n",
      "Epoch [42/500], Loss: 0.2500\n",
      "total loss: 2.4997588992118835\n",
      "Epoch [43/500], Loss: 0.2500\n",
      "total loss: 2.4997493028640747\n",
      "Epoch [44/500], Loss: 0.2500\n",
      "total loss: 2.4997571408748627\n",
      "Epoch [45/500], Loss: 0.2500\n",
      "total loss: 2.499784752726555\n",
      "Epoch [46/500], Loss: 0.2500\n",
      "total loss: 2.499791294336319\n",
      "Epoch [47/500], Loss: 0.2500\n",
      "total loss: 2.499767780303955\n",
      "Epoch [48/500], Loss: 0.2500\n",
      "total loss: 2.4997775107622147\n",
      "Epoch [49/500], Loss: 0.2500\n",
      "total loss: 2.4997617453336716\n",
      "Epoch [50/500], Loss: 0.2500\n",
      "total loss: 2.4997581839561462\n",
      "Epoch [51/500], Loss: 0.2500\n",
      "total loss: 2.4997697323560715\n",
      "Epoch [52/500], Loss: 0.2500\n",
      "total loss: 2.499762326478958\n",
      "Epoch [53/500], Loss: 0.2500\n",
      "total loss: 2.4997591823339462\n",
      "Epoch [54/500], Loss: 0.2500\n",
      "total loss: 2.499758079648018\n",
      "Epoch [55/500], Loss: 0.2500\n",
      "total loss: 2.499770313501358\n",
      "Epoch [56/500], Loss: 0.2500\n",
      "total loss: 2.4997521191835403\n",
      "Epoch [57/500], Loss: 0.2500\n",
      "total loss: 2.4997457712888718\n",
      "Epoch [58/500], Loss: 0.2500\n",
      "total loss: 2.4997537285089493\n",
      "Epoch [59/500], Loss: 0.2500\n",
      "total loss: 2.4997809380292892\n",
      "Epoch [60/500], Loss: 0.2500\n",
      "total loss: 2.499768152832985\n",
      "Epoch [61/500], Loss: 0.2500\n",
      "total loss: 2.4997432976961136\n",
      "Epoch [62/500], Loss: 0.2500\n",
      "total loss: 2.4997658878564835\n",
      "Epoch [63/500], Loss: 0.2500\n",
      "total loss: 2.4997569024562836\n",
      "Epoch [64/500], Loss: 0.2500\n",
      "total loss: 2.499743402004242\n",
      "Epoch [65/500], Loss: 0.2500\n",
      "total loss: 2.499746158719063\n",
      "Epoch [66/500], Loss: 0.2500\n",
      "total loss: 2.4997446537017822\n",
      "Epoch [67/500], Loss: 0.2500\n",
      "total loss: 2.499754458665848\n",
      "Epoch [68/500], Loss: 0.2500\n",
      "total loss: 2.4997666478157043\n",
      "Epoch [69/500], Loss: 0.2500\n",
      "total loss: 2.4998243898153305\n",
      "Epoch [70/500], Loss: 0.2500\n",
      "total loss: 2.4997368901968002\n",
      "Epoch [71/500], Loss: 0.2500\n",
      "total loss: 2.4997442811727524\n",
      "Epoch [72/500], Loss: 0.2500\n",
      "total loss: 2.499743729829788\n",
      "Epoch [73/500], Loss: 0.2500\n",
      "total loss: 2.499753013253212\n",
      "Epoch [74/500], Loss: 0.2500\n",
      "total loss: 2.499746188521385\n",
      "Epoch [75/500], Loss: 0.2500\n",
      "total loss: 2.4997547268867493\n",
      "Epoch [76/500], Loss: 0.2500\n",
      "total loss: 2.4997759014368057\n",
      "Epoch [77/500], Loss: 0.2500\n",
      "total loss: 2.499754399061203\n",
      "Epoch [78/500], Loss: 0.2500\n",
      "total loss: 2.4997581094503403\n",
      "Epoch [79/500], Loss: 0.2500\n",
      "total loss: 2.4997557997703552\n",
      "Epoch [80/500], Loss: 0.2500\n",
      "total loss: 2.4997443109750748\n",
      "Epoch [81/500], Loss: 0.2500\n",
      "total loss: 2.4997551292181015\n",
      "Epoch [82/500], Loss: 0.2500\n",
      "total loss: 2.499759301543236\n",
      "Epoch [83/500], Loss: 0.2500\n",
      "total loss: 2.499744400382042\n",
      "Epoch [84/500], Loss: 0.2500\n",
      "total loss: 2.4997436106204987\n",
      "Epoch [85/500], Loss: 0.2500\n",
      "total loss: 2.4997482746839523\n",
      "Epoch [86/500], Loss: 0.2500\n",
      "total loss: 2.4997498691082\n",
      "Epoch [87/500], Loss: 0.2500\n",
      "total loss: 2.499745786190033\n",
      "Epoch [88/500], Loss: 0.2500\n",
      "total loss: 2.4997518062591553\n",
      "Epoch [89/500], Loss: 0.2500\n",
      "total loss: 2.499744340777397\n",
      "Epoch [90/500], Loss: 0.2500\n",
      "total loss: 2.499746710062027\n",
      "Epoch [91/500], Loss: 0.2500\n",
      "total loss: 2.499748095870018\n",
      "Epoch [92/500], Loss: 0.2500\n",
      "total loss: 2.499745875597\n",
      "Epoch [93/500], Loss: 0.2500\n",
      "total loss: 2.4997598826885223\n",
      "Epoch [94/500], Loss: 0.2500\n",
      "total loss: 2.4997596591711044\n",
      "Epoch [95/500], Loss: 0.2500\n",
      "total loss: 2.49975024163723\n",
      "Epoch [96/500], Loss: 0.2500\n",
      "total loss: 2.4997428804636\n",
      "Epoch [97/500], Loss: 0.2500\n",
      "total loss: 2.4997518360614777\n",
      "Epoch [98/500], Loss: 0.2500\n",
      "total loss: 2.4997482299804688\n",
      "Epoch [99/500], Loss: 0.2500\n",
      "total loss: 2.4997675120830536\n",
      "Epoch [100/500], Loss: 0.2500\n",
      "total loss: 2.4997589886188507\n",
      "Epoch [101/500], Loss: 0.2500\n",
      "total loss: 2.4997598975896835\n",
      "Epoch [102/500], Loss: 0.2500\n",
      "total loss: 2.499749705195427\n",
      "Epoch [103/500], Loss: 0.2500\n",
      "total loss: 2.4997833967208862\n",
      "Epoch [104/500], Loss: 0.2500\n",
      "total loss: 2.4997488111257553\n",
      "Epoch [105/500], Loss: 0.2500\n",
      "total loss: 2.4997719824314117\n",
      "Epoch [106/500], Loss: 0.2500\n",
      "total loss: 2.499757617712021\n",
      "Epoch [107/500], Loss: 0.2500\n",
      "total loss: 2.499746784567833\n",
      "Epoch [108/500], Loss: 0.2500\n",
      "total loss: 2.499751776456833\n",
      "Epoch [109/500], Loss: 0.2500\n",
      "total loss: 2.4997608959674835\n",
      "Epoch [110/500], Loss: 0.2500\n",
      "total loss: 2.4998390674591064\n",
      "Epoch [111/500], Loss: 0.2500\n",
      "total loss: 2.4997930973768234\n",
      "Epoch [112/500], Loss: 0.2500\n",
      "total loss: 2.499744087457657\n",
      "Epoch [113/500], Loss: 0.2500\n",
      "total loss: 2.499754548072815\n",
      "Epoch [114/500], Loss: 0.2500\n",
      "total loss: 2.499752014875412\n",
      "Epoch [115/500], Loss: 0.2500\n",
      "total loss: 2.4997676461935043\n",
      "Epoch [116/500], Loss: 0.2500\n",
      "total loss: 2.4997433871030807\n",
      "Epoch [117/500], Loss: 0.2500\n",
      "total loss: 2.4997472912073135\n",
      "Epoch [118/500], Loss: 0.2500\n",
      "total loss: 2.499771550297737\n",
      "Epoch [119/500], Loss: 0.2500\n",
      "total loss: 2.4997441172599792\n",
      "Epoch [120/500], Loss: 0.2500\n",
      "total loss: 2.4997452795505524\n",
      "Epoch [121/500], Loss: 0.2500\n",
      "total loss: 2.4998030215501785\n",
      "Epoch [122/500], Loss: 0.2500\n",
      "total loss: 2.4998065531253815\n",
      "Epoch [123/500], Loss: 0.2500\n",
      "total loss: 2.4997491389513016\n",
      "Epoch [124/500], Loss: 0.2500\n",
      "total loss: 2.4997478425502777\n",
      "Epoch [125/500], Loss: 0.2500\n",
      "total loss: 2.499763458967209\n",
      "Epoch [126/500], Loss: 0.2500\n",
      "total loss: 2.4997429996728897\n",
      "Epoch [127/500], Loss: 0.2500\n",
      "total loss: 2.4997466802597046\n",
      "Epoch [128/500], Loss: 0.2500\n",
      "total loss: 2.4997705668210983\n",
      "Epoch [129/500], Loss: 0.2500\n",
      "total loss: 2.499755710363388\n",
      "Epoch [130/500], Loss: 0.2500\n",
      "total loss: 2.4997556656599045\n",
      "Epoch [131/500], Loss: 0.2500\n",
      "total loss: 2.4997502714395523\n",
      "Epoch [132/500], Loss: 0.2500\n",
      "total loss: 2.4997622221708298\n",
      "Epoch [133/500], Loss: 0.2500\n",
      "total loss: 2.49977870285511\n",
      "Epoch [134/500], Loss: 0.2500\n",
      "total loss: 2.4997421503067017\n",
      "Epoch [135/500], Loss: 0.2500\n",
      "total loss: 2.499763146042824\n",
      "Epoch [136/500], Loss: 0.2500\n",
      "total loss: 2.499748557806015\n",
      "Epoch [137/500], Loss: 0.2500\n",
      "total loss: 2.4997429698705673\n",
      "Epoch [138/500], Loss: 0.2500\n",
      "total loss: 2.4997586756944656\n",
      "Epoch [139/500], Loss: 0.2500\n",
      "total loss: 2.4997436106204987\n",
      "Epoch [140/500], Loss: 0.2500\n",
      "total loss: 2.4997573792934418\n",
      "Epoch [141/500], Loss: 0.2500\n",
      "total loss: 2.499755322933197\n",
      "Epoch [142/500], Loss: 0.2500\n",
      "total loss: 2.4997785687446594\n",
      "Epoch [143/500], Loss: 0.2500\n",
      "total loss: 2.4997558891773224\n",
      "Epoch [144/500], Loss: 0.2500\n",
      "total loss: 2.499815195798874\n",
      "Epoch [145/500], Loss: 0.2500\n",
      "total loss: 2.499752402305603\n",
      "Epoch [146/500], Loss: 0.2500\n",
      "total loss: 2.499773308634758\n",
      "Epoch [147/500], Loss: 0.2500\n",
      "total loss: 2.499751314520836\n",
      "Epoch [148/500], Loss: 0.2500\n",
      "total loss: 2.499759390950203\n",
      "Epoch [149/500], Loss: 0.2500\n",
      "total loss: 2.4997480362653732\n",
      "Epoch [150/500], Loss: 0.2500\n",
      "total loss: 2.4997638761997223\n",
      "Epoch [151/500], Loss: 0.2500\n",
      "total loss: 2.499753475189209\n",
      "Epoch [152/500], Loss: 0.2500\n",
      "total loss: 2.4997622668743134\n",
      "Epoch [153/500], Loss: 0.2500\n",
      "total loss: 2.4997450709342957\n",
      "Epoch [154/500], Loss: 0.2500\n",
      "total loss: 2.4997581839561462\n",
      "Epoch [155/500], Loss: 0.2500\n",
      "total loss: 2.499746799468994\n",
      "Epoch [156/500], Loss: 0.2500\n",
      "total loss: 2.49975423514843\n",
      "Epoch [157/500], Loss: 0.2500\n",
      "total loss: 2.4997479766607285\n",
      "Epoch [158/500], Loss: 0.2500\n",
      "total loss: 2.499750003218651\n",
      "Epoch [159/500], Loss: 0.2500\n",
      "total loss: 2.499748259782791\n",
      "Epoch [160/500], Loss: 0.2500\n",
      "total loss: 2.499749392271042\n",
      "Epoch [161/500], Loss: 0.2500\n",
      "total loss: 2.4997699856758118\n",
      "Epoch [162/500], Loss: 0.2500\n",
      "total loss: 2.4997489005327225\n",
      "Epoch [163/500], Loss: 0.2500\n",
      "total loss: 2.499764233827591\n",
      "Epoch [164/500], Loss: 0.2500\n",
      "total loss: 2.4997586756944656\n",
      "Epoch [165/500], Loss: 0.2500\n",
      "total loss: 2.499745950102806\n",
      "Epoch [166/500], Loss: 0.2500\n",
      "total loss: 2.4997552782297134\n",
      "Epoch [167/500], Loss: 0.2500\n",
      "total loss: 2.499759554862976\n",
      "Epoch [168/500], Loss: 0.2500\n",
      "total loss: 2.49977944791317\n",
      "Epoch [169/500], Loss: 0.2500\n",
      "total loss: 2.499743953347206\n",
      "Epoch [170/500], Loss: 0.2500\n",
      "total loss: 2.499744102358818\n",
      "Epoch [171/500], Loss: 0.2500\n",
      "total loss: 2.499743312597275\n",
      "Epoch [172/500], Loss: 0.2500\n",
      "total loss: 2.499749556183815\n",
      "Epoch [173/500], Loss: 0.2500\n",
      "total loss: 2.4997479766607285\n",
      "Epoch [174/500], Loss: 0.2500\n",
      "total loss: 2.4997559040784836\n",
      "Epoch [175/500], Loss: 0.2500\n",
      "total loss: 2.4997581094503403\n",
      "Epoch [176/500], Loss: 0.2500\n",
      "total loss: 2.4997452050447464\n",
      "Epoch [177/500], Loss: 0.2500\n",
      "total loss: 2.4997535943984985\n",
      "Epoch [178/500], Loss: 0.2500\n",
      "total loss: 2.499765768647194\n",
      "Epoch [179/500], Loss: 0.2500\n",
      "total loss: 2.4997433573007584\n",
      "Epoch [180/500], Loss: 0.2500\n",
      "total loss: 2.499747708439827\n",
      "Epoch [181/500], Loss: 0.2500\n",
      "total loss: 2.499755173921585\n",
      "Epoch [182/500], Loss: 0.2500\n",
      "total loss: 2.499764636158943\n",
      "Epoch [183/500], Loss: 0.2500\n",
      "total loss: 2.499750480055809\n",
      "Epoch [184/500], Loss: 0.2500\n",
      "total loss: 2.4997509121894836\n",
      "Epoch [185/500], Loss: 0.2500\n",
      "total loss: 2.4997956603765488\n",
      "Epoch [186/500], Loss: 0.2500\n",
      "total loss: 2.4997725039720535\n",
      "Epoch [187/500], Loss: 0.2500\n",
      "total loss: 2.4997634887695312\n",
      "Epoch [188/500], Loss: 0.2500\n",
      "total loss: 2.499766081571579\n",
      "Epoch [189/500], Loss: 0.2500\n",
      "total loss: 2.4997562021017075\n",
      "Epoch [190/500], Loss: 0.2500\n",
      "total loss: 2.4997426718473434\n",
      "Epoch [191/500], Loss: 0.2500\n",
      "total loss: 2.4997721910476685\n",
      "Epoch [192/500], Loss: 0.2500\n",
      "total loss: 2.499771937727928\n",
      "Epoch [193/500], Loss: 0.2500\n",
      "total loss: 2.499766394495964\n",
      "Epoch [194/500], Loss: 0.2500\n",
      "total loss: 2.4997586458921432\n",
      "Epoch [195/500], Loss: 0.2500\n",
      "total loss: 2.4997366070747375\n",
      "Epoch [196/500], Loss: 0.2500\n",
      "total loss: 2.4997431933879852\n",
      "Epoch [197/500], Loss: 0.2500\n",
      "total loss: 2.4997841864824295\n",
      "Epoch [198/500], Loss: 0.2500\n",
      "total loss: 2.4997510761022568\n",
      "Epoch [199/500], Loss: 0.2500\n",
      "total loss: 2.4997718036174774\n",
      "Epoch [200/500], Loss: 0.2500\n",
      "total loss: 2.4997705221176147\n",
      "Epoch [201/500], Loss: 0.2500\n",
      "total loss: 2.499757930636406\n",
      "Epoch [202/500], Loss: 0.2500\n",
      "total loss: 2.499755173921585\n",
      "Epoch [203/500], Loss: 0.2500\n",
      "total loss: 2.499751016497612\n",
      "Epoch [204/500], Loss: 0.2500\n",
      "total loss: 2.4997511953115463\n",
      "Epoch [205/500], Loss: 0.2500\n",
      "total loss: 2.4997472763061523\n",
      "Epoch [206/500], Loss: 0.2500\n",
      "total loss: 2.4997731298208237\n",
      "Epoch [207/500], Loss: 0.2500\n",
      "total loss: 2.499750867486\n",
      "Epoch [208/500], Loss: 0.2500\n",
      "total loss: 2.4997505843639374\n",
      "Epoch [209/500], Loss: 0.2500\n",
      "total loss: 2.499744400382042\n",
      "Epoch [210/500], Loss: 0.2500\n",
      "total loss: 2.499756097793579\n",
      "Epoch [211/500], Loss: 0.2500\n",
      "total loss: 2.499758556485176\n",
      "Epoch [212/500], Loss: 0.2500\n",
      "total loss: 2.499755412340164\n",
      "Epoch [213/500], Loss: 0.2500\n",
      "total loss: 2.4997494518756866\n",
      "Epoch [214/500], Loss: 0.2500\n",
      "total loss: 2.499751776456833\n",
      "Epoch [215/500], Loss: 0.2500\n",
      "total loss: 2.4997600615024567\n",
      "Epoch [216/500], Loss: 0.2500\n",
      "total loss: 2.499751538038254\n",
      "Epoch [217/500], Loss: 0.2500\n",
      "total loss: 2.4997574537992477\n",
      "Epoch [218/500], Loss: 0.2500\n",
      "total loss: 2.4997598528862\n",
      "Epoch [219/500], Loss: 0.2500\n",
      "total loss: 2.499754101037979\n",
      "Epoch [220/500], Loss: 0.2500\n",
      "total loss: 2.4997864067554474\n",
      "Epoch [221/500], Loss: 0.2500\n",
      "total loss: 2.4997547268867493\n",
      "Epoch [222/500], Loss: 0.2500\n",
      "total loss: 2.4997670650482178\n",
      "Epoch [223/500], Loss: 0.2500\n",
      "total loss: 2.4997461140155792\n",
      "Epoch [224/500], Loss: 0.2500\n",
      "total loss: 2.4997438341379166\n",
      "Epoch [225/500], Loss: 0.2500\n",
      "total loss: 2.4997453093528748\n",
      "Epoch [226/500], Loss: 0.2500\n",
      "total loss: 2.4997482150793076\n",
      "Epoch [227/500], Loss: 0.2500\n",
      "total loss: 2.4997520744800568\n",
      "Epoch [228/500], Loss: 0.2500\n",
      "total loss: 2.4997467398643494\n",
      "Epoch [229/500], Loss: 0.2500\n",
      "total loss: 2.49974724650383\n",
      "Epoch [230/500], Loss: 0.2500\n",
      "total loss: 2.499753877520561\n",
      "Epoch [231/500], Loss: 0.2500\n",
      "total loss: 2.499744564294815\n",
      "Epoch [232/500], Loss: 0.2500\n",
      "total loss: 2.499761775135994\n",
      "Epoch [233/500], Loss: 0.2500\n",
      "total loss: 2.4997599124908447\n",
      "Epoch [234/500], Loss: 0.2500\n",
      "total loss: 2.499829590320587\n",
      "Epoch [235/500], Loss: 0.2500\n",
      "total loss: 2.499742269515991\n",
      "Epoch [236/500], Loss: 0.2500\n",
      "total loss: 2.4997425079345703\n",
      "Epoch [237/500], Loss: 0.2500\n",
      "total loss: 2.4997538626194\n",
      "Epoch [238/500], Loss: 0.2500\n",
      "total loss: 2.499799370765686\n",
      "Epoch [239/500], Loss: 0.2500\n",
      "total loss: 2.499746397137642\n",
      "Epoch [240/500], Loss: 0.2500\n",
      "total loss: 2.499764323234558\n",
      "Epoch [241/500], Loss: 0.2500\n",
      "total loss: 2.4997490495443344\n",
      "Epoch [242/500], Loss: 0.2500\n",
      "total loss: 2.499767303466797\n",
      "Epoch [243/500], Loss: 0.2500\n",
      "total loss: 2.499746397137642\n",
      "Epoch [244/500], Loss: 0.2500\n",
      "total loss: 2.4997524470090866\n",
      "Epoch [245/500], Loss: 0.2500\n",
      "total loss: 2.4997531920671463\n",
      "Epoch [246/500], Loss: 0.2500\n",
      "total loss: 2.499753087759018\n",
      "Epoch [247/500], Loss: 0.2500\n",
      "total loss: 2.499756321310997\n",
      "Epoch [248/500], Loss: 0.2500\n",
      "total loss: 2.4997655898332596\n",
      "Epoch [249/500], Loss: 0.2500\n",
      "total loss: 2.4997573643922806\n",
      "Epoch [250/500], Loss: 0.2500\n",
      "total loss: 2.4997732639312744\n",
      "Epoch [251/500], Loss: 0.2500\n",
      "total loss: 2.4997498393058777\n",
      "Epoch [252/500], Loss: 0.2500\n",
      "total loss: 2.4997492879629135\n",
      "Epoch [253/500], Loss: 0.2500\n",
      "total loss: 2.499751016497612\n",
      "Epoch [254/500], Loss: 0.2500\n",
      "total loss: 2.4997555017471313\n",
      "Epoch [255/500], Loss: 0.2500\n",
      "total loss: 2.4997546523809433\n",
      "Epoch [256/500], Loss: 0.2500\n",
      "total loss: 2.4997615665197372\n",
      "Epoch [257/500], Loss: 0.2500\n",
      "total loss: 2.499746024608612\n",
      "Epoch [258/500], Loss: 0.2500\n",
      "total loss: 2.499748095870018\n",
      "Epoch [259/500], Loss: 0.2500\n",
      "total loss: 2.4997575730085373\n",
      "Epoch [260/500], Loss: 0.2500\n",
      "total loss: 2.4997402280569077\n",
      "Epoch [261/500], Loss: 0.2500\n",
      "total loss: 2.499782055616379\n",
      "Epoch [262/500], Loss: 0.2500\n",
      "total loss: 2.499743789434433\n",
      "Epoch [263/500], Loss: 0.2500\n",
      "total loss: 2.4997539967298508\n",
      "Epoch [264/500], Loss: 0.2500\n",
      "total loss: 2.499742165207863\n",
      "Epoch [265/500], Loss: 0.2500\n",
      "total loss: 2.4997539967298508\n",
      "Epoch [266/500], Loss: 0.2500\n",
      "total loss: 2.4997497498989105\n",
      "Epoch [267/500], Loss: 0.2500\n",
      "total loss: 2.499755322933197\n",
      "Epoch [268/500], Loss: 0.2500\n",
      "total loss: 2.4997512847185135\n",
      "Epoch [269/500], Loss: 0.2500\n",
      "total loss: 2.499760553240776\n",
      "Epoch [270/500], Loss: 0.2500\n",
      "total loss: 2.4997499138116837\n",
      "Epoch [271/500], Loss: 0.2500\n",
      "total loss: 2.499742954969406\n",
      "Epoch [272/500], Loss: 0.2500\n",
      "total loss: 2.4997494965791702\n",
      "Epoch [273/500], Loss: 0.2500\n",
      "total loss: 2.499751463532448\n",
      "Epoch [274/500], Loss: 0.2500\n",
      "total loss: 2.4997588843107224\n",
      "Epoch [275/500], Loss: 0.2500\n",
      "total loss: 2.499742642045021\n",
      "Epoch [276/500], Loss: 0.2500\n",
      "total loss: 2.499786615371704\n",
      "Epoch [277/500], Loss: 0.2500\n",
      "total loss: 2.4997601211071014\n",
      "Epoch [278/500], Loss: 0.2500\n",
      "total loss: 2.4997559189796448\n",
      "Epoch [279/500], Loss: 0.2500\n",
      "total loss: 2.4997506737709045\n",
      "Epoch [280/500], Loss: 0.2500\n",
      "total loss: 2.499742016196251\n",
      "Epoch [281/500], Loss: 0.2500\n",
      "total loss: 2.4997713416814804\n",
      "Epoch [282/500], Loss: 0.2500\n",
      "total loss: 2.499748781323433\n",
      "Epoch [283/500], Loss: 0.2500\n",
      "total loss: 2.4997483044862747\n",
      "Epoch [284/500], Loss: 0.2500\n",
      "total loss: 2.4997609853744507\n",
      "Epoch [285/500], Loss: 0.2500\n",
      "total loss: 2.49975249171257\n",
      "Epoch [286/500], Loss: 0.2500\n",
      "total loss: 2.4997484534978867\n",
      "Epoch [287/500], Loss: 0.2500\n",
      "total loss: 2.4997542649507523\n",
      "Epoch [288/500], Loss: 0.2500\n",
      "total loss: 2.499744936823845\n",
      "Epoch [289/500], Loss: 0.2500\n",
      "total loss: 2.499753922224045\n",
      "Epoch [290/500], Loss: 0.2500\n",
      "total loss: 2.4997602999210358\n",
      "Epoch [291/500], Loss: 0.2500\n",
      "total loss: 2.4998273998498917\n",
      "Epoch [292/500], Loss: 0.2500\n",
      "total loss: 2.4997508227825165\n",
      "Epoch [293/500], Loss: 0.2500\n",
      "total loss: 2.4998030215501785\n",
      "Epoch [294/500], Loss: 0.2500\n",
      "total loss: 2.4997491985559464\n",
      "Epoch [295/500], Loss: 0.2500\n",
      "total loss: 2.499864339828491\n",
      "Epoch [296/500], Loss: 0.2500\n",
      "total loss: 2.4997849613428116\n",
      "Epoch [297/500], Loss: 0.2500\n",
      "total loss: 2.499749541282654\n",
      "Epoch [298/500], Loss: 0.2500\n",
      "total loss: 2.4997454434633255\n",
      "Epoch [299/500], Loss: 0.2500\n",
      "total loss: 2.4997542798519135\n",
      "Epoch [300/500], Loss: 0.2500\n",
      "total loss: 2.499749571084976\n",
      "Epoch [301/500], Loss: 0.2500\n",
      "total loss: 2.499749794602394\n",
      "Epoch [302/500], Loss: 0.2500\n",
      "total loss: 2.499746009707451\n",
      "Epoch [303/500], Loss: 0.2500\n",
      "total loss: 2.499775618314743\n",
      "Epoch [304/500], Loss: 0.2500\n",
      "total loss: 2.499747261404991\n",
      "Epoch [305/500], Loss: 0.2500\n",
      "total loss: 2.4997593760490417\n",
      "Epoch [306/500], Loss: 0.2500\n",
      "total loss: 2.4997446686029434\n",
      "Epoch [307/500], Loss: 0.2500\n",
      "total loss: 2.4997511208057404\n",
      "Epoch [308/500], Loss: 0.2500\n",
      "total loss: 2.4997416138648987\n",
      "Epoch [309/500], Loss: 0.2500\n",
      "total loss: 2.4997496902942657\n",
      "Epoch [310/500], Loss: 0.2500\n",
      "total loss: 2.499746412038803\n",
      "Epoch [311/500], Loss: 0.2500\n",
      "total loss: 2.4997605979442596\n",
      "Epoch [312/500], Loss: 0.2500\n",
      "total loss: 2.4997638911008835\n",
      "Epoch [313/500], Loss: 0.2500\n",
      "total loss: 2.4997653663158417\n",
      "Epoch [314/500], Loss: 0.2500\n",
      "total loss: 2.499782755970955\n",
      "Epoch [315/500], Loss: 0.2500\n",
      "total loss: 2.499745026230812\n",
      "Epoch [316/500], Loss: 0.2500\n",
      "total loss: 2.4997449219226837\n",
      "Epoch [317/500], Loss: 0.2500\n",
      "total loss: 2.499754309654236\n",
      "Epoch [318/500], Loss: 0.2500\n",
      "total loss: 2.4997501522302628\n",
      "Epoch [319/500], Loss: 0.2500\n",
      "total loss: 2.499759465456009\n",
      "Epoch [320/500], Loss: 0.2500\n",
      "total loss: 2.49977008998394\n",
      "Epoch [321/500], Loss: 0.2500\n",
      "total loss: 2.4997690618038177\n",
      "Epoch [322/500], Loss: 0.2500\n",
      "total loss: 2.4997459203004837\n",
      "Epoch [323/500], Loss: 0.2500\n",
      "total loss: 2.499755769968033\n",
      "Epoch [324/500], Loss: 0.2500\n",
      "total loss: 2.499762147665024\n",
      "Epoch [325/500], Loss: 0.2500\n",
      "total loss: 2.4997434467077255\n",
      "Epoch [326/500], Loss: 0.2500\n",
      "total loss: 2.4997420758008957\n",
      "Epoch [327/500], Loss: 0.2500\n",
      "total loss: 2.4997464418411255\n",
      "Epoch [328/500], Loss: 0.2500\n",
      "total loss: 2.4997581392526627\n",
      "Epoch [329/500], Loss: 0.2500\n",
      "total loss: 2.4997539073228836\n",
      "Epoch [330/500], Loss: 0.2500\n",
      "total loss: 2.499745190143585\n",
      "Epoch [331/500], Loss: 0.2500\n",
      "total loss: 2.49977008998394\n",
      "Epoch [332/500], Loss: 0.2500\n",
      "total loss: 2.499745711684227\n",
      "Epoch [333/500], Loss: 0.2500\n",
      "total loss: 2.4997657239437103\n",
      "Epoch [334/500], Loss: 0.2500\n",
      "total loss: 2.4997530579566956\n",
      "Epoch [335/500], Loss: 0.2500\n",
      "total loss: 2.499751165509224\n",
      "Epoch [336/500], Loss: 0.2500\n",
      "total loss: 2.499746337532997\n",
      "Epoch [337/500], Loss: 0.2500\n",
      "total loss: 2.499755874276161\n",
      "Epoch [338/500], Loss: 0.2500\n",
      "total loss: 2.4997472167015076\n",
      "Epoch [339/500], Loss: 0.2500\n",
      "total loss: 2.4997493773698807\n",
      "Epoch [340/500], Loss: 0.2500\n",
      "total loss: 2.499752774834633\n",
      "Epoch [341/500], Loss: 0.2500\n",
      "total loss: 2.4997527450323105\n",
      "Epoch [342/500], Loss: 0.2500\n",
      "total loss: 2.49978668987751\n",
      "Epoch [343/500], Loss: 0.2500\n",
      "total loss: 2.4997464418411255\n",
      "Epoch [344/500], Loss: 0.2500\n",
      "total loss: 2.499841198325157\n",
      "Epoch [345/500], Loss: 0.2500\n",
      "total loss: 2.4997438341379166\n",
      "Epoch [346/500], Loss: 0.2500\n",
      "total loss: 2.499896824359894\n",
      "Epoch [347/500], Loss: 0.2500\n",
      "total loss: 2.499766692519188\n",
      "Epoch [348/500], Loss: 0.2500\n",
      "total loss: 2.499745339155197\n",
      "Epoch [349/500], Loss: 0.2500\n",
      "total loss: 2.4997428208589554\n",
      "Epoch [350/500], Loss: 0.2500\n",
      "total loss: 2.499767690896988\n",
      "Epoch [351/500], Loss: 0.2500\n",
      "total loss: 2.499749168753624\n",
      "Epoch [352/500], Loss: 0.2500\n",
      "total loss: 2.4997522681951523\n",
      "Epoch [353/500], Loss: 0.2500\n",
      "total loss: 2.499753311276436\n",
      "Epoch [354/500], Loss: 0.2500\n",
      "total loss: 2.4997488856315613\n",
      "Epoch [355/500], Loss: 0.2500\n",
      "total loss: 2.4997474253177643\n",
      "Epoch [356/500], Loss: 0.2500\n",
      "total loss: 2.4997538030147552\n",
      "Epoch [357/500], Loss: 0.2500\n",
      "total loss: 2.4997783601284027\n",
      "Epoch [358/500], Loss: 0.2500\n",
      "total loss: 2.4997873455286026\n",
      "Epoch [359/500], Loss: 0.2500\n",
      "total loss: 2.499758154153824\n",
      "Epoch [360/500], Loss: 0.2500\n",
      "total loss: 2.499754086136818\n",
      "Epoch [361/500], Loss: 0.2500\n",
      "total loss: 2.4997497498989105\n",
      "Epoch [362/500], Loss: 0.2500\n",
      "total loss: 2.4997629076242447\n",
      "Epoch [363/500], Loss: 0.2500\n",
      "total loss: 2.499751314520836\n",
      "Epoch [364/500], Loss: 0.2500\n",
      "total loss: 2.4997508078813553\n",
      "Epoch [365/500], Loss: 0.2500\n",
      "total loss: 2.499746650457382\n",
      "Epoch [366/500], Loss: 0.2500\n",
      "total loss: 2.499747559428215\n",
      "Epoch [367/500], Loss: 0.2500\n",
      "total loss: 2.4997734129428864\n",
      "Epoch [368/500], Loss: 0.2500\n",
      "total loss: 2.4997439980506897\n",
      "Epoch [369/500], Loss: 0.2500\n",
      "total loss: 2.499765768647194\n",
      "Epoch [370/500], Loss: 0.2500\n",
      "total loss: 2.499747395515442\n",
      "Epoch [371/500], Loss: 0.2500\n",
      "total loss: 2.499758720397949\n",
      "Epoch [372/500], Loss: 0.2500\n",
      "total loss: 2.4997982680797577\n",
      "Epoch [373/500], Loss: 0.2500\n",
      "total loss: 2.4997588098049164\n",
      "Epoch [374/500], Loss: 0.2500\n",
      "total loss: 2.4997489005327225\n",
      "Epoch [375/500], Loss: 0.2500\n",
      "total loss: 2.4997523725032806\n",
      "Epoch [376/500], Loss: 0.2500\n",
      "total loss: 2.4997688978910446\n",
      "Epoch [377/500], Loss: 0.2500\n",
      "total loss: 2.4997492283582687\n",
      "Epoch [378/500], Loss: 0.2500\n",
      "total loss: 2.4997464567422867\n",
      "Epoch [379/500], Loss: 0.2500\n",
      "total loss: 2.499754920601845\n",
      "Epoch [380/500], Loss: 0.2500\n",
      "total loss: 2.4997552931308746\n",
      "Epoch [381/500], Loss: 0.2500\n",
      "total loss: 2.499760240316391\n",
      "Epoch [382/500], Loss: 0.2500\n",
      "total loss: 2.499746724963188\n",
      "Epoch [383/500], Loss: 0.2500\n",
      "total loss: 2.499750569462776\n",
      "Epoch [384/500], Loss: 0.2500\n",
      "total loss: 2.4997691214084625\n",
      "Epoch [385/500], Loss: 0.2500\n",
      "total loss: 2.4997455775737762\n",
      "Epoch [386/500], Loss: 0.2500\n",
      "total loss: 2.499767303466797\n",
      "Epoch [387/500], Loss: 0.2500\n",
      "total loss: 2.4997502118349075\n",
      "Epoch [388/500], Loss: 0.2500\n",
      "total loss: 2.4997523576021194\n",
      "Epoch [389/500], Loss: 0.2500\n",
      "total loss: 2.4997505992650986\n",
      "Epoch [390/500], Loss: 0.2500\n",
      "total loss: 2.4997506886720657\n",
      "Epoch [391/500], Loss: 0.2500\n",
      "total loss: 2.499754324555397\n",
      "Epoch [392/500], Loss: 0.2500\n",
      "total loss: 2.499749556183815\n",
      "Epoch [393/500], Loss: 0.2500\n",
      "total loss: 2.4997433871030807\n",
      "Epoch [394/500], Loss: 0.2500\n",
      "total loss: 2.4998224526643753\n",
      "Epoch [395/500], Loss: 0.2500\n",
      "total loss: 2.4997448772192\n",
      "Epoch [396/500], Loss: 0.2500\n",
      "total loss: 2.49975948035717\n",
      "Epoch [397/500], Loss: 0.2500\n",
      "total loss: 2.4997681081295013\n",
      "Epoch [398/500], Loss: 0.2500\n",
      "total loss: 2.4997887164354324\n",
      "Epoch [399/500], Loss: 0.2500\n",
      "total loss: 2.499757409095764\n",
      "Epoch [400/500], Loss: 0.2500\n",
      "total loss: 2.4997521340847015\n",
      "Epoch [401/500], Loss: 0.2500\n",
      "total loss: 2.4997923374176025\n",
      "Epoch [402/500], Loss: 0.2500\n",
      "total loss: 2.4997814744710922\n",
      "Epoch [403/500], Loss: 0.2500\n",
      "total loss: 2.499749019742012\n",
      "Epoch [404/500], Loss: 0.2500\n",
      "total loss: 2.499747857451439\n",
      "Epoch [405/500], Loss: 0.2500\n",
      "total loss: 2.499756008386612\n",
      "Epoch [406/500], Loss: 0.2500\n",
      "total loss: 2.499752163887024\n",
      "Epoch [407/500], Loss: 0.2500\n",
      "total loss: 2.499787911772728\n",
      "Epoch [408/500], Loss: 0.2500\n",
      "total loss: 2.4997810274362564\n",
      "Epoch [409/500], Loss: 0.2500\n",
      "total loss: 2.499756336212158\n",
      "Epoch [410/500], Loss: 0.2500\n",
      "total loss: 2.4997532814741135\n",
      "Epoch [411/500], Loss: 0.2500\n",
      "total loss: 2.499745637178421\n",
      "Epoch [412/500], Loss: 0.2500\n",
      "total loss: 2.499768227338791\n",
      "Epoch [413/500], Loss: 0.2500\n",
      "total loss: 2.499748796224594\n",
      "Epoch [414/500], Loss: 0.2500\n",
      "total loss: 2.4997491985559464\n",
      "Epoch [415/500], Loss: 0.2500\n",
      "total loss: 2.4997555017471313\n",
      "Epoch [416/500], Loss: 0.2500\n",
      "total loss: 2.499759718775749\n",
      "Epoch [417/500], Loss: 0.2500\n",
      "total loss: 2.4997424334287643\n",
      "Epoch [418/500], Loss: 0.2500\n",
      "total loss: 2.4997572153806686\n",
      "Epoch [419/500], Loss: 0.2500\n",
      "total loss: 2.4997651427984238\n",
      "Epoch [420/500], Loss: 0.2500\n",
      "total loss: 2.499757170677185\n",
      "Epoch [421/500], Loss: 0.2500\n",
      "total loss: 2.499745711684227\n",
      "Epoch [422/500], Loss: 0.2500\n",
      "total loss: 2.499765321612358\n",
      "Epoch [423/500], Loss: 0.2500\n",
      "total loss: 2.499748930335045\n",
      "Epoch [424/500], Loss: 0.2500\n",
      "total loss: 2.4997507333755493\n",
      "Epoch [425/500], Loss: 0.2500\n",
      "total loss: 2.499761551618576\n",
      "Epoch [426/500], Loss: 0.2500\n",
      "total loss: 2.499754324555397\n",
      "Epoch [427/500], Loss: 0.2500\n",
      "total loss: 2.4997415989637375\n",
      "Epoch [428/500], Loss: 0.2500\n",
      "total loss: 2.4997875839471817\n",
      "Epoch [429/500], Loss: 0.2500\n",
      "total loss: 2.4998021721839905\n",
      "Epoch [430/500], Loss: 0.2500\n",
      "total loss: 2.499770924448967\n",
      "Epoch [431/500], Loss: 0.2500\n",
      "total loss: 2.499755159020424\n",
      "Epoch [432/500], Loss: 0.2500\n",
      "total loss: 2.4997499585151672\n",
      "Epoch [433/500], Loss: 0.2500\n",
      "total loss: 2.4997515082359314\n",
      "Epoch [434/500], Loss: 0.2500\n",
      "total loss: 2.4997466057538986\n",
      "Epoch [435/500], Loss: 0.2500\n",
      "total loss: 2.4997454434633255\n",
      "Epoch [436/500], Loss: 0.2500\n",
      "total loss: 2.4997455775737762\n",
      "Epoch [437/500], Loss: 0.2500\n",
      "total loss: 2.499745339155197\n",
      "Epoch [438/500], Loss: 0.2500\n",
      "total loss: 2.499746322631836\n",
      "Epoch [439/500], Loss: 0.2500\n",
      "total loss: 2.499760717153549\n",
      "Epoch [440/500], Loss: 0.2500\n",
      "total loss: 2.499761462211609\n",
      "Epoch [441/500], Loss: 0.2500\n",
      "total loss: 2.4997818022966385\n",
      "Epoch [442/500], Loss: 0.2500\n",
      "total loss: 2.499742418527603\n",
      "Epoch [443/500], Loss: 0.2500\n",
      "total loss: 2.499766781926155\n",
      "Epoch [444/500], Loss: 0.2500\n",
      "total loss: 2.4997919648885727\n",
      "Epoch [445/500], Loss: 0.2500\n",
      "total loss: 2.4997743666172028\n",
      "Epoch [446/500], Loss: 0.2500\n",
      "total loss: 2.499745100736618\n",
      "Epoch [447/500], Loss: 0.2500\n",
      "total loss: 2.4997510462999344\n",
      "Epoch [448/500], Loss: 0.2500\n",
      "total loss: 2.499809503555298\n",
      "Epoch [449/500], Loss: 0.2500\n",
      "total loss: 2.4997803270816803\n",
      "Epoch [450/500], Loss: 0.2500\n",
      "total loss: 2.4997726380825043\n",
      "Epoch [451/500], Loss: 0.2500\n",
      "total loss: 2.499741852283478\n",
      "Epoch [452/500], Loss: 0.2500\n",
      "total loss: 2.499745801091194\n",
      "Epoch [453/500], Loss: 0.2500\n",
      "total loss: 2.4997508972883224\n",
      "Epoch [454/500], Loss: 0.2500\n",
      "total loss: 2.4997515082359314\n",
      "Epoch [455/500], Loss: 0.2500\n",
      "total loss: 2.4997428953647614\n",
      "Epoch [456/500], Loss: 0.2500\n",
      "total loss: 2.4997432827949524\n",
      "Epoch [457/500], Loss: 0.2500\n",
      "total loss: 2.499748080968857\n",
      "Epoch [458/500], Loss: 0.2500\n",
      "total loss: 2.499741569161415\n",
      "Epoch [459/500], Loss: 0.2500\n",
      "total loss: 2.4997451454401016\n",
      "Epoch [460/500], Loss: 0.2500\n",
      "total loss: 2.499750033020973\n",
      "Epoch [461/500], Loss: 0.2500\n",
      "total loss: 2.4997442960739136\n",
      "Epoch [462/500], Loss: 0.2500\n",
      "total loss: 2.4997556656599045\n",
      "Epoch [463/500], Loss: 0.2500\n",
      "total loss: 2.4997606575489044\n",
      "Epoch [464/500], Loss: 0.2500\n",
      "total loss: 2.4997419863939285\n",
      "Epoch [465/500], Loss: 0.2500\n",
      "total loss: 2.4997532963752747\n",
      "Epoch [466/500], Loss: 0.2500\n",
      "total loss: 2.499756768345833\n",
      "Epoch [467/500], Loss: 0.2500\n",
      "total loss: 2.499750092625618\n",
      "Epoch [468/500], Loss: 0.2500\n",
      "total loss: 2.4997511357069016\n",
      "Epoch [469/500], Loss: 0.2500\n",
      "total loss: 2.4997544288635254\n",
      "Epoch [470/500], Loss: 0.2500\n",
      "total loss: 2.499748319387436\n",
      "Epoch [471/500], Loss: 0.2500\n",
      "total loss: 2.4997833520174026\n",
      "Epoch [472/500], Loss: 0.2500\n",
      "total loss: 2.4997475892305374\n",
      "Epoch [473/500], Loss: 0.2500\n",
      "total loss: 2.4997557401657104\n",
      "Epoch [474/500], Loss: 0.2500\n",
      "total loss: 2.499770760536194\n",
      "Epoch [475/500], Loss: 0.2500\n",
      "total loss: 2.499744400382042\n",
      "Epoch [476/500], Loss: 0.2500\n",
      "total loss: 2.4997576773166656\n",
      "Epoch [477/500], Loss: 0.2500\n",
      "total loss: 2.4997472167015076\n",
      "Epoch [478/500], Loss: 0.2500\n",
      "total loss: 2.4997589737176895\n",
      "Epoch [479/500], Loss: 0.2500\n",
      "total loss: 2.4997601360082626\n",
      "Epoch [480/500], Loss: 0.2500\n",
      "total loss: 2.4997482299804688\n",
      "Epoch [481/500], Loss: 0.2500\n",
      "total loss: 2.4997501373291016\n",
      "Epoch [482/500], Loss: 0.2500\n",
      "total loss: 2.499753177165985\n",
      "Epoch [483/500], Loss: 0.2500\n",
      "total loss: 2.4997503012418747\n",
      "Epoch [484/500], Loss: 0.2500\n",
      "total loss: 2.4997611939907074\n",
      "Epoch [485/500], Loss: 0.2500\n",
      "total loss: 2.499763607978821\n",
      "Epoch [486/500], Loss: 0.2500\n",
      "total loss: 2.499747574329376\n",
      "Epoch [487/500], Loss: 0.2500\n",
      "total loss: 2.4997431337833405\n",
      "Epoch [488/500], Loss: 0.2500\n",
      "total loss: 2.499770551919937\n",
      "Epoch [489/500], Loss: 0.2500\n",
      "total loss: 2.499746933579445\n",
      "Epoch [490/500], Loss: 0.2500\n",
      "total loss: 2.4997538626194\n",
      "Epoch [491/500], Loss: 0.2500\n",
      "total loss: 2.4997455328702927\n",
      "Epoch [492/500], Loss: 0.2500\n",
      "total loss: 2.499743327498436\n",
      "Epoch [493/500], Loss: 0.2500\n",
      "total loss: 2.4997890144586563\n",
      "Epoch [494/500], Loss: 0.2500\n",
      "total loss: 2.4997509717941284\n",
      "Epoch [495/500], Loss: 0.2500\n",
      "total loss: 2.4997434318065643\n",
      "Epoch [496/500], Loss: 0.2500\n",
      "total loss: 2.4997862726449966\n",
      "Epoch [497/500], Loss: 0.2500\n",
      "total loss: 2.4997705668210983\n",
      "Epoch [498/500], Loss: 0.2500\n",
      "total loss: 2.499759718775749\n",
      "Epoch [499/500], Loss: 0.2500\n",
      "total loss: 2.4997596442699432\n",
      "Epoch [500/500], Loss: 0.2500\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Train the model\n",
    "trained_model = train_model(input_data, desired_output, 500, viz_accuracy=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(trained_model, new_data, expected_result):\n",
    "    test_input = torch.tensor(new_data, dtype=torch.float32)\n",
    "\n",
    "    test_output = trained_model(test_input)\n",
    "\n",
    "    predicted_classes = torch.argmax(test_output, dim=1)\n",
    "\n",
    "    test_output_compare = torch.tensor(expected_result, dtype=torch.int)\n",
    "\n",
    "    return float(sum(test_output_compare == predicted_classes)/len(test_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xs/6yx8fshd7_v5fg9sl09cfpdh0000gn/T/ipykernel_77585/875939923.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_output_compare = torch.tensor(expected_result, dtype=torch.int)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.49959999322891235"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"matrices_test.csv\")\n",
    "just_input_test = test_df.drop('last', axis=1)\n",
    "input_data_test = np.array(just_input_test.values.tolist())\n",
    "desired_output = torch.tensor(test_df['last'].tolist(), dtype=torch.float32).long()\n",
    "\n",
    "test_model(trained_model, input_data_test, desired_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
