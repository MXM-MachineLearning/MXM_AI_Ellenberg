{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning: this doesn't work. This was from Donald's code that I just changed a few things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(string): \n",
    "    a = np.array([[1, 1, 2],[0, 1, 1],[0,-3,-2]])\n",
    "    b = np.array([[-2, 0, -1],[-5, 1, -1],[3,0,1]])\n",
    "\n",
    "    maxMats = 50\n",
    "    dataPoints = 1000\n",
    "    prevMat = -1\n",
    "\n",
    "\n",
    "    for j in range(dataPoints):\n",
    "        nextMat = random.randint(0,1)\n",
    "        if nextMat == 0: \n",
    "            mat = a\n",
    "            prevMat = 0\n",
    "        elif nextMat == 1:\n",
    "            mat = b\n",
    "            prevMat = 1\n",
    "        numOfMatrices = random.randint(1, maxMats)\n",
    "        for i in range(numOfMatrices): \n",
    "            nextMat = random.randint(0,1)\n",
    "            if (nextMat == 0):\n",
    "                mat = np.matmul(a, mat)\n",
    "                prevMat = nextMat\n",
    "            elif (nextMat == 1):\n",
    "                mat = np.matmul(b, mat)\n",
    "                prevMat = nextMat\n",
    "        if j == 0: \n",
    "            df = pd.DataFrame([[mat[0][0],mat[0][1],mat[0][2], mat[1][0],mat[1][1],mat[1][2],mat[2][0],mat[2][1],mat[2][2], prevMat]], columns = ['00', '01', '02','10','11','12', '20', '21','22',\"last\"])\n",
    "        else: \n",
    "            df = pd.concat([df, pd.DataFrame([[mat[0][0],mat[0][1],mat[0][2], mat[1][0],mat[1][1],mat[1][2],mat[2][0],mat[2][1],mat[2][2], prevMat]],columns = ['00', '01', '02','10','11','12', '20', '21','22',\"last\"])])\n",
    "# mat[0][0] = mat[0][0]  % 7\n",
    "# mat [0][1] = mat[0][1]  % 7\n",
    "# mat[0][2] = mat[0][2]  % 7\n",
    "# mat [1][0] = mat[1][0]  % 7\n",
    "# mat [1][1] = mat[1][1]  % 7\n",
    "# mat [1][1] = mat[1][1]  % 7\n",
    "    df.to_csv(string, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code adapted from ChatGPT\n",
    "\n",
    "# Define your neural network model\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "\n",
    "        self.step1 = nn.Linear(9, 128, bias=True)\n",
    "        self.step2 = nn.ReLU()\n",
    "        self.step3 = nn.Linear(128, 64, bias=True)\n",
    "        self.step4 = nn.ReLU()\n",
    "        self.step5 = nn.Linear(64, 16, bias=True)\n",
    "        self.step6 = nn.ReLU()\n",
    "        self.step7 = nn.Linear(16, 1, bias=True)\n",
    "        self.step8 = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # RUN IT ON A GPU if it exists\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.to(\"cuda\")\n",
    "\n",
    "        x = self.step1(x)\n",
    "        x = self.step2(x)\n",
    "        x = self.step3(x)\n",
    "        x = self.step4(x)\n",
    "        x = self.step5(x)\n",
    "        x = self.step6(x)\n",
    "        x = self.step7(x)\n",
    "        x = self.step8(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def train_model(inputs, desired_outputs, num_epochs=100, learning_rate=0.02, momentum=0.9, viz_accuracy=False):\n",
    "    # Convert inputs and desired_outputs to PyTorch tensors\n",
    "    inputs = torch.tensor(inputs, dtype=torch.float32)\n",
    "    desired_outputs = torch.tensor(desired_outputs, dtype=torch.float32)\n",
    "    \n",
    "    # Create a DataLoader to handle batching (if needed)\n",
    "    dataset = TensorDataset(inputs, desired_outputs)\n",
    "    dataloader = DataLoader(dataset, batch_size=1000, shuffle=True)  # Adjust batch_size as needed\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = SimpleModel()\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.MSELoss()  # Mean Squared Error loss\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "    losses = []\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for batch_inputs, batch_desired_outputs in dataloader:\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            outputs = model(batch_inputs)  # Forward pass\n",
    "            loss = criterion(outputs, batch_desired_outputs)  # Compute the loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update the model's parameters\n",
    "            cur_item = loss.item()\n",
    "            total_loss += cur_item\n",
    "        \n",
    "        # Print the average loss for this epoch\n",
    "        print(f\"total loss: {total_loss}\")\n",
    "        average_loss = total_loss / len(dataloader)\n",
    "        if viz_accuracy:\n",
    "            losses.append(average_loss)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.4f}')\n",
    "    \n",
    "    if viz_accuracy:\n",
    "        plt.scatter(x=range(1, len(losses)+1), y=losses)\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Average loss\")\n",
    "        plt.show()\n",
    "\n",
    "    # Return the trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate('matrices_train.csv')\n",
    "generate('matrices_test.csv')\n",
    "df = pd.read_csv(\"matrices_train.csv\")\n",
    "just_input = df.drop('last', axis=1)\n",
    "\n",
    "# adapted from https://stackoverflow.com/questions/43898035/pandas-combine-column-values-into-a-list-in-a-new-column\n",
    "input_data = np.array(just_input.values.tolist())\n",
    "desired_output = torch.tensor(df['last'].tolist(), dtype=torch.float32).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xs/6yx8fshd7_v5fg9sl09cfpdh0000gn/T/ipykernel_77585/3187993261.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  desired_outputs = torch.tensor(desired_outputs, dtype=torch.float32)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1000])) that is different to the input size (torch.Size([1000, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total loss: 0.40301987528800964\n",
      "Epoch [1/500], Loss: 0.4030\n",
      "total loss: 0.40232881903648376\n",
      "Epoch [2/500], Loss: 0.4023\n",
      "total loss: 0.39714398980140686\n",
      "Epoch [3/500], Loss: 0.3971\n",
      "total loss: 0.38839876651763916\n",
      "Epoch [4/500], Loss: 0.3884\n",
      "total loss: 0.3769459128379822\n",
      "Epoch [5/500], Loss: 0.3769\n",
      "total loss: 0.3573397994041443\n",
      "Epoch [6/500], Loss: 0.3573\n",
      "total loss: 0.352821409702301\n",
      "Epoch [7/500], Loss: 0.3528\n",
      "total loss: 0.35677963495254517\n",
      "Epoch [8/500], Loss: 0.3568\n",
      "total loss: 0.3543459475040436\n",
      "Epoch [9/500], Loss: 0.3543\n",
      "total loss: 0.3561733663082123\n",
      "Epoch [10/500], Loss: 0.3562\n",
      "total loss: 0.36547479033470154\n",
      "Epoch [11/500], Loss: 0.3655\n",
      "total loss: 0.37693050503730774\n",
      "Epoch [12/500], Loss: 0.3769\n",
      "total loss: 0.3852001428604126\n",
      "Epoch [13/500], Loss: 0.3852\n",
      "total loss: 0.38999900221824646\n",
      "Epoch [14/500], Loss: 0.3900\n",
      "total loss: 0.3921763002872467\n",
      "Epoch [15/500], Loss: 0.3922\n",
      "total loss: 0.39260488748550415\n",
      "Epoch [16/500], Loss: 0.3926\n",
      "total loss: 0.39144062995910645\n",
      "Epoch [17/500], Loss: 0.3914\n",
      "total loss: 0.3887631297111511\n",
      "Epoch [18/500], Loss: 0.3888\n",
      "total loss: 0.3875177800655365\n",
      "Epoch [19/500], Loss: 0.3875\n",
      "total loss: 0.38452181220054626\n",
      "Epoch [20/500], Loss: 0.3845\n",
      "total loss: 0.3815361559391022\n",
      "Epoch [21/500], Loss: 0.3815\n",
      "total loss: 0.3765166103839874\n",
      "Epoch [22/500], Loss: 0.3765\n",
      "total loss: 0.36933964490890503\n",
      "Epoch [23/500], Loss: 0.3693\n",
      "total loss: 0.3664807081222534\n",
      "Epoch [24/500], Loss: 0.3665\n",
      "total loss: 0.365431010723114\n",
      "Epoch [25/500], Loss: 0.3654\n",
      "total loss: 0.3633786737918854\n",
      "Epoch [26/500], Loss: 0.3634\n",
      "total loss: 0.35931581258773804\n",
      "Epoch [27/500], Loss: 0.3593\n",
      "total loss: 0.36427992582321167\n",
      "Epoch [28/500], Loss: 0.3643\n",
      "total loss: 0.36771446466445923\n",
      "Epoch [29/500], Loss: 0.3677\n",
      "total loss: 0.36710646748542786\n",
      "Epoch [30/500], Loss: 0.3671\n",
      "total loss: 0.36209121346473694\n",
      "Epoch [31/500], Loss: 0.3621\n",
      "total loss: 0.35167643427848816\n",
      "Epoch [32/500], Loss: 0.3517\n",
      "total loss: 0.36216986179351807\n",
      "Epoch [33/500], Loss: 0.3622\n",
      "total loss: 0.3691970109939575\n",
      "Epoch [34/500], Loss: 0.3692\n",
      "total loss: 0.3731151223182678\n",
      "Epoch [35/500], Loss: 0.3731\n",
      "total loss: 0.36755767464637756\n",
      "Epoch [36/500], Loss: 0.3676\n",
      "total loss: 0.35939449071884155\n",
      "Epoch [37/500], Loss: 0.3594\n",
      "total loss: 0.3676123023033142\n",
      "Epoch [38/500], Loss: 0.3676\n",
      "total loss: 0.37006086111068726\n",
      "Epoch [39/500], Loss: 0.3701\n",
      "total loss: 0.37045907974243164\n",
      "Epoch [40/500], Loss: 0.3705\n",
      "total loss: 0.37163257598876953\n",
      "Epoch [41/500], Loss: 0.3716\n",
      "total loss: 0.3674391210079193\n",
      "Epoch [42/500], Loss: 0.3674\n",
      "total loss: 0.35986804962158203\n",
      "Epoch [43/500], Loss: 0.3599\n",
      "total loss: 0.3470384180545807\n",
      "Epoch [44/500], Loss: 0.3470\n",
      "total loss: 0.3366432785987854\n",
      "Epoch [45/500], Loss: 0.3366\n",
      "total loss: 0.32281047105789185\n",
      "Epoch [46/500], Loss: 0.3228\n",
      "total loss: 0.31025904417037964\n",
      "Epoch [47/500], Loss: 0.3103\n",
      "total loss: 0.298566073179245\n",
      "Epoch [48/500], Loss: 0.2986\n",
      "total loss: 0.27584272623062134\n",
      "Epoch [49/500], Loss: 0.2758\n",
      "total loss: 0.2653048038482666\n",
      "Epoch [50/500], Loss: 0.2653\n",
      "total loss: 0.25932711362838745\n",
      "Epoch [51/500], Loss: 0.2593\n",
      "total loss: 0.2569691836833954\n",
      "Epoch [52/500], Loss: 0.2570\n",
      "total loss: 0.25266510248184204\n",
      "Epoch [53/500], Loss: 0.2527\n",
      "total loss: 0.2514357566833496\n",
      "Epoch [54/500], Loss: 0.2514\n",
      "total loss: 0.2507786154747009\n",
      "Epoch [55/500], Loss: 0.2508\n",
      "total loss: 0.2504875361919403\n",
      "Epoch [56/500], Loss: 0.2505\n",
      "total loss: 0.2502191364765167\n",
      "Epoch [57/500], Loss: 0.2502\n",
      "total loss: 0.25018221139907837\n",
      "Epoch [58/500], Loss: 0.2502\n",
      "total loss: 0.25014764070510864\n",
      "Epoch [59/500], Loss: 0.2501\n",
      "total loss: 0.2501150965690613\n",
      "Epoch [60/500], Loss: 0.2501\n",
      "total loss: 0.25008445978164673\n",
      "Epoch [61/500], Loss: 0.2501\n",
      "total loss: 0.2500557005405426\n",
      "Epoch [62/500], Loss: 0.2501\n",
      "total loss: 0.25002846121788025\n",
      "Epoch [63/500], Loss: 0.2500\n",
      "total loss: 0.25000298023223877\n",
      "Epoch [64/500], Loss: 0.2500\n",
      "total loss: 0.24997900426387787\n",
      "Epoch [65/500], Loss: 0.2500\n",
      "total loss: 0.2499564290046692\n",
      "Epoch [66/500], Loss: 0.2500\n",
      "total loss: 0.24993519484996796\n",
      "Epoch [67/500], Loss: 0.2499\n",
      "total loss: 0.24991540610790253\n",
      "Epoch [68/500], Loss: 0.2499\n",
      "total loss: 0.24989672005176544\n",
      "Epoch [69/500], Loss: 0.2499\n",
      "total loss: 0.24987931549549103\n",
      "Epoch [70/500], Loss: 0.2499\n",
      "total loss: 0.24986299872398376\n",
      "Epoch [71/500], Loss: 0.2499\n",
      "total loss: 0.2498476505279541\n",
      "Epoch [72/500], Loss: 0.2498\n",
      "total loss: 0.249833345413208\n",
      "Epoch [73/500], Loss: 0.2498\n",
      "total loss: 0.24981996417045593\n",
      "Epoch [74/500], Loss: 0.2498\n",
      "total loss: 0.2498074322938919\n",
      "Epoch [75/500], Loss: 0.2498\n",
      "total loss: 0.24979573488235474\n",
      "Epoch [76/500], Loss: 0.2498\n",
      "total loss: 0.24978482723236084\n",
      "Epoch [77/500], Loss: 0.2498\n",
      "total loss: 0.24977464973926544\n",
      "Epoch [78/500], Loss: 0.2498\n",
      "total loss: 0.24976515769958496\n",
      "Epoch [79/500], Loss: 0.2498\n",
      "total loss: 0.24975623190402985\n",
      "Epoch [80/500], Loss: 0.2498\n",
      "total loss: 0.24974793195724487\n",
      "Epoch [81/500], Loss: 0.2497\n",
      "total loss: 0.24974024295806885\n",
      "Epoch [82/500], Loss: 0.2497\n",
      "total loss: 0.2497331202030182\n",
      "Epoch [83/500], Loss: 0.2497\n",
      "total loss: 0.24972635507583618\n",
      "Epoch [84/500], Loss: 0.2497\n",
      "total loss: 0.24972017109394073\n",
      "Epoch [85/500], Loss: 0.2497\n",
      "total loss: 0.24971431493759155\n",
      "Epoch [86/500], Loss: 0.2497\n",
      "total loss: 0.24970890581607819\n",
      "Epoch [87/500], Loss: 0.2497\n",
      "total loss: 0.24970389902591705\n",
      "Epoch [88/500], Loss: 0.2497\n",
      "total loss: 0.24969922006130219\n",
      "Epoch [89/500], Loss: 0.2497\n",
      "total loss: 0.2496948391199112\n",
      "Epoch [90/500], Loss: 0.2497\n",
      "total loss: 0.24969083070755005\n",
      "Epoch [91/500], Loss: 0.2497\n",
      "total loss: 0.2496870756149292\n",
      "Epoch [92/500], Loss: 0.2497\n",
      "total loss: 0.24968355894088745\n",
      "Epoch [93/500], Loss: 0.2497\n",
      "total loss: 0.24968025088310242\n",
      "Epoch [94/500], Loss: 0.2497\n",
      "total loss: 0.24967727065086365\n",
      "Epoch [95/500], Loss: 0.2497\n",
      "total loss: 0.2496744990348816\n",
      "Epoch [96/500], Loss: 0.2497\n",
      "total loss: 0.24967192113399506\n",
      "Epoch [97/500], Loss: 0.2497\n",
      "total loss: 0.24966947734355927\n",
      "Epoch [98/500], Loss: 0.2497\n",
      "total loss: 0.24966734647750854\n",
      "Epoch [99/500], Loss: 0.2497\n",
      "total loss: 0.24966515600681305\n",
      "Epoch [100/500], Loss: 0.2497\n",
      "total loss: 0.24966326355934143\n",
      "Epoch [101/500], Loss: 0.2497\n",
      "total loss: 0.24966147541999817\n",
      "Epoch [102/500], Loss: 0.2497\n",
      "total loss: 0.24965977668762207\n",
      "Epoch [103/500], Loss: 0.2497\n",
      "total loss: 0.2496582567691803\n",
      "Epoch [104/500], Loss: 0.2497\n",
      "total loss: 0.2496568262577057\n",
      "Epoch [105/500], Loss: 0.2497\n",
      "total loss: 0.24965550005435944\n",
      "Epoch [106/500], Loss: 0.2497\n",
      "total loss: 0.24965427815914154\n",
      "Epoch [107/500], Loss: 0.2497\n",
      "total loss: 0.24965313076972961\n",
      "Epoch [108/500], Loss: 0.2497\n",
      "total loss: 0.24965208768844604\n",
      "Epoch [109/500], Loss: 0.2497\n",
      "total loss: 0.24965108931064606\n",
      "Epoch [110/500], Loss: 0.2497\n",
      "total loss: 0.24965021014213562\n",
      "Epoch [111/500], Loss: 0.2497\n",
      "total loss: 0.24964934587478638\n",
      "Epoch [112/500], Loss: 0.2496\n",
      "total loss: 0.24964861571788788\n",
      "Epoch [113/500], Loss: 0.2496\n",
      "total loss: 0.24964787065982819\n",
      "Epoch [114/500], Loss: 0.2496\n",
      "total loss: 0.24964720010757446\n",
      "Epoch [115/500], Loss: 0.2496\n",
      "total loss: 0.24964657425880432\n",
      "Epoch [116/500], Loss: 0.2496\n",
      "total loss: 0.24964603781700134\n",
      "Epoch [117/500], Loss: 0.2496\n",
      "total loss: 0.24964548647403717\n",
      "Epoch [118/500], Loss: 0.2496\n",
      "total loss: 0.24964502453804016\n",
      "Epoch [119/500], Loss: 0.2496\n",
      "total loss: 0.24964456260204315\n",
      "Epoch [120/500], Loss: 0.2496\n",
      "total loss: 0.24964414536952972\n",
      "Epoch [121/500], Loss: 0.2496\n",
      "total loss: 0.2496437430381775\n",
      "Epoch [122/500], Loss: 0.2496\n",
      "total loss: 0.24964340031147003\n",
      "Epoch [123/500], Loss: 0.2496\n",
      "total loss: 0.24964307248592377\n",
      "Epoch [124/500], Loss: 0.2496\n",
      "total loss: 0.2496427297592163\n",
      "Epoch [125/500], Loss: 0.2496\n",
      "total loss: 0.24964244663715363\n",
      "Epoch [126/500], Loss: 0.2496\n",
      "total loss: 0.24964219331741333\n",
      "Epoch [127/500], Loss: 0.2496\n",
      "total loss: 0.24964191019535065\n",
      "Epoch [128/500], Loss: 0.2496\n",
      "total loss: 0.24964168667793274\n",
      "Epoch [129/500], Loss: 0.2496\n",
      "total loss: 0.2496415227651596\n",
      "Epoch [130/500], Loss: 0.2496\n",
      "total loss: 0.2496413141489029\n",
      "Epoch [131/500], Loss: 0.2496\n",
      "total loss: 0.24964112043380737\n",
      "Epoch [132/500], Loss: 0.2496\n",
      "total loss: 0.24964100122451782\n",
      "Epoch [133/500], Loss: 0.2496\n",
      "total loss: 0.2496408075094223\n",
      "Epoch [134/500], Loss: 0.2496\n",
      "total loss: 0.24964070320129395\n",
      "Epoch [135/500], Loss: 0.2496\n",
      "total loss: 0.2496405839920044\n",
      "Epoch [136/500], Loss: 0.2496\n",
      "total loss: 0.24964043498039246\n",
      "Epoch [137/500], Loss: 0.2496\n",
      "total loss: 0.2496403604745865\n",
      "Epoch [138/500], Loss: 0.2496\n",
      "total loss: 0.24964019656181335\n",
      "Epoch [139/500], Loss: 0.2496\n",
      "total loss: 0.24964013695716858\n",
      "Epoch [140/500], Loss: 0.2496\n",
      "total loss: 0.249640092253685\n",
      "Epoch [141/500], Loss: 0.2496\n",
      "total loss: 0.24963997304439545\n",
      "Epoch [142/500], Loss: 0.2496\n",
      "total loss: 0.2496398687362671\n",
      "Epoch [143/500], Loss: 0.2496\n",
      "total loss: 0.2496398538351059\n",
      "Epoch [144/500], Loss: 0.2496\n",
      "total loss: 0.24963980913162231\n",
      "Epoch [145/500], Loss: 0.2496\n",
      "total loss: 0.24963971972465515\n",
      "Epoch [146/500], Loss: 0.2496\n",
      "total loss: 0.24963964521884918\n",
      "Epoch [147/500], Loss: 0.2496\n",
      "total loss: 0.24963964521884918\n",
      "Epoch [148/500], Loss: 0.2496\n",
      "total loss: 0.2496395707130432\n",
      "Epoch [149/500], Loss: 0.2496\n",
      "total loss: 0.24963951110839844\n",
      "Epoch [150/500], Loss: 0.2496\n",
      "total loss: 0.24963942170143127\n",
      "Epoch [151/500], Loss: 0.2496\n",
      "total loss: 0.24963942170143127\n",
      "Epoch [152/500], Loss: 0.2496\n",
      "total loss: 0.24963940680027008\n",
      "Epoch [153/500], Loss: 0.2496\n",
      "total loss: 0.2496393620967865\n",
      "Epoch [154/500], Loss: 0.2496\n",
      "total loss: 0.2496393471956253\n",
      "Epoch [155/500], Loss: 0.2496\n",
      "total loss: 0.24963928759098053\n",
      "Epoch [156/500], Loss: 0.2496\n",
      "total loss: 0.24963931739330292\n",
      "Epoch [157/500], Loss: 0.2496\n",
      "total loss: 0.24963928759098053\n",
      "Epoch [158/500], Loss: 0.2496\n",
      "total loss: 0.24963919818401337\n",
      "Epoch [159/500], Loss: 0.2496\n",
      "total loss: 0.24963922798633575\n",
      "Epoch [160/500], Loss: 0.2496\n",
      "total loss: 0.24963922798633575\n",
      "Epoch [161/500], Loss: 0.2496\n",
      "total loss: 0.24963918328285217\n",
      "Epoch [162/500], Loss: 0.2496\n",
      "total loss: 0.24963918328285217\n",
      "Epoch [163/500], Loss: 0.2496\n",
      "total loss: 0.24963918328285217\n",
      "Epoch [164/500], Loss: 0.2496\n",
      "total loss: 0.24963915348052979\n",
      "Epoch [165/500], Loss: 0.2496\n",
      "total loss: 0.2496391236782074\n",
      "Epoch [166/500], Loss: 0.2496\n",
      "total loss: 0.24963915348052979\n",
      "Epoch [167/500], Loss: 0.2496\n",
      "total loss: 0.2496391236782074\n",
      "Epoch [168/500], Loss: 0.2496\n",
      "total loss: 0.249639093875885\n",
      "Epoch [169/500], Loss: 0.2496\n",
      "total loss: 0.2496391236782074\n",
      "Epoch [170/500], Loss: 0.2496\n",
      "total loss: 0.249639093875885\n",
      "Epoch [171/500], Loss: 0.2496\n",
      "total loss: 0.2496391236782074\n",
      "Epoch [172/500], Loss: 0.2496\n",
      "total loss: 0.249639093875885\n",
      "Epoch [173/500], Loss: 0.2496\n",
      "total loss: 0.24963907897472382\n",
      "Epoch [174/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [175/500], Loss: 0.2496\n",
      "total loss: 0.24963906407356262\n",
      "Epoch [176/500], Loss: 0.2496\n",
      "total loss: 0.24963906407356262\n",
      "Epoch [177/500], Loss: 0.2496\n",
      "total loss: 0.24963904917240143\n",
      "Epoch [178/500], Loss: 0.2496\n",
      "total loss: 0.24963903427124023\n",
      "Epoch [179/500], Loss: 0.2496\n",
      "total loss: 0.24963906407356262\n",
      "Epoch [180/500], Loss: 0.2496\n",
      "total loss: 0.24963906407356262\n",
      "Epoch [181/500], Loss: 0.2496\n",
      "total loss: 0.24963906407356262\n",
      "Epoch [182/500], Loss: 0.2496\n",
      "total loss: 0.24963906407356262\n",
      "Epoch [183/500], Loss: 0.2496\n",
      "total loss: 0.24963903427124023\n",
      "Epoch [184/500], Loss: 0.2496\n",
      "total loss: 0.24963901937007904\n",
      "Epoch [185/500], Loss: 0.2496\n",
      "total loss: 0.24963904917240143\n",
      "Epoch [186/500], Loss: 0.2496\n",
      "total loss: 0.24963904917240143\n",
      "Epoch [187/500], Loss: 0.2496\n",
      "total loss: 0.24963904917240143\n",
      "Epoch [188/500], Loss: 0.2496\n",
      "total loss: 0.24963904917240143\n",
      "Epoch [189/500], Loss: 0.2496\n",
      "total loss: 0.24963897466659546\n",
      "Epoch [190/500], Loss: 0.2496\n",
      "total loss: 0.24963897466659546\n",
      "Epoch [191/500], Loss: 0.2496\n",
      "total loss: 0.24963903427124023\n",
      "Epoch [192/500], Loss: 0.2496\n",
      "total loss: 0.24963901937007904\n",
      "Epoch [193/500], Loss: 0.2496\n",
      "total loss: 0.24963898956775665\n",
      "Epoch [194/500], Loss: 0.2496\n",
      "total loss: 0.24963901937007904\n",
      "Epoch [195/500], Loss: 0.2496\n",
      "total loss: 0.24963898956775665\n",
      "Epoch [196/500], Loss: 0.2496\n",
      "total loss: 0.24963903427124023\n",
      "Epoch [197/500], Loss: 0.2496\n",
      "total loss: 0.24963901937007904\n",
      "Epoch [198/500], Loss: 0.2496\n",
      "total loss: 0.24963901937007904\n",
      "Epoch [199/500], Loss: 0.2496\n",
      "total loss: 0.24963904917240143\n",
      "Epoch [200/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [201/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [202/500], Loss: 0.2496\n",
      "total loss: 0.24963901937007904\n",
      "Epoch [203/500], Loss: 0.2496\n",
      "total loss: 0.24963903427124023\n",
      "Epoch [204/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [205/500], Loss: 0.2496\n",
      "total loss: 0.24963898956775665\n",
      "Epoch [206/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [207/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [208/500], Loss: 0.2496\n",
      "total loss: 0.24963901937007904\n",
      "Epoch [209/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [210/500], Loss: 0.2496\n",
      "total loss: 0.24963901937007904\n",
      "Epoch [211/500], Loss: 0.2496\n",
      "total loss: 0.24963903427124023\n",
      "Epoch [212/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [213/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [214/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [215/500], Loss: 0.2496\n",
      "total loss: 0.24963901937007904\n",
      "Epoch [216/500], Loss: 0.2496\n",
      "total loss: 0.24963891506195068\n",
      "Epoch [217/500], Loss: 0.2496\n",
      "total loss: 0.24963897466659546\n",
      "Epoch [218/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [219/500], Loss: 0.2496\n",
      "total loss: 0.24963903427124023\n",
      "Epoch [220/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [221/500], Loss: 0.2496\n",
      "total loss: 0.24963897466659546\n",
      "Epoch [222/500], Loss: 0.2496\n",
      "total loss: 0.24963903427124023\n",
      "Epoch [223/500], Loss: 0.2496\n",
      "total loss: 0.24963901937007904\n",
      "Epoch [224/500], Loss: 0.2496\n",
      "total loss: 0.24963897466659546\n",
      "Epoch [225/500], Loss: 0.2496\n",
      "total loss: 0.24963895976543427\n",
      "Epoch [226/500], Loss: 0.2496\n",
      "total loss: 0.24963901937007904\n",
      "Epoch [227/500], Loss: 0.2496\n",
      "total loss: 0.24963897466659546\n",
      "Epoch [228/500], Loss: 0.2496\n",
      "total loss: 0.24963898956775665\n",
      "Epoch [229/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [230/500], Loss: 0.2496\n",
      "total loss: 0.24963906407356262\n",
      "Epoch [231/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [232/500], Loss: 0.2496\n",
      "total loss: 0.24963901937007904\n",
      "Epoch [233/500], Loss: 0.2496\n",
      "total loss: 0.24963901937007904\n",
      "Epoch [234/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [235/500], Loss: 0.2496\n",
      "total loss: 0.24963898956775665\n",
      "Epoch [236/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [237/500], Loss: 0.2496\n",
      "total loss: 0.24963901937007904\n",
      "Epoch [238/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [239/500], Loss: 0.2496\n",
      "total loss: 0.24963903427124023\n",
      "Epoch [240/500], Loss: 0.2496\n",
      "total loss: 0.24963895976543427\n",
      "Epoch [241/500], Loss: 0.2496\n",
      "total loss: 0.24963901937007904\n",
      "Epoch [242/500], Loss: 0.2496\n",
      "total loss: 0.24963901937007904\n",
      "Epoch [243/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [244/500], Loss: 0.2496\n",
      "total loss: 0.24963898956775665\n",
      "Epoch [245/500], Loss: 0.2496\n",
      "total loss: 0.24963898956775665\n",
      "Epoch [246/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [247/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [248/500], Loss: 0.2496\n",
      "total loss: 0.24963898956775665\n",
      "Epoch [249/500], Loss: 0.2496\n",
      "total loss: 0.24963898956775665\n",
      "Epoch [250/500], Loss: 0.2496\n",
      "total loss: 0.24963897466659546\n",
      "Epoch [251/500], Loss: 0.2496\n",
      "total loss: 0.24963903427124023\n",
      "Epoch [252/500], Loss: 0.2496\n",
      "total loss: 0.24963903427124023\n",
      "Epoch [253/500], Loss: 0.2496\n",
      "total loss: 0.24963901937007904\n",
      "Epoch [254/500], Loss: 0.2496\n",
      "total loss: 0.24963894486427307\n",
      "Epoch [255/500], Loss: 0.2496\n",
      "total loss: 0.24963901937007904\n",
      "Epoch [256/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [257/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [258/500], Loss: 0.2496\n",
      "total loss: 0.24963898956775665\n",
      "Epoch [259/500], Loss: 0.2496\n",
      "total loss: 0.24963903427124023\n",
      "Epoch [260/500], Loss: 0.2496\n",
      "total loss: 0.24963898956775665\n",
      "Epoch [261/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [262/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [263/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [264/500], Loss: 0.2496\n",
      "total loss: 0.24963903427124023\n",
      "Epoch [265/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [266/500], Loss: 0.2496\n",
      "total loss: 0.24963898956775665\n",
      "Epoch [267/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [268/500], Loss: 0.2496\n",
      "total loss: 0.24963898956775665\n",
      "Epoch [269/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [270/500], Loss: 0.2496\n",
      "total loss: 0.24963895976543427\n",
      "Epoch [271/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [272/500], Loss: 0.2496\n",
      "total loss: 0.24963903427124023\n",
      "Epoch [273/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [274/500], Loss: 0.2496\n",
      "total loss: 0.24963898956775665\n",
      "Epoch [275/500], Loss: 0.2496\n",
      "total loss: 0.24963894486427307\n",
      "Epoch [276/500], Loss: 0.2496\n",
      "total loss: 0.24963897466659546\n",
      "Epoch [277/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [278/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [279/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [280/500], Loss: 0.2496\n",
      "total loss: 0.24963903427124023\n",
      "Epoch [281/500], Loss: 0.2496\n",
      "total loss: 0.24963904917240143\n",
      "Epoch [282/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [283/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [284/500], Loss: 0.2496\n",
      "total loss: 0.24963897466659546\n",
      "Epoch [285/500], Loss: 0.2496\n",
      "total loss: 0.24963898956775665\n",
      "Epoch [286/500], Loss: 0.2496\n",
      "total loss: 0.24963901937007904\n",
      "Epoch [287/500], Loss: 0.2496\n",
      "total loss: 0.24963901937007904\n",
      "Epoch [288/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [289/500], Loss: 0.2496\n",
      "total loss: 0.24963898956775665\n",
      "Epoch [290/500], Loss: 0.2496\n",
      "total loss: 0.24963897466659546\n",
      "Epoch [291/500], Loss: 0.2496\n",
      "total loss: 0.24963901937007904\n",
      "Epoch [292/500], Loss: 0.2496\n",
      "total loss: 0.24963903427124023\n",
      "Epoch [293/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [294/500], Loss: 0.2496\n",
      "total loss: 0.24963903427124023\n",
      "Epoch [295/500], Loss: 0.2496\n",
      "total loss: 0.24963897466659546\n",
      "Epoch [296/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [297/500], Loss: 0.2496\n",
      "total loss: 0.24963901937007904\n",
      "Epoch [298/500], Loss: 0.2496\n",
      "total loss: 0.24963895976543427\n",
      "Epoch [299/500], Loss: 0.2496\n",
      "total loss: 0.24963901937007904\n",
      "Epoch [300/500], Loss: 0.2496\n",
      "total loss: 0.24963898956775665\n",
      "Epoch [301/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [302/500], Loss: 0.2496\n",
      "total loss: 0.24963897466659546\n",
      "Epoch [303/500], Loss: 0.2496\n",
      "total loss: 0.24963898956775665\n",
      "Epoch [304/500], Loss: 0.2496\n",
      "total loss: 0.24963901937007904\n",
      "Epoch [305/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [306/500], Loss: 0.2496\n",
      "total loss: 0.24963897466659546\n",
      "Epoch [307/500], Loss: 0.2496\n",
      "total loss: 0.24963901937007904\n",
      "Epoch [308/500], Loss: 0.2496\n",
      "total loss: 0.24963901937007904\n",
      "Epoch [309/500], Loss: 0.2496\n",
      "total loss: 0.24963901937007904\n",
      "Epoch [310/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [311/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [312/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [313/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [314/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [315/500], Loss: 0.2496\n",
      "total loss: 0.24963898956775665\n",
      "Epoch [316/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [317/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [318/500], Loss: 0.2496\n",
      "total loss: 0.24963898956775665\n",
      "Epoch [319/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [320/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [321/500], Loss: 0.2496\n",
      "total loss: 0.24963898956775665\n",
      "Epoch [322/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [323/500], Loss: 0.2496\n",
      "total loss: 0.24963898956775665\n",
      "Epoch [324/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [325/500], Loss: 0.2496\n",
      "total loss: 0.24963901937007904\n",
      "Epoch [326/500], Loss: 0.2496\n",
      "total loss: 0.24963903427124023\n",
      "Epoch [327/500], Loss: 0.2496\n",
      "total loss: 0.24963903427124023\n",
      "Epoch [328/500], Loss: 0.2496\n",
      "total loss: 0.24963901937007904\n",
      "Epoch [329/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [330/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [331/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [332/500], Loss: 0.2496\n",
      "total loss: 0.24963901937007904\n",
      "Epoch [333/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [334/500], Loss: 0.2496\n",
      "total loss: 0.24963901937007904\n",
      "Epoch [335/500], Loss: 0.2496\n",
      "total loss: 0.24963901937007904\n",
      "Epoch [336/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [337/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [338/500], Loss: 0.2496\n",
      "total loss: 0.24963901937007904\n",
      "Epoch [339/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [340/500], Loss: 0.2496\n",
      "total loss: 0.24963898956775665\n",
      "Epoch [341/500], Loss: 0.2496\n",
      "total loss: 0.24963903427124023\n",
      "Epoch [342/500], Loss: 0.2496\n",
      "total loss: 0.24963904917240143\n",
      "Epoch [343/500], Loss: 0.2496\n",
      "total loss: 0.24963903427124023\n",
      "Epoch [344/500], Loss: 0.2496\n",
      "total loss: 0.24963903427124023\n",
      "Epoch [345/500], Loss: 0.2496\n",
      "total loss: 0.24963903427124023\n",
      "Epoch [346/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [347/500], Loss: 0.2496\n",
      "total loss: 0.24963898956775665\n",
      "Epoch [348/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [349/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [350/500], Loss: 0.2496\n",
      "total loss: 0.24963898956775665\n",
      "Epoch [351/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [352/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [353/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [354/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [355/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [356/500], Loss: 0.2496\n",
      "total loss: 0.24963898956775665\n",
      "Epoch [357/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [358/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [359/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [360/500], Loss: 0.2496\n",
      "total loss: 0.24963898956775665\n",
      "Epoch [361/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [362/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [363/500], Loss: 0.2496\n",
      "total loss: 0.24963898956775665\n",
      "Epoch [364/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [365/500], Loss: 0.2496\n",
      "total loss: 0.24963901937007904\n",
      "Epoch [366/500], Loss: 0.2496\n",
      "total loss: 0.24963901937007904\n",
      "Epoch [367/500], Loss: 0.2496\n",
      "total loss: 0.24963901937007904\n",
      "Epoch [368/500], Loss: 0.2496\n",
      "total loss: 0.24963901937007904\n",
      "Epoch [369/500], Loss: 0.2496\n",
      "total loss: 0.24963901937007904\n",
      "Epoch [370/500], Loss: 0.2496\n",
      "total loss: 0.24963903427124023\n",
      "Epoch [371/500], Loss: 0.2496\n",
      "total loss: 0.24963901937007904\n",
      "Epoch [372/500], Loss: 0.2496\n",
      "total loss: 0.24963903427124023\n",
      "Epoch [373/500], Loss: 0.2496\n",
      "total loss: 0.24963901937007904\n",
      "Epoch [374/500], Loss: 0.2496\n",
      "total loss: 0.24963901937007904\n",
      "Epoch [375/500], Loss: 0.2496\n",
      "total loss: 0.24963901937007904\n",
      "Epoch [376/500], Loss: 0.2496\n",
      "total loss: 0.24963901937007904\n",
      "Epoch [377/500], Loss: 0.2496\n",
      "total loss: 0.24963901937007904\n",
      "Epoch [378/500], Loss: 0.2496\n",
      "total loss: 0.24963901937007904\n",
      "Epoch [379/500], Loss: 0.2496\n",
      "total loss: 0.24963901937007904\n",
      "Epoch [380/500], Loss: 0.2496\n",
      "total loss: 0.24963901937007904\n",
      "Epoch [381/500], Loss: 0.2496\n",
      "total loss: 0.24963901937007904\n",
      "Epoch [382/500], Loss: 0.2496\n",
      "total loss: 0.24963901937007904\n",
      "Epoch [383/500], Loss: 0.2496\n",
      "total loss: 0.24963901937007904\n",
      "Epoch [384/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [385/500], Loss: 0.2496\n",
      "total loss: 0.24963898956775665\n",
      "Epoch [386/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [387/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [388/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [389/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [390/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [391/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [392/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [393/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [394/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [395/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [396/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [397/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [398/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [399/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [400/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [401/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [402/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [403/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [404/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [405/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [406/500], Loss: 0.2496\n",
      "total loss: 0.24963898956775665\n",
      "Epoch [407/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [408/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [409/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [410/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [411/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [412/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [413/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [414/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [415/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [416/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [417/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [418/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [419/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [420/500], Loss: 0.2496\n",
      "total loss: 0.24963898956775665\n",
      "Epoch [421/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [422/500], Loss: 0.2496\n",
      "total loss: 0.24963898956775665\n",
      "Epoch [423/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [424/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [425/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [426/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [427/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [428/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [429/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [430/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [431/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [432/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [433/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [434/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [435/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [436/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [437/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [438/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [439/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [440/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [441/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [442/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [443/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [444/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [445/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [446/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [447/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [448/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [449/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [450/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [451/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [452/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [453/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [454/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [455/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [456/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [457/500], Loss: 0.2496\n",
      "total loss: 0.24963898956775665\n",
      "Epoch [458/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [459/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [460/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [461/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [462/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [463/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [464/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [465/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [466/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [467/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [468/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [469/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [470/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [471/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [472/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [473/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [474/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [475/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [476/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [477/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [478/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [479/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [480/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [481/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [482/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [483/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [484/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [485/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [486/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [487/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [488/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [489/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [490/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [491/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [492/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [493/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [494/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [495/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [496/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [497/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [498/500], Loss: 0.2496\n",
      "total loss: 0.24963898956775665\n",
      "Epoch [499/500], Loss: 0.2496\n",
      "total loss: 0.24963900446891785\n",
      "Epoch [500/500], Loss: 0.2496\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Train the model\n",
    "trained_model = train_model(input_data, desired_output, 500, viz_accuracy=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(trained_model, new_data, expected_result):\n",
    "    test_input = torch.tensor(new_data, dtype=torch.float32)\n",
    "\n",
    "    test_output = trained_model(test_input)\n",
    "\n",
    "    predicted_classes = torch.argmax(test_output, dim=1)\n",
    "\n",
    "    test_output_compare = torch.tensor(expected_result, dtype=torch.int)\n",
    "\n",
    "    return float(sum(test_output_compare == predicted_classes)/len(test_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xs/6yx8fshd7_v5fg9sl09cfpdh0000gn/T/ipykernel_77585/875939923.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_output_compare = torch.tensor(expected_result, dtype=torch.int)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5270000100135803"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"matrices_test.csv\")\n",
    "just_input_test = test_df.drop('last', axis=1)\n",
    "input_data_test = np.array(just_input_test.values.tolist())\n",
    "desired_output = torch.tensor(test_df['last'].tolist(), dtype=torch.float32).long()\n",
    "\n",
    "test_model(trained_model, input_data_test, desired_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
